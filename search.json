[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Preface\nindex.qmd\nIn keeping with the Environmental Health Sciences doctoral program’s objective to develop independent scientists capable of self-directed study and original research, this dissertation presents a three-stage program that was self-designed to address complex problems at the intersection of data science and public health.\nThe project begins with the engineering of a novel research tool: a data ETL (Extract, Transform, Load) and RAG (Retrieval-Augmented Generation) pipeline designed for advanced literature analytics (Aim 1). The efficacy of this tool is then demonstrated through a systematic review of research on ozone exposure and cardiovascular disease (CVD), which informs a baseline machine learning analysis of the ozone-CVD relationship using large-scale public health data (Aim 2). Finally, to address a key methodological gap identified by the pipeline; the underutilization of deep learning, the project culminates in applying a state-of-the-art framework using Convolutional and Graph Neural Networks (CNNs/GNNs) to a challenging environmental monitoring task: the identification of unconventional oil and gas (fracking) sites from satellite imagery and the health effects associated with methane leaks from abandoned UOG sites (Aim 3).\nMethodologically, this dissertation incorporates a hybrid approach, positioned at the intersection of (b) an epidemiologic study (specifically, an ecological study of ozone-CVD risk) and (c) quantitative environmental modeling, with a primary focus on the development of novel computational methods. As such, the creation of the pipeline itself serves as a demonstration of competency in the research methodologies of computer programming, data engineering, and applied machine learning, thereby fulfilling the program’s Research Tool Requirement not merely by taking a course, but through the creation of a new research instrument.\n\n\nNOTE ON DATA AVIALABILITY\nGiven the massive size of the initial datasets and subsequently generated subset datasets, the only datasets privided within the data folder are the ones necessary to reproduce the random forest analysis. all other preliminary datasets will be available to view and download via institutianl link to a folder on my onedrive.\nKey sections of this document include:\n\nProject Proposal (01-proposal.qmd): Details the overall research objectives, the architecture of the AI-Powered ETL & RAG system, its significance, and includes an initial literature review 13 Literature Review that contextualizes the project’s aims and the development of the automated pipeline. See the Proposal’s Literature Review for foundational context.\nData Collection (02-data-collection.qmd): Describes the data sources (e.g., PubMed, OpenAlex, CDC datasets) and the methodologies employed for acquiring academic literature and other relevant datasets used throughout the project.\nSystematic Literature Review: Ozone, CVD, and Advanced Analytical Methods (03-literature-review.qmd): This chapter presents the comprehensive, pipeline-driven systematic literature review. It begins with the introduction and focuses on the intersection of ambient ozone exposure, cardiovascular disease outcomes, and the application of geospatial and machine learning techniques, with a specific emphasis on identifying methodological gaps related to CNNs and GNNs.\nAnalysis: Modeling Ozone-CVD Relationships (04-analysis.qmd): This chapter details the two core analytical components of the dissertation. First, it presents the baseline Random Forest analysis of ozone-CVD associations (Aim 2). Second, it outlines the proposed methodology for the advanced deep learning framework (CNNs and GNNs) for identifying and monitoring unconventional oil and gas sites using satellite data (Aim 3).\nTechnical Report (05-technical-report.qmd): Provides a detailed technical description of the ETL+HySemRAG system’s architecture and methodology. This chapter covers all phases of operation, from data acquisition and enrichment to document analysis, and includes a video demonstration of the system’s capabilities.\nAppendices: Provide supplementary technical details. Key appendices include:\n\nAppendix D: Knowledge Graph Queries and Supporting Data: Details the Cypher queries used with the Neo4j knowledge graph for study selection and data characterization. (View Appendix D)\nAppendix E: Database Queries: the queries which were introduced into the ETL system to search the three databases. (View Appendix E)\nAppendix B: Preliminary LCA Random Forest Analysis Preliminary Analysis conducted for LCA, the work conducted therein will be enhanced through the detailed approach in the introduction. (View Appendix B)\nAppendix C: Computational Hardware: The computation resources and hardware which were used to make this project possible are detailed within this appendix. (View Appendix C)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "intro.qmd\n\n2 Background and Motivation\nThe escalating volume and complexity of scientific literature present a formidable challenge to researchers striving to synthesize knowledge, discern methodological advancements, and identify critical research gaps, particularly within multifaceted domains like geospatial epidemiology and other data-intensive fields such as environmental systems analysis, including Life Cycle Assessment (LCA). Ambient air pollution, with ground-level ozone as a pervasive constituent, poses significant public health risks, and its potential impact on cardiovascular disease (CVD) remains an area of intense investigation and concern due to the global burden of CVD.\nUnderstanding these intricate environment-health relationships necessitates rigorous analytical approaches; however, the traditional process of literature navigation to pinpoint novel research questions or underutilized methodologies can be exceptionally demanding, often imposing considerable stress and time constraints that divert resources from other essential research activities, personal well-being, and even timely grant or job applications. This motivation, driven by the desire to optimize research efficiency, thereby creating more bandwidth for deep analytical work, exploring diverse research facets across various scientific inquiries, and maintaining a sustainable work-life balance, for instance, ensuring adequate time for personal commitments like caring for an aging canine companion, was a significant motivator for developing a more streamlined approach to the foundational stages of research.\nWhile this work initially focuses on the pervasive issue of ground-level ozone, the methodological framework is designed to be extensible to other critical environmental health challenges, such as quantifying exposure from point-source emissions like methane from unconventional oil and gas (UOG) development, which represents a key future direction of this research.\n\n\n3 The AI-Powered ETL+RAG System: A Novel Approach\nTo address the aforementioned challenges of large-scale literature analysis and efficient gap identification, this project developed an AI-powered, semi-automated pipeline combining Extract, Transform, Load (ETL) processes with Retrieval-Augmented Generation (RAG) capabilities. This system (as detailed in our project proposal) is designed with a flexible architecture for broad applicability, enabling it to systematically collect, process, and analyze extensive collections of academic articles, extracting structured text, key metadata, and domain-specific information.\nWhile its primary demonstration in this paper focuses on facilitating the targeted identification of under-explored research areas and methodologies within geospatial epidemiology, its utility extends to other domains. The efficiency gained through such automation significantly alleviates the traditional burdens of manual review, streamlining the path to discovering impactful research questions. It is also crucial to emphasize that while this system leverages artificial intelligence for its analytical power, it does not operate without human oversight; to counter any misinterpretation of placing blind trust in automated outputs or potential machine ‘hallucinations,’ a critical final step involves manual review and verification of system-identified articles using a custom-developed interactive tool (Flask app.py, as detailed in the project repository), ensuring expert validation of relevance and context prior to inclusion in semantic and graph databases like Qdrant and Neo4j.\n\n\n4 A Pipeline-Driven Application\nIn this paper, we present a comprehensive literature review that serves as a direct application and demonstration of our AI-powered ETL and RAG pipeline’s utility. The specific research domain chosen for this demonstration is the intersection of ambient ozone exposure, cardiovascular (specifically heart) disease outcomes, and the application of geospatial and machine learning methodologies.\nThis thematic focus was strategically selected, informed by both the pressing public health questions at this nexus and the prior availability of extensive, highly granular datasets suitable for subsequent empirical analysis (including daily census-tract level ozone concentrations and CDC cardiovascular disease mortality data stratified by age group, sex, and race).\nA primary outcome of this application was the data-driven identification of a significant methodological gap applicable across environmental epidemiology: the notable underutilization of advanced deep learning techniques, such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), for modeling complex spatiotemporal exposures. This finding, while discovered through an ozone-CVD lens, directly motivates the novel framework proposed in our third specific aim.\n\n\n5 Methodological Distinction and Interdisciplinary Scope\nThis project distinguishes itself primarily through the transparent, pipeline-driven methodology used for its construction and focus. While traditional narrative reviews often rely on manual curation and subjective identification of themes, our approach leverages the ETL+RAG system to systematically process a broad corpus of literature, identify salient topics, and pinpoint specific methodological gaps, such as the limited use of CNNs and GNNs for ozone-CVD health outcome modeling. The system’s interdisciplinary adaptability can also be demonstrated through its application in analyzing literature for Life Cycle Assessment (LCA) of agricultural systems, where its configurable field extraction selections though a custom built graphic user interface (GUI) and extensive synonym dictionaries (approx. 10,000 synonyms across 27+ dictionaries) aid in constructing Life Cycle Inventories and contextualizing empirical findings from GWP (Global Warming Potential) models, such as a Random Forest analysis of cover crop impacts.\nIn both domains, the RAG component, featuring an agentic pipeline (agentic_pipeline.py) for iterative reasoning, refinement, evaluation, and QA checks using locally deployed LLMs (Qwen3-32B) and systematic storage of curated interactions and data for potential fine-tuning, significantly streamlines the learning process and knowledge synthesis when entering or reviewing complex fields. This AI-assisted process not only enhances the efficiency of identifying a focused research area but also provides a data-backed foundation for the review’s scope. By concentrating on studies employing machine learning, and subsequently highlighting the scarcity of certain advanced applications therein, this review offers a targeted synthesis that complements broader reviews on ozone and cardiovascular health by specifically examining the cutting edge of analytical techniques and their current limitations in this specific epidemiological context.\n\n\n6 An Interdisciplinary Approach: Bridging Data Engineering, AI, and Environmental Health\nThis project is, by design, an interdisciplinary endeavor, demonstrating how methodologies from distinct technical fields can be integrated to solve complex problems in Environmental Health Sciences. The work presented here represents a synthesis of skills spanning multiple domains:\n\nData and Software Engineering: The foundation of this research is a custom-built ETL pipeline. This involved architecting a robust, multi-stage system for data acquisition, implementing asynchronous and parallel processing for high-throughput performance, and even performing low-level debugging and modification of existing C++ libraries to ensure stability.\nApplied Machine Learning: The entire pipeline leverages machine learning, from the initial document layout analysis and topic modeling (LDA) to the preliminary predictive modeling (Random Forest) and the final, advanced deep learning applications (CNNs and GNNs). A core component is the implementation of a Retrieval-Augmented Generation (RAG) system using locally deployed LLMs for nuanced knowledge synthesis and gap analysis.\nEnvironmental Health and Epidemiology: The technical framework is applied directly to a critical environmental health issue: the relationship between ambient air pollution (ozone) and cardiovascular disease (CVD). The project uses real-world CDC mortality data and environmental datasets to conduct an epidemiologic analysis, identifying risks and informing future public health research.\n\nBy weaving these fields together, this work serves as a case study in how modern data science can accelerate scientific discovery. It moves from engineering a novel research tool to applying it to identify a specific methodological gap, and finally, to using it to analyze real-world environmental health data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-proposal.html",
    "href": "01-proposal.html",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "",
    "text": "3 Overall Goal\nI aim to create and execute an end-to-end research program that will:\nThe core objective is to demonstrate a complete research lifecycle: from AI-driven gap identification to baseline analysis, and culminating in the application of cutting-edge methods to create a new, foundational dataset for environmental health research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#interdisciplinary-impact-example",
    "href": "01-proposal.html#interdisciplinary-impact-example",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "10.1 Interdisciplinary Impact Example",
    "text": "10.1 Interdisciplinary Impact Example\nBeyond its application in geospatial epidemiology, the inherent flexibility of the AI-Powered ETL & RAG system architecture makes it directly applicable to other complex, data-intensive research domains such as Life Cycle Assessment (LCA) for agricultural systems, particularly concerning cover crop GWP and the compilation of Life Cycle Inventories (LCI). The existing ETL pipeline (including fast_pubmed.py, fast_openalex.py) can be readily aimed at literature pertaining to LCA, GWP, and cover crop management. The core field_extraction.py module, configurable via etl_config.json and the project’s GUI (config_tab.py), is already equipped with extraction fields highly pertinent to LCA/LCI. Specifically, pre-defined fields such as “LCA System Boundaries”, “Environmental Impact Categories”, “Management Practices and Operational Details”, and the structured “Metrics” field (designed to capture {“Metric”: “name”, “Value”: “value”, “Unit”: “unit”}) are directly suited for systematically extracting quantitative LCI data (fertilizer inputs, fuel consumption, emission values, crop yields) and qualitative contextual information from a large corpus of relevant scientific articles.\nThe system’s capacity for knowledge structuration further enhances its utility for LCA/GWP research. The extensive existing knowledge base, comprising over 25 dictionaries with approximately 10,000 synonyms managed via dictionaries_gui.py (from dictionaries_config.json), includes relevant lexicons like “AGRONOMIC_MGMT_PRACTICES_SYNONYMS”, “ENVIRONMENTAL_IMPACT_CATEGORIES”, “COVER_CROP_SYNONYMS”, and “SOIL_TYPE_SYNONYMS”. These require only clicking a few checkboxes for LCA-specific terminology (specific LCI flow names, impact assessment methods, GWP characterization factors) within the GUI rather than development from scratch. The extracted and semantically unified data, including detailed LCI parameters and GWP values, is then processed by kg_pipeline.py to construct a domain-specific knowledge graph. This KG can serve as a queryable, literature-derived database of LCI components and GWP factors, enabling systematic comparison of different cover crop systems, validation of LCI data, and benchmarking of GWP model results, such as those from the previously developed Random Forest analysis for GWP_Total.\nThis application to LCA/GWP research effectively demonstrates the system’s broader “Interdisciplinary Impact” and its function as a “Flexible Framework” capable of large-scale, structured data extraction and analysis from scientific literature across diverse domains. By selecting fields within the GUI to prioritize LCA-relevant fields and refining the domain-specific dictionaries, the pipeline can efficiently build a comprehensive knowledge base to support detailed LCA studies and contextualize empirical GWP findings. This not only addresses specific research needs within agricultural systems analysis but also validates the core design principles of the ETL+RAG system for robust and adaptable automated literature analytics.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#introduction",
    "href": "01-proposal.html#introduction",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nRetrieval-Augmented Generation (RAG) is a machine-learning framework combining information retrieval with deep-learning models, such as transformer-based large language models (Rawson 2024). This approach enhances factual accuracy, coherence, and context relevance of generated responses by incorporating retrieved information into the generation process (Li and Lai 2024). Within geospatial epidemiology, particularly concerning ozone exposure and heart disease, RAG offers a tool for synthesizing spatial and temporal data from environmental and health datasets. By integrating parametric memory of large language models with non-parametric external knowledge bases, RAG supports researchers. For example, RAG can help identify critical knowledge gaps and underrepresented methodologies (Barnett et al. 2024), and improve the factual accuracy of health information (Upadhyay and Viviani 2025), advancing the understanding of chronic disease risk factors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#technical-background",
    "href": "01-proposal.html#technical-background",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.2 Technical Background",
    "text": "13.2 Technical Background\nRAG combines deep learning with traditional information retrieval, integrating parametric and non-parametric memory systems to enhance text generation accuracy. The parametric component involves transformer-based neural models, such as sequence-to-sequence architectures (BART, T5, LLaMA), that store learned information within model weights and can be fine-tuned for specific tasks (Rawson 2024; Feng et al. 2024). Non-parametric memory relies on external databases indexed as dense vector representations, queried using neural retrievers like Dense Passage Retriever (DPR) or dual-encoder architectures, enabling retrieval of contextually relevant information without model retraining (Rawson 2024; Şakar and Emekci 2024). Transformer-based architectures are employed in RAG pipelines for their language modeling capabilities, facilitating coherent and contextually informed responses. The combination of transformer generators and retrieval encoders improves accuracy, relevance, and freshness of generated content by leveraging external, updateable knowledge bases (Fan et al. 2024; Wang et al. 2024). RAG systems bridge gaps between static, trained model knowledge and evolving external data sources, often outperforming traditional NLP methods in factual precision and resource efficiency (Li and Lai 2024; Upadhyay and Viviani 2025).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#empirical-evidence-and-performance-evaluation-of-rag-systems",
    "href": "01-proposal.html#empirical-evidence-and-performance-evaluation-of-rag-systems",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.3 Empirical Evidence and Performance Evaluation of RAG Systems",
    "text": "13.3 Empirical Evidence and Performance Evaluation of RAG Systems\nEmpirical evaluation of RAG assesses its enhancement of factual accuracy, retrieval effectiveness, and efficiency in text generation. Studies like Fateh Ali et al. (2024) compared RAG-based pipelines against conventional NLP approaches, including transformer-only models and frequency-based topic modeling, finding RAG superior based on higher ROUGE-1 (0.364) and ROUGE-2 (0.123) scores. Upadhyay and Viviani (2025) demonstrated RAG’s effectiveness in health-information retrieval, nearly doubling performance metrics like CAM NDCG (0.2146 vs. 0.1119) and CAM MAP (0.1079 vs. 0.0455) compared to non-RAG systems. A central hypothesis is that integrating retrieval with generation boosts the accuracy, reliability, and contextual relevance of outputs.\nStudies employed experimental designs comparing RAG models with multiple baselines, including transformer-based models (T5, GPT-3.5), frequency-based methods (spaCy), and traditional IR techniques (BM25). Data sources included academic corpora, scientific literature collections, and clinical notes, selected for relevance, quality, and representativeness (Li and Lai 2024; Rawson 2024; Shah-Mohammadi and Finkelstein 2024). Common evaluation metrics included ROUGE scores, precision, recall, F1-scores, CAM-MAP, and CAM-NDCG, assessing textual relevance, accuracy, and coherence.\nQuantitative results from empirical studies indicate RAG’s improved performance in generating contextually accurate and relevant outputs. For example, Kreimeyer et al. (2024) reported RAG reproducing over 80% of expert-curated information in oncology literature, indicating reliability and precision. Gu et al. (2024) demonstrated RAG’s efficiency gains by reducing manual systematic review extraction time from 1.310 minutes to 5 minutes, without sacrificing data quality. These studies highlight RAG’s strengths but also emphasize managing retrieval parameters (e.g., chunk size, embedding strategies) to avoid retrieval inaccuracies or context misalignment (Barnett et al. 2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#applications-in-systematic-reviews-and-evidence-synthesis",
    "href": "01-proposal.html#applications-in-systematic-reviews-and-evidence-synthesis",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.4 Applications in Systematic Reviews and Evidence Synthesis",
    "text": "13.4 Applications in Systematic Reviews and Evidence Synthesis\nRAG streamlines evidence collection in systematic reviews by combining parametric and non-parametric memory systems. These systems pair neural sequence-to-sequence generators with dense-vector indices queried by neural retrievers, automating the identification and extraction of relevant literature (Rawson 2024; Şakar and Emekci 2024). This automation reduces the manual workload associated with literature screening and data extraction. For instance, GPT-3.5 Turbo with RAG reduced data extraction time in systematic reviews from 1,310 minutes to 5 minutes, maintaining high concordance with human reviewers for extracting structured data (Gu et al. 2024).\nRAG also enhances the accuracy and consistency of data extraction processes. In health information retrieval tasks, RAG-driven models demonstrated improved topical relevance and factual accuracy, with improved performance metrics such as CAM-MAP and CAM-NDCG compared to traditional information retrieval methods (Upadhyay and Viviani 2025). Furthermore, RAG with quantized LLMs improves data extraction accuracy (exceeding 93%) while reducing computational resource demands (Ranjan Maharana and Joshi 2024). These approaches improve efficiency and can mitigate human biases, aiding uniformity in evidence synthesis.\nAdvanced RAG implementations in systematic reviews leverage semantic search capabilities through dense-vector indices and neural retrieval methods. This approach often retrieves literature with greater contextual understanding than conventional keyword-based searches. When such retrieval is combined with large language models for generation, it can support the identification of relevant and potentially overlooked literature. Recent empirical studies illustrate that integrating RAG into literature review processes yields better summarization accuracy, with measurable improvements in ROUGE metrics compared to other NLP methods (Fateh Ali et al. 2024). This demonstrates RAG’s capability to provide accurate, contextually relevant summaries for high-quality evidence synthesis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#applications-in-public-health-and-epidemiology",
    "href": "01-proposal.html#applications-in-public-health-and-epidemiology",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.5 Applications in Public Health and Epidemiology",
    "text": "13.5 Applications in Public Health and Epidemiology\nRAG applications in Public Health and Epidemiology enhance information synthesis and decision-making. RAG systems ground responses in current, domain-specific evidence by incorporating information from diverse sources like clinical guidelines and surveillance data, improving factual accuracy (Rawson 2024; Li and Lai 2024). This capability aids public health professionals in synthesizing evolving information for interventions. Examples include updating knowledge bases for health worker training (Al Ghadban et al. 2023), streamlining surveillance via clinical note data extraction (Shah-Mohammadi and Finkelstein 2024), and processing large document sets for key point summarization with tools like ‘Cognitive Reviewer’ (Barnett et al. 2024). RAG also automates accurate structured dataset creation from literature (Ranjan Maharana and Joshi 2024). Policy experts recognize RAG for improving accuracy in evidence synthesis (Fleurence et al. 2024b).\nIn health communication and public engagement, RAG contributes to generating clear, accurate, and personalized health information. For instance, RAG integrated into models for breast-cancer nursing care questions yielded high user-rated accuracy and empathy (R. Xu et al. 2024). RAG-driven systems are also used to train frontline health workers by providing traceable, guideline-based information adaptable to evolving clinical knowledge (Al Ghadban et al. 2023). By enhancing topical relevance and factual accuracy in consumer health information retrieval and constraining hallucinations, RAG contributes to reliable health education and efforts against misinformation (Upadhyay and Viviani 2025).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#identification-of-methodological-gaps-using-retrieval-augmented-generation",
    "href": "01-proposal.html#identification-of-methodological-gaps-using-retrieval-augmented-generation",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.6 Identification of Methodological Gaps using Retrieval-Augmented Generation",
    "text": "13.6 Identification of Methodological Gaps using Retrieval-Augmented Generation\nRAG can be a meta-analytic tool to identify methodological gaps by processing scientific literature. RAG deployments map ‘failure points’ in various workflows, pinpointing areas with weak or insufficient methodologies (Barnett et al. 2024). For example, by analyzing large datasets, RAG reveals under-explored techniques like low-resource quantized LLMs, offering efficiency and accuracy benefits overlooked in conventional dataset-building (Ranjan Maharana and Joshi 2024). This capability allows surveying current methods and identifying areas lacking detailed research.\nRAG application in benchmarking and comparative analysis exposes inconsistencies and under-researched areas. When used for evaluations, such as in multilingual medical question answering, RAG reveals performance drops in contexts like non-English languages, underscoring methodological blind spots or areas where methods lack robustness (Alonso, Oronoz, and Agerri 2024). Highlighting performance discrepancies helps recognize variations in methodological effectiveness or under-defined methods, surfacing research gaps (Barnett et al. 2024).\nInsights from RAG application guide selecting and refining future research methodologies. RAG spotlights actionable improvements; for example, quantized LLMs reducing computational demand while improving accuracy suggests a methodological shift (Ranjan Maharana and Joshi 2024). RAG systems also recommend analytical strategies, such as guiding target selection in drug design or suggesting techniques to overcome limitations in traditional methods, informing future research (Zhang, Wang, and Sun 2024; H. Xu et al. 2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#discussion",
    "href": "01-proposal.html#discussion",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.7 Discussion",
    "text": "13.7 Discussion\nThe development of this automated Extract, Transform, Load (ETL) and Retrieval-Augmented Generation (RAG) pipeline was undertaken as a direct response to the inherent challenges of identifying a novel and impactful research topic within a complex domain like geospatial epidemiology. As highlighted in our course’s “Advice on Research,” selecting a research problem carefully, focusing on fundamentals, and finding a unique perspective in potentially crowded areas are crucial early steps. This system was engineered to provide a systematic and efficient methodology for this discovery process, using the intersection of ozone exposure, heart disease, and geospatial/machine learning techniques as its initial proving ground.\nThe pipeline’s architecture is tailored to facilitate an iterative research discovery workflow, as outlined in the project’s data collection and integration strategy. The rapid ETL phase, capable of processing hundreds of articles in minutes using modules like fast_pubmed.py and GPU-accelerated Docling (Auer et al. 2024), allows for an initial broad sweep of the literature. Subsequently, the integration of Latent Dirichlet Allocation (LDA) via topic_modeling_culda.py enables the automated categorization of this literature and, crucially, the identification of “weakly populated or contradictory clusters,” which serve as indicators of potential knowledge gaps or under-explored methodological niches. This aligns with the goal of finding less “crowded” research areas. The system then leverages these insights to refine search queries, allowing for a more targeted collection of literature to deepen understanding around these identified gaps. This iterative process of broad collection, gap identification via topic modeling and LLM interpretation, and refined collection is central to the system’s utility in topic formulation.\nFurthermore, the RAG component, with its knowledge graph integration (Neo4j, kg_pipeline.py, inspired by Tu et al. (2024)) and novel Reciprocal-Rank Fusion (RRF) for precise retrieval, allows for nuanced exploration of these identified gaps. It enables users to query the enriched, gap-focused corpus to understand existing methodologies and pinpoint specific underrepresented geospatial or machine learning approaches. The high metadata similarity scores (Levenshtein &gt;0.90) and stringent QA processes aim to ensure that the insights derived are based on accurate data, supporting the selection of a research question that is both relevant and methodologically sound. The system’s current application to the literature on ozone exposure and heart disease, with the intent to link findings to CDC datasets, serves as a practical demonstration of how this pipeline can navigate from a broad area of interest to a specific, data-informed research question, thereby operationalizing the principles of effective research topic selection. The pipeline doesn’t just automate literature review; it structures the exploratory phase of research itself.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#conclusion",
    "href": "01-proposal.html#conclusion",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.8 Conclusion",
    "text": "13.8 Conclusion\nThe primary contribution of this work is the development and practical demonstration of an automated ETL+RAG pipeline designed as a robust tool for systematically identifying research gaps and formulating novel research questions in complex scientific fields such as geospatial epidemiology. This system directly addresses the challenge of selecting a meaningful research topic by providing an efficient, scalable, and transparent framework for navigating and analyzing vast quantities of academic literature. By integrating rapid data ingestion, advanced PDF content extraction, iterative topic modeling for gap identification, and sophisticated RAG capabilities, the pipeline operationalizes key advice for effective research, enabling a structured approach to move from broad exploration to specific, actionable research directions.\nThe value of this pipeline lies in its capacity to empower researchers, particularly those embarking on new projects or exploring interdisciplinary areas, to efficiently discover under-explored methodological niches or substantive knowledge gaps. In the context of this project, it has laid the groundwork for pinpointing underrepresented geospatial and machine learning techniques in the study of ozone exposure and heart disease. More broadly, this process-oriented tool offers a significant advancement over traditional manual methods, demonstrating how AI can be leveraged not just for data analysis, but for the foundational stage of research discovery itself, fostering methodologically sound and impactful inquiry.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#future-directions",
    "href": "01-proposal.html#future-directions",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "13.9 Future Directions",
    "text": "13.9 Future Directions\nFuture development of this ETL+RAG pipeline will focus on enhancing its efficacy as a research discovery tool and broadening its application. A key priority is the refinement of the quality assurance (QA) process to reduce the “occasional false negatives” and improve the precision of gap identification. This involves exploring more dynamic error detection and advanced hallucination mitigation strategies for the LLM components (Huang et al. 2024; Fan et al. 2024).\nFrom a research process perspective, the immediate next step involves applying the methodologies identified by the pipeline to the large-scale CDC datasets on ozone and heart disease. This will involve implementing relevant machine learning approaches (e.g., Random Forest, geospatial cross-validation) to analyze these datasets, thereby empirically validating the research gap identified by the pipeline and completing the class project’s analytical component. The insights from this application will also serve as a feedback loop for further refining the pipeline’s gap identification capabilities.\nFurther enhancements to the pipeline itself will include exploring computational efficiencies, such as the integration of quantized LLMs (Ranjan Maharana and Joshi 2024) and optimized retrieval mechanisms (Wang et al. 2024), to manage costs and improve processing speeds for even larger literature corpora. Incorporating more sophisticated uncertainty-aware retrieval (Dhole 2025) and explicit bias detection mechanisms (Li and Lai 2024) will also be critical. Finally, the modular design of the pipeline lends itself to expansion. Future work will aim to test its generalizability by applying it to identify methodological gaps in other epidemiological areas and diverse scientific domains, potentially fine-tuning domain-specific LLMs to enhance its utility for a wider range of research discovery tasks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#scholarly-literature-corpus-for-aim-1",
    "href": "01-proposal.html#scholarly-literature-corpus-for-aim-1",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "14.1 Scholarly Literature Corpus (for Aim 1)",
    "text": "14.1 Scholarly Literature Corpus (for Aim 1)\n\nSources: PubMed, OpenAlex, and Scopus for article metadata; Unpaywall and publisher websites for full-text PDFs.\nVolume: An initial corpus of up to 1,000 PDF articles, with the system designed to scale to several thousand.\nFormats: Source metadata (JSON), extracted article content (Markdown), structured data (Feather files), and semantic embeddings stored in a Qdrant vector database.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#public-health-and-environmental-datasets-for-aim-2",
    "href": "01-proposal.html#public-health-and-environmental-datasets-for-aim-2",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "14.2 Public Health and Environmental Datasets (for Aim 2)",
    "text": "14.2 Public Health and Environmental Datasets (for Aim 2)\n\nSources: U.S. Centers for Disease Control and Prevention (CDC) Wonder and Data.gov portals.\nSpecific Datasets: Daily census-tract level ozone concentrations (2001-2014); county-level cardiovascular disease mortality rates (2000-2019); and U.S. Census data for demographic covariates.\nVolume: Tens of gigabytes, comprising over 400 million row-level observations in the source files.\nFormats: CSV files, which will be processed and stored in the efficient Feather format for analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#geospatial-and-remote-sensing-imagery-for-aim-3",
    "href": "01-proposal.html#geospatial-and-remote-sensing-imagery-for-aim-3",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "14.3 Geospatial and Remote Sensing Imagery (for Aim 3)",
    "text": "14.3 Geospatial and Remote Sensing Imagery (for Aim 3)\n\nSources: Public and commercial satellite data providers.\n\nOptical Imagery: High-resolution (0.5m - 2m) multi-spectral imagery from sources such as the National Agriculture Imagery Program (NAIP), Planet, or Maxar.\nMethane Data: High-resolution methane concentration data from emerging platforms like MethaneSAT, Carbon Mapper, and GHGSat.\nContextual Data: Geocoded tax parcel data from county governments and LULC data from the USGS.\n\nVolume: Terabytes of raster and vector data covering large geographical areas of interest.\nFormats: GeoTIFF (for raster imagery), Shapefile or GeoJSON (for vector data), and CSVs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#a-novel-ai-powered-literature-analysis-pipeline-from-aim-1",
    "href": "01-proposal.html#a-novel-ai-powered-literature-analysis-pipeline-from-aim-1",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "15.1 A Novel AI-Powered Literature Analysis Pipeline (from Aim 1)",
    "text": "15.1 A Novel AI-Powered Literature Analysis Pipeline (from Aim 1)\n\nA complete, open-source Python-based ETL and RAG pipeline that enables the seamless, automated ingestion, extraction, and analysis of scientific literature to systematically identify methodological research gaps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#a-baseline-model-of-ozones-impact-on-cardiovascular-health-from-aim-2",
    "href": "01-proposal.html#a-baseline-model-of-ozones-impact-on-cardiovascular-health-from-aim-2",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "15.2 A Baseline Model of Ozone’s Impact on Cardiovascular Health (from Aim 2)",
    "text": "15.2 A Baseline Model of Ozone’s Impact on Cardiovascular Health (from Aim 2)\n\nA robust machine learning analysis quantifying the association between various ozone exposure metrics and county-level CVD mortality in the U.S. This will serve as a critical baseline and a direct application of the gap-finding pipeline.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#a-state-of-the-art-framework-and-inventory-for-uog-monitoring-from-aim-3",
    "href": "01-proposal.html#a-state-of-the-art-framework-and-inventory-for-uog-monitoring-from-aim-3",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "15.3 A State-of-the-Art Framework and Inventory for UOG Monitoring (from Aim 3)",
    "text": "15.3 A State-of-the-Art Framework and Inventory for UOG Monitoring (from Aim 3)\n\nA validated deep learning framework (CNN/GNN) capable of identifying and classifying the activity status of UOG sites at scale using multi-modal satellite data. The primary deliverable will be a novel, high-resolution spatiotemporal inventory of UOG wells, providing a foundational dataset for future environmental health research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#comprehensive-documentation-and-dissemination",
    "href": "01-proposal.html#comprehensive-documentation-and-dissemination",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "15.4 Comprehensive Documentation and Dissemination",
    "text": "15.4 Comprehensive Documentation and Dissemination\n\nA final dissertation document and an interactive Quarto-based report demonstrating the literature-derived insights, the baseline geospatial analysis, and the innovative deep learning framework and its outputs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-fast_pubmed.py",
    "href": "01-proposal.html#module-fast_pubmed.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.1 Module: fast_pubmed.py",
    "text": "16.1 Module: fast_pubmed.py\nDescription:\nA drop-in turbo-replacement for the two original helpers:\n\npubmed_search_chunked\nfetch_pubmed_metadata\n\nKey features: - &lt; 1 s single esearch to collect up-to-10,000 PMIDs. - Thread-pooled, rate-limited bulk-efetch (≤ 3 req/s/IP – NCBI TOS). - Parses the same rich metadata as previously used: Authors, Date, Journal, Volume, Issue, Pages, plus Title / DOI / Abstract. - Robust logging with granular exception capture; a bad article never aborts the batch. - No hard-wired globals; everything is configurable via keyword args.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-fast_openalex.py",
    "href": "01-proposal.html#module-fast_openalex.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.2 Module: fast_openalex.py",
    "text": "16.2 Module: fast_openalex.py\nDescription:\nA Python module for rapidly querying and retrieving scholarly records from the OpenAlex API, designed to complement PubMed searches by capturing literature not indexed in PubMed. Utilizing asynchronous I/O and parallel processing, this tool fetches article metadata at scale, parsing and formatting results into a convenient DataFrame for immediate integration into the analysis pipeline.\nKey Features:\n\nHigh-Speed Retrieval: Uses asynchronous API requests (aiohttp) and concurrent fetching to maximize throughput while respecting rate limits.\nEfficient Pagination: Automatically handles pagination and cursor-based retrieval, fetching results swiftly and transparently.\nRobust Parsing and Cleaning: Extracts and cleans metadata such as DOI, abstracts, authorship, publication dates, journal information, and page ranges, ensuring standardized, analysis-ready outputs.\nEasy Integration: Provides a simple wrapper (search_works_df) that returns results in a Pandas DataFrame consistent with existing data processing workflows.\nPolite API Interaction: Adheres strictly to recommended OpenAlex rate limits, ensuring reliable, long-term API access without throttling.\nConfigurable & Flexible: Easily adjustable query parameters, such as items per page, concurrency level, and query-per-second rate, allow for custom optimizations.\n\nIdeal for comprehensive literature reviews, systematic analyses, and dataset augmentation tasks that require accessing a broader scope of scholarly publications beyond traditional indexing services like PubMed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-etl_elsevier.py",
    "href": "01-proposal.html#module-etl_elsevier.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.3 Module: etl_elsevier.py",
    "text": "16.3 Module: etl_elsevier.py\nDescription:\nA Python module for retrieving scholarly articles from Elsevier’s Scopus database, integrating with the existing ETL workflow. It uses asynchronous requests (aiohttp) to fetch article metadata, enriches records via Crossref, and handles data parsing, error management, and structured data outputs.\nMain Functionalities:\nThe module retrieves metadata from Scopus using paginated requests while adhering to Elsevier’s API rate limits and parsing key metadata such as DOI, Scopus ID, titles, and journal information. It concurrently enriches this data with additional details from Crossref, including authors, abstracts, and PDF links, incorporating error handling, retries, and standardized field formatting. Configuration settings—including API credentials, endpoints, rate limits, concurrency, and logging—are managed externally via a JSON file (etl_config.json), which supports flexible adjustments and provides detailed logging for error tracking. The final enriched metadata is structured into Pandas DataFrames with standardized columns optimized for downstream ETL workflows.\nKey Features:\n\nAsynchronous Requests: Utilizes asyncio/aiohttp for scalable, concurrent API requests, improving throughput.\nFlexible Configuration: All configurations (API keys, search parameters, file paths) managed via external JSON for flexibility.\nRobust Parsing & Data Integrity: Implements metadata parsing with safeguards to maintain data quality.\nComprehensive Logging: Detailed logging provides real-time insights into data fetching, processing status, and error handling.\n\nIdeal for comprehensive literature reviews, systematic analyses, and dataset augmentation tasks that require accessing a broader scope of scholarly publications beyond traditional indexing services like PubMed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-async_unpaywall.py",
    "href": "01-proposal.html#module-async_unpaywall.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.4 Module: async_unpaywall.py",
    "text": "16.4 Module: async_unpaywall.py\nDescription:\nA helper function for quickly retrieving full-text PDFs from the Unpaywall API given DOI identifiers. Ideal for automated bulk PDF downloads in an ETL workflow.\nKey features:\n\nUses Python’s asynchronous libraries (asyncio + aiohttp) for concurrent fetching.\nAdheres to Unpaywall’s API limits (≤ 8 requests per second).\nSupports optional proxy rotation for reliability.\nAutomatically sanitizes PDF filenames, preventing file-system errors.\nhandles HTTP failures, missing PDFs, and other edge cases with logging.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-docling_extract_formulas_mp_multi.py",
    "href": "01-proposal.html#module-docling_extract_formulas_mp_multi.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.5 Module docling_extract_formulas_mp_multi.py",
    "text": "16.5 Module docling_extract_formulas_mp_multi.py\nDescription:\nA multiprocessing-optimized PDF extraction module using Docling’s layout analysis capabilities, enriched with GPU acceleration. It converts PDFs into structured text, accurately extracting elements such as tables, equations, and formatted text.\nKey features:\n\nMulti-process parallelization with GPU allocation for optimized throughput.\nExtracts full document text as Markdown, handling complex formatting and special characters.\nSpecialized extraction for embedded LaTeX formulas and structured table data.\nerror handling and logging ensure resilience against problematic PDFs.\nIntegrated token-counting for downstream NLP workflows.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-fast_zotero.py",
    "href": "01-proposal.html#module-fast_zotero.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.6 Module: fast_zotero.py",
    "text": "16.6 Module: fast_zotero.py\nDescription:\nA helper module designed forintegration with Zotero citation management workflows. Enables batch creation of Zotero items, parallelized PDF uploads, and automatic citation formatting via local citeproc rendering.\nKey features:\n\nbatch creation of Zotero items (up to 50 items per API call).\nParallelized PDF attachment uploads via thread pooling.\nLocal citation rendering using the CSL (citeproc) engine for instant citation generation without API latency.\nOptional fallback to Zotero’s built-in citation rendering API if local citation rendering encounters issues.\ncustomizable citation-style handling through a configurable local CSL style registry.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-field_extraction.py",
    "href": "01-proposal.html#module-field_extraction.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.7 Module: field_extraction.py",
    "text": "16.7 Module: field_extraction.py\nDescription:\nA extraction module that enhances a Feather-format DataFrame by retrieving structured metadata fields through iterative Large Language Model (LLM) calls. It uses techniques for JSON parsing, LLM response caching, tokenization, parallelized processing, and merging strategies to maintain data integrity and avoid duplication.\nKey features:\n\nRobust JSON Parsing: Cleans and interprets LLM-generated JSON output, handling common formatting errors.\nIntelligent LLM Caching: Avoids redundant API calls by caching LLM responses based on prompt content hashes.\nParallel Processing: Employs thread pools for efficient concurrent handling of multiple data rows and text chunks.\nDynamic Text Chunking: Splits long texts into manageable segments to stay within LLM token limits, ensuring comprehensive extraction.\nField Unification: Carefully merges newly extracted data with existing metadata, ensuring consistency and preventing duplication, especially within structured fields such as numeric metrics.\nComprehensive Logging: Detailed logging and exception handling provide transparency and facilitate troubleshooting.\n\nThis module is particularly optimized for workflows involving complex metadata extraction scenarios, such as academic literature reviews, where accuracy, scalability, and structured data consistency are crucial.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-topic_modeling_gui.py",
    "href": "01-proposal.html#module-topic_modeling_gui.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.8 Module: topic_modeling_gui.py",
    "text": "16.8 Module: topic_modeling_gui.py\nDescription:\nA Python module for performing topic modeling on large document collections using parallelized workflows. It combines text preprocessing with Latent Dirichlet Allocation (LDA) modeling, supporting both scikit-learn and Gensim frameworks. The module emphasizes computational efficiency through parallel processing, optimized BLAS thread usage, and structured hyperparameter searches to achieve the highest-quality topic coherence.\nKey features:\n\nRobust Preprocessing: Efficiently cleans text by removing references, standardizing formats, tokenizing, removing stopwords, and lemmatizing.\nParallel Processing: Utilizes multiprocessing and threading for rapid tokenization, preprocessing, and dominant-topic extraction, significantly speeding up operations on large datasets.\nFlexible LDA Implementation: integrates scikit-learn’s LDA models with fallback support for Gensim models, ensuring consistent performance and reliability.\nHyperparameter Optimization: Uses exhaustive grid search across critical LDA parameters (e.g., number of topics, bigram/trigram thresholds), optimizing for coherence scores.\nDominant Topic Identification: Efficiently extracts and summarizes the most significant topics per document, indicating their contribution and top keywords.\nDetailed Logging and Error Handling: Provides logging at each processing step, simplifying debugging and ensuring transparency of the modeling process.\n\nIdeal for research-intensive tasks such as systematic literature reviews, thematic content analysis, or exploratory text analysis, this module enhances productivity and result quality through its structured approach.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-kg_pipeline.py",
    "href": "01-proposal.html#module-kg_pipeline.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.9 Module: kg_pipeline.py",
    "text": "16.9 Module: kg_pipeline.py\nDescription:\nThe kg_pipeline.py script serves as a integration module that transforms structured article metadata and analytical results into a Knowledge Graph (KG) within a Neo4j database. It handles entity and relationship management, ensures data integrity via constraints, and uses unified terminology dictionaries for consistent entity naming.\nMain functionalities include:\n\nNeo4j Integration:\n\nEstablishes connection and handles transactional updates to Neo4j.\nUses merge operations to add/update entities such as Articles, Study Types, Soil Types, Cover Crops, and more.\n\nEntity Unification:\n\nApplies extensive synonym unification via dictionaries.py to standardize terminology across various fields, ensuring consistency and accuracy in the knowledge graph.\n\nDynamic Schema Management:\n\nAutomatically sets up Neo4j constraints to maintain schema integrity and database performance.\n\nError Handling & Logging:\n\nComprehensive logging and exception handling facilitate easy debugging and robust data integrity verification during the KG construction process.\n\nIntegration with Topic Modeling:\n\nIncorporates dominant topics and associated metadata, linking textual analysis results directly within the KG.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "01-proposal.html#module-dictionaries.py",
    "href": "01-proposal.html#module-dictionaries.py",
    "title": "2  Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology",
    "section": "16.10 Module: dictionaries.py",
    "text": "16.10 Module: dictionaries.py\nDescription:\nThis module contains dictionaries used for unifying terminology across numerous categories in the ETL pipeline. With thousands of terms and synonyms across 30+ dictionaries, it supports consistency and accuracy of entity representation within the knowledge graph.\nKey Dictionaries Include: - Study Types - Environmental Impact Categories - Management Practices - Cover Crops - Tillage Practices - Metric Names - Machine Learning Methods - and many more.\nThese unified dictionaries provide synonym resolution capabilities to handle variations in terminology commonly encountered across scientific literature.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html",
    "href": "02-data-collection.html",
    "title": "3  Data Collection",
    "section": "",
    "text": "4 Purpose and Relevance\nWhile the initial project was an AI-powered ETL & RAG pipeline for literature analytics, the same pipeline could be adapted to identify research gaps and create unique insights into public health or geospatial phenomena.\nI am considering including these CDC datasets for either:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#cdc-500-cities-project-20162019",
    "href": "02-data-collection.html#cdc-500-cities-project-20162019",
    "title": "3  Data Collection",
    "section": "5.1 CDC 500 Cities Project (2016–2019)",
    "text": "5.1 CDC 500 Cities Project (2016–2019)\n\nFocus: Provided tract-level estimates for chronic disease risk factors and outcomes (e.g., smoking prevalence, obesity, diabetes) within the largest 500 US cities.\nFormat & Size: Over 28,000 census tracts across ~500 cities. Population coverage of over 100 million.\nPotential Use Case: Could cross-reference academic articles discussing urban health interventions with actual city-level data on risk factors (via the RAG pipeline’s references).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#nationwide-blood-donor-seroprevalence-survey-20222023",
    "href": "02-data-collection.html#nationwide-blood-donor-seroprevalence-survey-20222023",
    "title": "3  Data Collection",
    "section": "5.2 Nationwide Blood Donor Seroprevalence Survey (2022–2023)",
    "text": "5.2 Nationwide Blood Donor Seroprevalence Survey (2022–2023)\n\nFocus: SARS-CoV-2 (COVID-19) antibody testing among blood donors, capturing infection- and vaccine-induced seroprevalence.\nData: De-identified blood samples tested for anti-spike (vaccinated or previously infected) vs. anti-nucleocapsid (infection-only) antibodies.\nPotential Use Case: Combine with academic literature on vaccination strategies or compare the pipeline’s reference retrieval on “COVID-19 seroprevalence” to real, up-to-date trends.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#pm2.5-and-ozone-concentrations",
    "href": "02-data-collection.html#pm2.5-and-ozone-concentrations",
    "title": "3  Data Collection",
    "section": "5.3 PM2.5 and Ozone Concentrations",
    "text": "5.3 PM2.5 and Ozone Concentrations\n\nDaily County-Level PM2.5 (2001–2019)\nDaily Census-Tract-Level Ozone (2001–2005, 2006–2010, 2011–2014)\nSize: Up to 11 GB or more, with 100M+ rows in certain files.\nSpatial Coverage: Each row has FIPS codes and lat/long coordinates for geospatial analysis.\nPotential Use Case: If the project investigates air quality or environment-related health outcomes and references environment-based interventions (e.g., “cover crops to reduce air pollutants or improve soil health”), the pipeline could unify the references about environmental intervention strategies with real local air quality data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#places-measures-expanded-from-500-cities",
    "href": "02-data-collection.html#places-measures-expanded-from-500-cities",
    "title": "3  Data Collection",
    "section": "5.4 PLACES Measures (Expanded from 500 Cities)",
    "text": "5.4 PLACES Measures (Expanded from 500 Cities)\n\nFocus: Health outcomes, preventive measures, risk behaviors, disabilities, and more. This extends the 500 Cities concept to every county, place, and census tract in the United States.\nFormat: Data measures in multiple categories: (1) Health Outcomes, (2) Prevention,\n\nRisk Behaviors, (4) Disabilities, (5) Social Determinants, etc.\n\nPotential Use Case: Tie in references discussing health disparities or social determinants of health to actual local or census-tract-level data on risk factors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#statecounty-mortality-datasets",
    "href": "02-data-collection.html#statecounty-mortality-datasets",
    "title": "3  Data Collection",
    "section": "5.5 State/County Mortality Datasets",
    "text": "5.5 State/County Mortality Datasets\n\nCoronary Heart Disease & Stroke\nHeartDisease_Stroke_Mortality_Adults (≥35 years)\nStroke_Mortality_State_County\nFocus: Mortality rates, changes over time, demographic breakdowns (race, age, sex), all georeferenced by county or sometimes census tract.\nPotential Use Case: Link references to epidemiological insights about heart disease or stroke with the real aggregated data for analysis or cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#covid-19-case-surveillance-data",
    "href": "02-data-collection.html#covid-19-case-surveillance-data",
    "title": "3  Data Collection",
    "section": "5.6 COVID-19 Case Surveillance Data",
    "text": "5.6 COVID-19 Case Surveillance Data\n\nSize: ~14 GB, 106+ million rows.\nContents: De-identified line-level data for COVID-19 cases (date of symptom onset, demographic info, comorbidities, etc.).\nPotential Use Case: Potential to pair large-scale case counts or demographic breakdowns with references describing COVID-19 interventions or analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#daily-census-tract-level-ozone-concentrations-2001-2015",
    "href": "02-data-collection.html#daily-census-tract-level-ozone-concentrations-2001-2015",
    "title": "3  Data Collection",
    "section": "5.7 Daily Census Tract level Ozone Concentrations 2001-2015",
    "text": "5.7 Daily Census Tract level Ozone Concentrations 2001-2015\n\nsize: 27.62 GB, 412 million rows\ncolnames: “year” “date” “statefips” “countyfips” “ctfips” “latitude” “longitude” “DS_O3_pred” “DS_O3_stdd”\nContents: census tract level daily Ozone metrics\nPotential use Case: Time-Series or Panel Regression\n\nlongitudinal or panel model that includes each county across multiple years.\nrun a machine-learning approach (like a random forest regression or XGBoost) to see if ozone is predictive of heart disease/stroke mortality\nincorporate geospatial features (e.g., a “neighbors” matrix or geospatial embedding) in more advanced models\n\nSpatiotemporal Clustering or Hotspot Detection\n\nidentify “hotspots” of high ozone vs. “hotspots” of high stroke mortality over time.\nCompare them visually and quantitatively\ndo clusters of elevated ozone correlate with clusters of elevated stroke mortality?\n\nLag or Distributed‐Lag Models\n\nOften in environmental epidemiology, the effect of ozone on cardiovascular outcomes can have a short-term or delayed effect (like a lag of a few days or weeks)\nmight attempt simpler day-lag correlation or more advanced ML that includes “ozone from the previous X days/weeks” as features to predict mortality rates.\nbut the stroke data is annual, so might consider that year’s or previous year’s average ozone as input.\n\nTime Series + County Heterogeneity\n\nUse a method that accounts for repeated measures by county.\n\nE.g., a random forest where the rows are “(county, year)” with features:\n\nAnnual average ozone or “peak ozone days.”\nDemographics (like median age, % race/ethnicity, etc.).\nYear or region as a factor\nThe label is the heart disease or stroke mortality rate.\nEvaluate variable importance or partial dependence to see how ozone contributes to predicted mortality",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#rates-and-trends-in-heart-disease-and-stroke-mortality-among-us-adults-stratified-by-age-group-race-and-ethnicity-and-sex-200-2009",
    "href": "02-data-collection.html#rates-and-trends-in-heart-disease-and-stroke-mortality-among-us-adults-stratified-by-age-group-race-and-ethnicity-and-sex-200-2009",
    "title": "3  Data Collection",
    "section": "5.8 Rates and Trends in Heart Disease and Stroke Mortality Among US Adults Stratified by age group, race and ethnicity, and sex (200-2009)",
    "text": "5.8 Rates and Trends in Heart Disease and Stroke Mortality Among US Adults Stratified by age group, race and ethnicity, and sex (200-2009)\n\nsize: 1.21 GB, ~ 5.7 million rows\ncontents: yearly county level data on stroke and heart disease stratefied by age group, race, and sex. - datavalue is continuous variable - data value units are percent and per 100,000 - data value types are age-standardized, spatiotemporally smoothed rate, total percent change - some data are suppressed - included high and low confidence limits - 3 stratification categories",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#topic-modeling-research-gaps",
    "href": "02-data-collection.html#topic-modeling-research-gaps",
    "title": "3  Data Collection",
    "section": "7.1 Topic Modeling & Research Gaps",
    "text": "7.1 Topic Modeling & Research Gaps\n\nA key new step is topic modeling and clustering to discover underexplored areas\n\n\nAggregate or fetch relevant PDFs (via PubMed, OpenAlex, Unpaywall) on ozone, cardiovascular disease (CVD), stroke, environmental epidemiology, and machine learning for big data\nExtract text from each PDF (as before) and generate embeddings or tokenized data.\nRun topic modeling/clustering (e.g., LDA or vector-based K-Means) on the corpus, producing clusters or “topics” that summarize major themes (e.g., “Ozone exposure in older adults,” “Regional NAAQS compliance,” “Machine-learning approaches in environmental epidemiology,” etc.).\nuse the LLM to label each topic, provide short descriptions, and highlight any “weakly populated” or contradictory clusters indicating knowledge gaps.\nFocus on those gaps or subtopics by prompting the LLM to propose additional search queries (“What about ozone exposures specifically in understudied areas or populations, like rural counties or certain demographics?”).\nRe-run the ETL pipeline with these new queries, pulling more targeted literature, then incrementally building a corpus that can inform the investigation on ozone + stroke.\n\nIn short, the system helps expose poorly covered or inconclusive domains, which we can then map to the ozone/stroke data to propose new research questions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#etl-phase",
    "href": "02-data-collection.html#etl-phase",
    "title": "3  Data Collection",
    "section": "7.2 ETL Phase",
    "text": "7.2 ETL Phase\n\nPrevious::\n\nThe existing ETL script collects PDFs by searching for broad keywords (ozone, heart disease, machine learning, etc.) and extracts text and metadata into Zotero.\n\nUpdated:\n\nInitial Query & Collection\n\nstill do a broad search (like “ozone + cardiovascular + epidemiology + big data”)\nget 100–200 PDFs or as many as feasible\n\nExtract + Store\n\nParse text from PDFs, store references in Zotero, and push embeddings into vector DB\n\nTopic Modeling\n\nUse an LDA or clustering script to generate 8–15 major topics.\nAn LLM interprets each topic, describing key coverage and potential missing angles (like “very few articles mention stroke in low ozone areas,” or “lack of cluster on chronic ozone exposure among pregnant women”).\n\nRefined Query Generation\n\nThe LLM proposes narrower queries to fill these specific gaps.\nETL re-crawls PubMed/OpenAlex, again extracts new PDFs, and merges them into the corpus\n\nRepeat until we have a sufficiently deep literature set.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#rag-phase-linking-literature-to-the-ozone-stroke-data",
    "href": "02-data-collection.html#rag-phase-linking-literature-to-the-ozone-stroke-data",
    "title": "3  Data Collection",
    "section": "7.3 RAG Phase: Linking Literature to the Ozone & Stroke Data",
    "text": "7.3 RAG Phase: Linking Literature to the Ozone & Stroke Data\n\nWith the final curated corpus in place:\n\n\nEmbedding & Retrieval\n\n\n\nChunk each paper’s text and store embeddings\nUse RAG to respond to user queries: “How does long-term ozone exposure affect stroke mortality in Southeastern states?”\n\n\nChain-of-Thought + QA\n\n\nThe LLM consults the newly enriched literature set.\nIt merges textual references with the numeric (ozone/stroke) data if needed.\nWe see how well the existing research addresses my specific question; if coverage is sparse, that reaffirms a research gap.\n\n\nConnecting to the Ozone & Stroke Datasets\n\nBecause we have daily census-tract ozone data and county-level stroke mortality data, the system can highlight “No strong consensus or study” in certain subregions or demographic slices. If the LLM-based retrieval reveals minimal or contradicting references, we have a candidate for novel analysis: we can directly apply a big data approach to see if there is an actual association in those regions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#potential-for-publication",
    "href": "02-data-collection.html#potential-for-publication",
    "title": "3  Data Collection",
    "section": "7.4 Potential for Publication",
    "text": "7.4 Potential for Publication\nThis dissertation is designed as a foundational research program with multiple avenues for scholarly publication, each corresponding to a specific research aim.\n1. A Methodological Contribution (from Aim 1): The AI-powered ETL+RAG pipeline itself represents a significant methodological innovation. The architecture, custom-developed modules, and the framework for systematic gap analysis are suitable for publication in a journal focused on computational social science, digital scholarship, or data science (Journal of Open Source Software, Data Science Journal, etc.). This paper would detail the system’s technical architecture and demonstrate its utility.\n2. An Environmental Epidemiology Study (from Aim 2): The baseline analysis of the relationship between county-level ozone exposure and cardiovascular disease mortality constitutes a classic epidemiological study. The findings, contextualized by the systematic literature review, would be suitable for submission to a public health or environmental science journal (Environmental Health Perspectives, American Journal of Epidemiology, etc.), contributing to the existing body of knowledge on ozone’s health impacts.\n3. A High-Impact Deep Learning Application (from Aim 3): This aim offers at least two high-impact publication opportunities:\n\nA Technical Paper: The novel deep learning framework for identifying and monitoring UOG sites from multi-modal satellite data is a significant contribution to the fields of remote sensing and computer science. This paper would be targeted at a leading journal in those areas (Remote Sensing of Environment, IEEE Transactions on Geoscience and Remote Sensing, etc.).\nA Novel Epidemiological Study: The spatiotemporal inventory of UOG sites produced by the framework will be a unique and valuable dataset. A subsequent paper using this inventory to conduct a preliminary analysis of the association between UOG site proximity/activity and public health outcomes would represent a novel contribution to environmental health.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#future-research-horizons-and-technical-scaling",
    "href": "02-data-collection.html#future-research-horizons-and-technical-scaling",
    "title": "3  Data Collection",
    "section": "7.5 Future Research Horizons and Technical Scaling",
    "text": "7.5 Future Research Horizons and Technical Scaling\nThe completion of this dissertation will serve as the basis for future research directions and presents opportunities for significant technical scaling.\n\n7.5.1 Technical Scaling and Computational Frontiers\n\nDistributed Processing of Satellite Imagery: The methodology for Aim 3, while developed on a powerful single-node system, is designed for scalability. Future work will involve adapting the data preprocessing and CNN inference pipelines to run on distributed computing frameworks like Dask or Spark, enabling analysis of satellite imagery at a continental or global scale.\nAdvanced Distributed Model Training: To further accelerate the training of the deep learning models, future iterations will explore distributed training strategies ( namely PyTorch’s DistributedDataParallel) across multiple GPU nodes on a high-performance computing (HPC) cluster.\nFine-Tuning Domain-Specific LLMs: The ETL+RAG system generates a rich dataset of curated questions, contexts, and answers. A key future direction is to use this data to fine-tune a smaller, open-source language model (Llama 3, Mistral, Qwen3, etc.) specifically for environmental health literature, creating a highly specialized and efficient “expert” model.\n\n\n\n7.5.2 Future Research Directions\n\nIntegrated Multi-Pollutant Health Models: The datasets from Aim 2 (ozone) and Aim 3 (UOG/methane) can be combined to build sophisticated multi-pollutant models. Future research will investigate the synergistic or interactive effects of co-exposure to both ambient ozone and point-source pollutants from UOG sites on cardiovascular health.\nPublic-Facing Environmental Data Platforms: A major goal is to translate the research outputs into tools with direct societal impact. Future work could involve developing an interactive web-based dashboard that allows the public, researchers, and policymakers to explore the spatiotemporal UOG inventory developed in Aim 3.\nExpansion to New Domains: The core ETL+RAG pipeline will be applied to new research domains to continuously test and validate its generalizability, such as analyzing literature on climate change adaptation strategies or emerging infectious diseases.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#summary-of-assembled-data-assets",
    "href": "02-data-collection.html#summary-of-assembled-data-assets",
    "title": "3  Data Collection",
    "section": "7.6 Summary of Assembled Data Assets",
    "text": "7.6 Summary of Assembled Data Assets\nThe data collection and management phase of this dissertation has successfully outlined the acquisition and preparation of three distinct, large-scale data assets, each corresponding to a specific research aim:\n\nA Scholarly Literature Corpus: A comprehensive body of scientific articles from PubMed, OpenAlex, and Scopus, which serves as the primary input for the methodological gap analysis in Aim 1.\nPublic Health and Environmental Data: A rich set of county and census-tract level data from the CDC, including historical ozone concentrations and cardiovascular disease mortality rates. This forms the foundation for the baseline epidemiological modeling in Aim 2.\nRemote Sensing and Geospatial Data: A planned collection of high-resolution optical and methane satellite imagery, supplemented by contextual LULC and tax parcel data. This will be the primary input for the novel deep learning framework developed in Aim 3.\n\nWith these foundational data assets collected, processed, and managed, the subsequent chapters of this dissertation will proceed with the analytical execution of the research aims. We will begin with the systematic literature review informed by Aim 1, followed by the empirical analyses detailed in Aims 2 and 3.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#load-libraries-and-set-up-environment",
    "href": "02-data-collection.html#load-libraries-and-set-up-environment",
    "title": "3  Data Collection",
    "section": "9.1 Load Libraries and Set Up Environment",
    "text": "9.1 Load Libraries and Set Up Environment\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(knitr)\nlibrary(flextable)\nlibrary(skimr)\nlibrary(Hmisc)\nlibrary(soiltexture)\nlibrary(data.table)\nlibrary(arrow)       # For Feather export\nlibrary(parallel)\nlibrary(lubridate)\n\n\nWe load the parallel package for multi-core processing, arrow for Feather file writing, and data.table (via fread) to handle large CSVs efficiently.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#importing-the-data",
    "href": "02-data-collection.html#importing-the-data",
    "title": "3  Data Collection",
    "section": "9.2 Importing the Data",
    "text": "9.2 Importing the Data\nBelow, we read the relevant CDC datasets. Note that some files exceed 10 GB, so fread() or chunk-based reading is used. Adjust paths as needed.\n\n\nCode\n# PLACES data\nplaces_gis_friendly_2020 &lt;- read.csv(\"data/PLACES_Place_Data_GIS_Friendly_Format_2020_release.csv\")\nplaces_gis_ZCTA_2020     &lt;- read.csv(\"data/PLACES_ZCTA_Data_GIS_Friendly_Format_2020_release.csv\")\n\n# SDOH measures for ZCTA ACS 2017-2021\nSDOH_measures_for_zcta_acs &lt;- read.csv(\"data/SDOH_Measures_for_ZCTA_ACS_2017-2021.csv\", header = TRUE, sep = \",\")\n\n# COVID-19 case surveillance with geography\ncovid_19_case_surveillance &lt;- as.data.frame(fread(\"data/COVID-19_Case_Surveillance_Public_Use_Data_with_Geography.csv\"))\n\n# daily PM 2.5 county level 2001-2014\ndaily_pm_2_5 &lt;- read.csv(\"data/Daily_County-Level_PM2.5_Concentrations_2001-2014.csv\", header = TRUE, sep = \",\")\n\n# Heart disease & stroke mortality\nstroke_mortality_state_county &lt;- read.csv(\"data/Stroke_Mortality_Data_Among_US_Adults_35_by_State_Territory_and_County_2019-2021.csv\", header = TRUE, sep = \",\")\n\ncoronary_heart_disease_stroke &lt;- read.csv(\"data/Rates_and_Trends_in_Coronary_Heart_Disease_and_Stroke_Mortality_Data_Among_US_Adults_35_by_County_1999-2018.csv\")\nheartdisease_stroke_mortality_adults &lt;- as.data.frame(\n  fread(\"data/Rates_and_Trends_in_Heart_Disease_and_Stroke_Mortality_Among_US_Adults_35_by_County_Age_Group_Race_Ethnicity_and_Sex_2000-2019.csv\")\n)\n\n# Ozone data can be huge, so we use fread\ndaily_census_tract_ozone_2001_2005 &lt;- as.data.frame(\n  fread(\"data/Daily_Census_Tract-Level_Ozone_Concentrations_2001-2005.csv\")\n)\ndaily_census_tract_ozone_2006_2010 &lt;- as.data.frame(\n  fread(\"data/Daily_Census_Tract-Level_Ozone_Concentrations_2006-2010.csv\")\n)\ndaily_census_tract_ozone_2011_2014 &lt;- as.data.frame(\n  fread(\"data/Daily_Census_Tract-Level_Ozone_Concentrations_2011-2014.csv\")\n)\n\n\nNote that eval is set to false, these are not files or code lines that someone may want to accidentally start",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#summaries-with-multi-threading",
    "href": "02-data-collection.html#summaries-with-multi-threading",
    "title": "3  Data Collection",
    "section": "9.3 Summaries with Multi-Threading",
    "text": "9.3 Summaries with Multi-Threading\nNext, we define helper functions to summarize each column, then use parLapply to parallelize. We demonstrate it first on the heart disease & stroke dataset, then on one portion of the ozone dataset.\n\n\nCode\n# Define summarization for typical numeric/factor columns\nsummarize_column &lt;- function(x, threshold = 20) {\n  unique_vals &lt;- sort(unique(x))\n  if (is.numeric(x) && length(unique_vals) &gt; threshold) {\n    summ &lt;- summary(x)\n    return(paste(\"Continuous variable summary:\",\n                 \"Min:\", summ[\"Min.\"],\n                 \"1st Qu.:\", summ[\"1st Qu.\"],\n                 \"Median:\", summ[\"Median\"],\n                 \"Mean:\", round(as.numeric(summ[\"Mean\"]), 2),\n                 \"3rd Qu.:\", summ[\"3rd Qu.\"],\n                 \"Max:\", summ[\"Max.\"]))\n  } else {\n    return(paste(unique_vals, collapse = \", \"))\n  }\n}\n\n# For ozone data (which can have many numeric columns)\nsummarize_column_ozone &lt;- function(x, threshold = 20, sample_size = 20) {\n  unique_vals &lt;- sort(unique(x))\n  if (is.numeric(x) && length(unique_vals) &gt; threshold) {\n    summ &lt;- summary(x)\n    return(paste(\"Continuous variable summary:\",\n                 \"Min:\", summ[\"Min.\"],\n                 \"1st Qu.:\", summ[\"1st Qu.\"],\n                 \"Median:\", summ[\"Median\"],\n                 \"Mean:\", round(as.numeric(summ[\"Mean\"]), 2),\n                 \"3rd Qu.:\", summ[\"3rd Qu.\"],\n                 \"Max:\", summ[\"Max.\"]))\n  } else if(length(unique_vals) &gt; sample_size) {\n    sampled_vals &lt;- sample(unique_vals, sample_size)\n    return(paste(sampled_vals, collapse = \", \"))\n  } else {\n    return(paste(unique_vals, collapse = \", \"))\n  }\n}\n\n# Create a cluster\nn_cores &lt;- detectCores() - 2\ncl &lt;- makeCluster(n_cores)\n\n# Summarize columns of heart disease/stroke data\nclusterExport(cl, varlist = c(\"summarize_column\"), envir = environment())\nresult_list_hd &lt;- parLapply(cl, heartdisease_stroke_mortality_adults, summarize_column)\nresult_hd      &lt;- unlist(result_list_hd)\nresult_df_hd   &lt;- data.frame(Column = names(result_hd),\n                             Summary = as.character(result_hd),\n                             stringsAsFactors = FALSE)\nprint(head(result_df_hd, 10))  # just preview\n\n# Summarize columns for 2001-2005 ozone data\nclusterExport(cl, varlist = c(\"summarize_column_ozone\"), envir = environment())\nresult_list_o &lt;- parLapply(cl, daily_census_tract_ozone_2001_2005, summarize_column_ozone)\nresult_o      &lt;- unlist(result_list_o)\nresult_df_o   &lt;- data.frame(Column = names(result_o),\n                            Summary = as.character(result_o),\n                            stringsAsFactors = FALSE)\nprint(head(result_df_o, 10))\n\nstopCluster(cl)\n\n\nAbove, you can see how we parallelize the summary using parLapply.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#merge-standardize-ozone-data",
    "href": "02-data-collection.html#merge-standardize-ozone-data",
    "title": "3  Data Collection",
    "section": "9.4 Merge & Standardize Ozone Data",
    "text": "9.4 Merge & Standardize Ozone Data\nThe next step is standardizing column names, converting date strings with lubridate, and merging all three ozone subsets into a single data.table. Then we write the final table to a Feather file.\n\n\nCode\n# Example: unify column names for the 2011-2014 dataset\nsetnames(daily_census_tract_ozone_2011_2014,\n         old = c(\"ds_o3_pred\", \"ds_o3_stdd\"),\n         new = c(\"DS_O3_pred\", \"DS_O3_stdd\"))\n\n# Put the three ozone data frames in a list\nozone_list &lt;- list(\n  daily_census_tract_ozone_2001_2005,\n  daily_census_tract_ozone_2006_2010,\n  daily_census_tract_ozone_2011_2014\n)\n\n# Convert date using parallel again\nconvert_date &lt;- function(df) {\n  df$date &lt;- lubridate::dmy(df$date)\n  df\n}\n\ncl2 &lt;- makeCluster(detectCores() - 2)\nclusterEvalQ(cl2, library(lubridate))\nclusterExport(cl2, varlist = c(\"convert_date\"), envir = environment())\n\nozone_list_converted &lt;- parLapply(cl2, ozone_list, convert_date)\nstopCluster(cl2)\n\n# Merge with rbindlist\ndaily_ozone_all &lt;- data.table::rbindlist(ozone_list_converted, use.names = TRUE, fill = TRUE)\n\n# Quick check\nsummary(daily_ozone_all$date)\n\n# Save as feather (reduces from ~27 GB to ~13 GB)\narrow::write_feather(daily_ozone_all, \n  \"data/daily_census_tract_ozone_2001_2014.feather\"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "02-data-collection.html#export-other-datasets-as-feather",
    "href": "02-data-collection.html#export-other-datasets-as-feather",
    "title": "3  Data Collection",
    "section": "9.5 Export Other Datasets as Feather",
    "text": "9.5 Export Other Datasets as Feather\nFinally, any large data frames (such as heart disease/stroke mortality) can also be exported:\n\n\nCode\narrow::write_feather(\n  heartdisease_stroke_mortality_adults, \n  \"data/HeartDisease_StrokeMortality.feather\"\n)\n\n\n\nThat’s it for now in terms of describing the possible data collection approach. As the project evolves, I might refine exactly how these datasets are integrated and whether I keep them in a separate Big Data environment or incorporate them directly into the pipeline.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Collection</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html",
    "href": "03-literature-review.html",
    "title": "4  Literature Review",
    "section": "",
    "text": "4.1 Introduction:\nThe rapidly expanding volume of scientific literature presents considerable challenges for researchers, particularly in complex, interdisciplinary fields such as geospatial epidemiology, where manually synthesizing vast amounts of information to identify emergent methodological trends and pinpoint specific research gaps can be exceptionally demanding and time-consuming. To address these limitations, an automated pipeline leveraging Extract, Transform, Load (ETL) processes and Retrieval-Augmented Generation (RAG) has been developed as outlined in our broader project aimed at enhancing large-scale academic literature analytics and methodological gap analysis. This system is designed to systematically process extensive collections of academic articles, extract structured text and key information including domain-specific metrics and methodologies, and thereby facilitate the efficient identification of under-explored research areas and novel or underutilized analytical techniques.\nThis literature review serves as a practical application and demonstration of the aforementioned AI-powered pipeline’s capabilities in navigating a complex research domain. The chosen area for this demonstration is the intersection of ambient ozone exposure, cardiovascular (specifically heart) disease outcomes, and the application of geospatial and machine learning methodologies, with a particular emphasis on deep learning techniques such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs). This thematic focus was strategically selected not only due to the pressing public health questions involved but also facilitated by prior access to extensive, highly granular datasets, including daily census-tract level ozone concentrations and CDC cardiovascular disease mortality data stratified by age group, sex, and race. Initial explorations using the pipeline further sharpened this focus by highlighting a notable underutilization of sophisticated neural network architectures like CNNs and GNNs in studies specifically linking ozone exposure to cardiovascular disease, despite their considerable potential for modeling intricate spatiotemporal relationships.\nTherefore, the objectives of this review are: (1) to present a comprehensive overview of the current research landscape concerning ozone, heart disease, and the use of geospatial and machine learning methods, particularly CNNs and GNNs, as identified and synthesized through our automated pipeline; (2) to detail the methodological trends, common practices, and critically, the specific research and methodological gaps revealed by this AI-assisted analysis, such as the limited application of deep learning or the scarcity of studies focusing exclusively on ozone and cardiovascular disease; and (3) to thereby provide an evidence-based foundation that will directly inform and guide subsequent empirical research, specifically the planned application of novel or underutilized analytical techniques to address these identified gaps.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#sec-methods",
    "href": "03-literature-review.html#sec-methods",
    "title": "4  Literature Review",
    "section": "4.2 Methods",
    "text": "4.2 Methods\nThis literature review employed a semi-automated pipeline, leveraging a custom-built Extract, Transform, Load (ETL) process and a Neo4j knowledge graph, to identify, screen, and categorize relevant literature. The entire workflow, from data acquisition to final study selection and characterization, was designed to be transparent and reproducible. The study selection process adapted the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines Figure 4.1.\n\n\n\n\n\n\nFigure 4.1: PRISMA Flow Diagram illustrating the literature selection process. Reasons for exclusion: Reason 1: Missing study type or study type null (n=145); Reason 2: Non-empirical study (e.g., review, meta-analysis, opinion, report) (n=60); Reason 3: Empirical study not using machine learning methods (n=159); Reason 4: Empirical ML study not focused on both ozone AND heart disease (n=39). 1260 records identified, 821 not retrieved, 439 assessed for eligibility, 403 excluded, and 36 studies included, with a further 2 manually excluded, resulting in 34 final studies.\n\n\n\n\n4.2.1 Eligibility Criteria\nThis review focused on studies investigating the intersection of ambient ozone exposure, cardiovascular (specifically heart) disease outcomes, and the application of geospatial and/or machine learning methodologies. To be included, studies had to: 1. Be empirical research (not reviews, commentaries, meta-analyses, reports, dissertations, etc.). 2. Explicitly identify ozone as a pollutant of interest. 3. Report an association with or investigation of heart disease or broader cardiovascular disease outcomes. 4. Employ one or more machine learning techniques in their analysis or modeling. 5. Be published as full-text articles. Studies focusing exclusively on non-ozone pollutants, non-cardiovascular outcomes, or those not utilizing machine learning were excluded. Commentaries and replies to commentaries were also excluded during a final review stage.\n\n\n4.2.2 Information Sources and Search Strategy\nA literature search was conducted across three primary scholarly databases: Scopus (via Elsevier API), OpenAlex, and PubMed. To ensure comprehensive coverage of the multidisciplinary research area, over 30 distinct search query combinations were developed (See Appendix E Appendix E). These queries combined keywords and concepts related to “ozone exposure,” “heart disease,” “cardiovascular disease,” “geospatial epidemiology,” “spatiotemporal analysis,” “machine learning,” and various specific geospatial and ML techniques (e.g., “GIS,” “Random Forest,” “neural networks”). The complete list of search query terms is provided in Supplementary Appendix E.\nThe execution of these searches was automated using custom Python modules (fast_pubmed.py, fast_openalex.py, etl_elsevier.py), which facilitated efficient querying and metadata retrieval from each database API.\n\n\n4.2.3 Study Selection and Data Management\n\n4.2.3.1 Initial Record Collection and PDF Retrieval\nThe combined searches initially yielded 1,260 records (see Figure 4.1). Metadata for these records, including DOI, title, authors, retraction status, citation counts, abstract (where available), and publication details, were programmatically collected.\nFull-text PDF acquisition was then attempted for all identified records. This was automated using available DOIs to query Unpaywall (via async_unpaywall.py) and by extracting direct PDF links from Crossref metadata (obtained through the Elsevier and OpenAlex data fetching modules). This process successfully retrieved 439 unique full-text PDF articles. Records for which full-text PDFs could not be accessed (n=821) were excluded at this stage.\n\n\n4.2.3.2 Data Extraction and Knowledge Graph Preprocessing\nThe 439 retrieved PDF articles were processed through a custom ETL pipeline to extract and structure relevant information:\n\nText and Content Extraction: Full textual content, along with structured elements such as tables and LaTeX formulas, was extracted using a multiprocessing-optimized pipeline leveraging Docling’s machine learning layout analysis capabilities (docling_extract_formulas_mp_multi.py).\nLLM-based Field Extraction: Key metadata fields, study characteristics (e.g., study type, pollutant terms, heart diseases mentioned, ML methods used), and other relevant entities were identified and extracted from the text using a Large Language Model (LLM)-based pipeline (field_extraction.py). This process included robust JSON parsing, response caching, and dynamic text chunking.\nTerminology Unification: Extracted terms across various fields were standardized using a comprehensive set of custom synonym dictionaries (managed via dictionaries_gui.py), employing semantic similarity against precomputed embeddings to map variant terms to canonical labels. This step was crucial for ensuring consistency in the subsequent knowledge graph.\nTopic Modeling: To further characterize the thematic landscape of the 439 retrieved full-text articles, topic modeling was performed using Latent Dirichlet Allocation (LDA) facilitated by our topic_modeling_culda.py module. This process involved robust text preprocessing (including tokenization, stopword removal, and lemmatization) and n-gram detection. Hyperparameter tuning for the LDA model was conducted to optimize topic coherence, resulting in a final model with 26 topics and a coherence score (CV) of approximately 0.61. The specific optimized hyperparameters included settings for the number of passes, iterations, n-gram thresholds, and document frequency cutoffs (best params: {'num_topics': 26, 'passes': 65, 'iterations': 200, 'bigram_min_count': 3, 'bigram_threshold': 84, 'trigram_threshold': 140, 'no_below': 7, 'no_above': 0.5}). This procedure successfully generated interpretable thematic labels for the identified topics (e.g., ‘Air Quality Forecasting Models’, ‘Satellite Aerosol Monitoring Uncertainty’, ‘Geospatial Health Risk Mapping’), providing an additional layer of content characterization for the broader corpus prior to final eligibility assessment.\nData Structuring: All extracted and processed information was compiled into structured Feather files.\n\n\n\n4.2.3.3 Knowledge Graph Construction\nThe structured data from the 439 articles was integrated into a Neo4j knowledge graph using a dedicated Python script (kg_pipeline_gui.py). This script created nodes for articles (identified by DOI) and linked them to nodes representing entities such as Study Types, Pollutant Terms, Heart Diseases, ML Methods, and Analytical Tools, based on the extracted and unified data. Relationships between these entities were explicitly modeled (e.g., (Article)-[:STUDY_TYPE]-&gt;(StudyType)). Uniqueness constraints were applied to key node properties (e.g., Article.doi, Author.canonicalName) to ensure data integrity.\n\n\n4.2.3.4 Eligibility Assessment using the Knowledge Graph\nThe Neo4j knowledge graph served as the primary tool for the systematic application of eligibility criteria to the 439 full-text articles. A series of Cypher queries were developed to categorize articles and identify those meeting the review’s inclusion criteria. This knowledge graph-driven approach allowed for precise and reproducible filtering.\nThe exclusion criteria, applied sequentially based on the comprehensive categorization query (see Appendix D, Section Section D.3), were as follows:\n\nArticles with missing or null study type information (n=145): Excluded if a study type could not be robustly determined or was absent in the knowledge graph.\nNon-empirical studies (n=60): Articles classified with study types such as ‘review’, ‘systematic review’, ‘meta-analysis’, ‘expert opinion’, ‘report’, ‘dissertation/thesis’, ‘short communication’, ‘methodological paper’, or ‘theoretical study’. Articles were prioritized into this category if any of their identified study types matched this list.\nEmpirical studies not using Machine Learning methods (n=159): Articles identified as empirical but lacking an association with any ML Method node in the knowledge graph.\nEmpirical ML studies not focused on both ozone AND heart disease (n=39): Articles that were empirical and used ML but did not have established links to both ‘ozone’ (as a PollutantTerm) AND a relevant HeartDisease node.\n\nThis graph-based filtering process initially yielded 36 potentially eligible studies.\n\n\n\n4.2.4 Final Included Studies and Data Synthesis\nThe 36 studies identified through the knowledge graph queries underwent a final manual review of their titles and abstracts to ensure they represented primary research contributions. At this stage, two articles were excluded as they were determined to be a commentary and a reply to a commentary, respectively.\nThis resulted in a final set of 34 unique studies being included in the literature review. Data on study characteristics, pollutants, machine learning methods, heart diseases, and other analytical tools for these 34 studies were extracted from the knowledge graph (see Appendix D, Section Section D.4 for detailed distributions and queries). The synthesis of findings from these studies focused on identifying methodological gaps and common practices in the application of geospatial and machine learning techniques to ozone and heart disease research. Bibliographic data for all processed articles, including the final included set, were managed using Zotero, facilitated by programmatic integration (fast_zotero.py).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#health-impacts-of-ozone",
    "href": "03-literature-review.html#health-impacts-of-ozone",
    "title": "4  Literature Review",
    "section": "4.3 Health Impacts of Ozone:",
    "text": "4.3 Health Impacts of Ozone:\nEmpirical evidence indicates that ozone exposure affects the cardiovascular system. Short-term increases in ambient O₃ have been associated with modest but statistically significant alterations in cardiovascular physiological markers, such as decreases in systolic and diastolic blood pressure in middle-aged and older adults (Tang et al. 2024). At the population level, interventions targeting traffic emissions that resulted in a 5.8% reduction in ambient ozone concentrations were directly correlated with a 5.9% decline in cardiovascular mortality, suggesting that lowering ozone exposure can lead to fewer cardiovascular deaths (Burns et al. 2019). These findings highlight ozone’s role in cardiovascular outcomes, from acute physiological changes to mortality.\nMechanistic studies have begun to elucidate the pathways through which ozone exposure may contribute to cardiovascular disease. Ozone has been shown to induce the oxidation of cholesterol, generating oxysterols that can disrupt lipid metabolism regulated by Liver X Receptors and form pro-atherogenic lipid-protein adducts; these ozonolysis products have also been identified within human atherosclerotic plaques Furthermore, ozone inhalation can provoke systemic inflammatory responses, including peripheral neutrophilia, which can contribute to vascular injury (Perryman et al. 2023). Additionally, ozone exposure may trigger airway sensory nerves, leading to autonomic reflexes that influence vascular tone and blood pressure, representing another hemodynamic pathway relevant to cardiac stress (Tang et al. 2024).\nDespite these findings, significant epidemiological gaps remain in understanding the full impact of ozone on cardiovascular mortality. Short-term studies have yielded inconsistent results, with some analyses finding no significant association between ozone and acute cardiovascular hospitalizations (Fasola et al. 2021). Several large prospective cohort studies, while assessing other pollutants like PM₂.₅, have omitted analyses of ozone’s impact on cardiovascular outcomes despite collecting exposure data (C. Liu et al. 2022). Intervention reviews often focus on particulate matter, with the specific contribution of ozone reductions to cardiovascular mortality not always isolated. Moreover, the accuracy of ozone exposure assessment can be compromised by limited monitoring infrastructure, potentially affecting the reliability of epidemiological findings (Tan et al. 2022). Finally, many ozone-focused studies analyze intermediate cardiovascular endpoints, such as blood pressure changes, rather than direct mortality (Tang et al. 2024), underscoring the need for further research with robust exposure assessment and explicit analysis of cardiovascular mortality.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#geospatial-epidemiological-methods",
    "href": "03-literature-review.html#geospatial-epidemiological-methods",
    "title": "4  Literature Review",
    "section": "4.4 Geospatial Epidemiological Methods:",
    "text": "4.4 Geospatial Epidemiological Methods:\nTraditional geospatial methods in ozone and cardiovascular epidemiology have historically included a range of spatial-statistical tools to estimate exposure. These have encompassed regression models, notably land-use regression (LUR), universal kriging for spatial interpolation, and various dispersion models such as CALINE3, CALINE4, AERMOD, and other Gaussian or Lagrangian models to predict pollutant concentrations at specific locations (Xie et al. 2017). Such approaches formed the basis for assigning ozone exposure levels in early studies examining its association with heart disease.\nRecent advancements have introduced more sophisticated geospatial techniques to enhance the accuracy and resolution of ozone exposure assessment. These include high-resolution spatial and spatiotemporal modelling that integrates data from fixed-site and mobile monitoring, often employing LUR and Generalized Additive Models (Clark et al. 2024). Concurrently, modern analytical tools such as geodetectors, wavelet transformation, and photochemical trajectory models, alongside machine learning methods like support vector machines and kernel extreme learning machines, are being utilized to diagnose spatial heterogeneity and improve predictive accuracy (Tan et al. 2022). Furthermore, epidemiological designs, such as geocoded case-crossover studies, now incorporate fine-scale exposure grids, with resolutions down to 200 meters, combined with methods like conditional logistic regression, Random Forest machine learning, and distributed lag models to assess acute cardiovascular impacts (Fasola et al. 2021).\nDespite these advancements, significant methodological limitations persist in current geospatial epidemiological methods for ozone exposure assessment. The accuracy of data-fusion methods used to create ozone maps can be compromised by sparse ground monitoring networks, leading to reduced precision in areas with few monitoring sites. The reviewed studies are also limited by the application of a single spatial scale, thereby neglecting the influence of multi-scale landscape effects on ozone formation and distribution (Tan et al. 2022). Moreover, some forecasting models, such as diffusion convolutional recurrent neural networks, may oversimplify meteorological drivers by, for instance, representing dynamic wind direction as a static average vector (Wang et al. 2022). Finally, in multi-pollutant epidemiological models, multicollinearity among co-pollutants can obscure or falsely attribute effects to ozone, complicating causal inference regarding its specific impact on cardiovascular health (Shi et al. 2021).\nNotable recent methodological advancements are addressing some of these challenges, particularly in large-scale cohort studies. For example, the integration of national high-resolution reanalysis datasets, such as the CAQRA (China High Air Pollutants Reanalysis) product, provides comprehensive spatial coverage for exposure assessment. These high-resolution exposure surfaces are increasingly combined with statistical tools like distributed-lag models (DLM) to capture delayed health effects and Bayesian kernel machine regression (BKMR) to assess complex, non-linear relationships and interactions within multi-pollutant mixtures. Such approaches have been effectively employed to quantify the short-term effects of ozone exposure on cardiovascular-related physiological indices, like blood pressure, in large populations (Tang et al. 2024).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#machine-learning-in-ozone-epidemiology",
    "href": "03-literature-review.html#machine-learning-in-ozone-epidemiology",
    "title": "4  Literature Review",
    "section": "4.5 Machine Learning in Ozone Epidemiology:",
    "text": "4.5 Machine Learning in Ozone Epidemiology:\n\n4.5.1 Frequently Applied Machine Learning Techniques\nMachine learning methodologies are increasingly employed in ozone epidemiology, both for enhancing exposure assessment and for elucidating relationships with health outcomes, including cardiovascular disease. For exposure modeling, various algorithms are utilized to generate fine-scale spatiotemporal ozone concentration fields. These include ensemble methods like Random Forests, alongside Support Vector Regression, Deep Learning architectures, and Quantile Regression Forests, which have been compared for their efficacy in predicting ambient ozone levels at high resolutions (Zhu, Lee, and Stoner 2024). More deep learning approaches, such as Diffusion-Convolutional Recurrent Neural Networks (DCRNN), a type of graph neural network incorporating recurrent components like GRU or LSTM, have been developed specifically for regional forecasting of ozone and PM₂.₅ concentrations, aiming to capture complex dependencies in the data (Wang et al. 2022).\nIn the context of health outcome analysis, machine learning classifiers and non-linear models are applied to investigate the impact of ozone exposure. For instance, a pipeline utilizing gradient-boosting (XGBoost), Random Forest, Decision Tree, LightGBM, and Multilayer Perceptron (MLP) classifiers, along with SHAP for model interpretation, identified ozone as a top predictor of hypertension, achieving an AUROC of 0.710 for this metabolic disease (J. Liu et al. 2025). Furthermore, Bayesian Kernel Machine Regression (BKMR), a non-linear machine learning technique, has been applied in large cohort studies to assess the joint and individual effects of ozone and co-pollutants on cardiovascular physiological markers, such as blood pressure indices (Tang et al. 2024). These applications demonstrate a dual role for machine learning: refining exposure estimates and providing data-driven insights into ozone’s contribution to cardiovascular risk.\n\n\n4.5.2 Limitations of Conventional Machine Learning Methods\nConventional machine learning approaches, particularly tree-based ensemble methods like Random Forest and XGBoost, face certain limitations when modeling the complex spatiotemporal dynamics of ozone. Evidence suggests that models explicitly designed to capture spatial dependencies and long-range temporal patterns, such as the graph-based DCRNN, achieve significantly lower errors in ozone forecasting compared to traditional baseline models. This improved performance is attributed to the DCRNN’s ability to incorporate spatial linkages in predicting ozone concentrations (Wang et al. 2022).\nFurthermore, review literature indicates that for gaseous pollutants like ozone, neural network-based approaches consistently outperform conventional algorithms (Rautela and Goyal 2024). This suggests that simpler tree ensembles may struggle to adequately model the complex non-linear atmospheric processes and chemical transport phenomena that govern ozone formation and distribution. While these observations point towards the inherent constraints of some conventional learners in handling intricate spatiotemporal ozone data, it is important to note that the provided evidence base does not contain direct quantitative evaluations or specific studies detailing the shortcomings of Random Forest or XGBoost in the context of spatio-temporal ozone prediction.\n\n\n4.5.3 Applications of CNN and GNN in Ozone and Cardiovascular Epidemiology\nWithin the provided evidence, specific applications of deep learning architectures like Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN) in ozone modeling are documented, although their direct application to cardiovascular outcome modeling appears limited. Wang et al. (2022) developed and employed a Diffusion Convolutional Recurrent Neural Network (DCRNN), which incorporates graph neural network principles, to forecast regional ozone concentrations. This approach leverages the network structure of monitoring stations to improve prediction accuracy. Separately, a dissertation, although exlucded at first, by Ferrer-Cid (2023) discusses the application of “Convolutional Neural Networks on Graphs” and more broadly “Graph Neural Networks” for enhancing data quality from low-cost air pollution sensor networks that monitor pollutants including ozone, noting that cardiovascular diseases fall within the general health impact domain of such pollution.\nHowever, despite these advancements in using CNN and GNN methodologies for ozone exposure assessment and sensor data processing, the provided observations do not contain studies that explicitly apply these specific deep learning techniques (CNN or GNN) to model the relationship between ozone exposure and direct cardiovascular disease outcomes. The existing applications focus on the environmental modeling aspect of ozone or mention cardiovascular health in a broader contextual sense rather than as a direct modeling endpoint for these neural network architectures.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#summary-of-gaps",
    "href": "03-literature-review.html#summary-of-gaps",
    "title": "4  Literature Review",
    "section": "4.6 Summary of Gaps:",
    "text": "4.6 Summary of Gaps:\n\n4.6.1 Current Gaps in CNN/GNN Application for Ozone-Cardiovascular Health Studies\nA significant gap exists in the application of Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN) to ozone-related cardiovascular epidemiology, primarily characterized by a disconnect between exposure modeling and health outcome assessment. Current implementations of GNN and CNN architectures within the ozone literature are almost exclusively focused on enhancing the spatio-temporal prediction of ozone concentrations. For example, diffusion-convolutional recurrent neural networks (DCRNN) have been developed for regional ozone forecasting (Wang et al. 2022), and reviews catalogue various spatio-temporal deep neural networks (ST-DNN), including CNN, LSTM, and graph-convolution LSTM (GC-LSTM), as tools for air quality prediction (Rautela and Goyal 2024). However, these sophisticated exposure modeling techniques are not subsequently integrated into frameworks that assess cardiovascular health impacts.\nIn contrast, epidemiological studies investigating the cardiovascular effects of ozone exposure predominantly rely on traditional statistical methodologies. Large-scale studies examining ozone’s influence on cardiovascular endpoints, such as blood pressure changes or hospitalizations, continue to employ linear mixed models, distributed lag models, or other conventional regression techniques (Tang et al. (2024); Fasola et al. (2021)). Evidence within the reviewed literature indicates no instances where CNN or GNN models are directly applied to analyze the relationship between ozone exposure and cardiovascular outcomes. This highlights a critical lack of integrated CNN/GNN pipelines that would couple high-resolution, machine-learning-driven ozone exposure predictions with cardiovascular health data, which limits the potential to capture complex non-linear exposure-response relationships and network-level effects.\n\n\n4.6.2 Calls for Advanced CNN/GNN Spatial-Temporal Methods\nThe reviewed literature identifies the need for more advanced spatial-temporal modeling techniques, such as CNN and GNN, particularly for improving air quality prediction which is a foundational component of environmental epidemiology. Wang et al. (2022) argue that conventional RNN-based time-series models often neglect spatial topology. They explain that “an exploration of the GNN model driven by a directed graph is necessary to model the influence of wind factors and further improve regional air-quality prediction”. This underscores the limitation of existing models and directly calls for the development and application of GNNs to better capture the complex dynamics of air pollutants like ozone.\nSimilarly, Ferrer-Cid (2023) notes that while techniques for spatio-temporal signal reconstruction using graph neural networks are emerging for applications like air pollution sensor networks, these “need to be further developed,” highlighting an ongoing research gap in refining these architectures . Furthermore, the call for progress in air pollution prediction through the creation of “extensive benchmark datasets for testing learning algorithms and designing deep network topologies” (Duncan Morapedi and Christiana Obagbuwa 2023) advocates for the development and testing of more sophisticated deep learning models, including CNN and GNN variants, to advance spatio-temporal modeling in air pollution research, which would ultimately benefit epidemiological investigations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#graph-neural-networks-gnns",
    "href": "03-literature-review.html#graph-neural-networks-gnns",
    "title": "4  Literature Review",
    "section": "4.7 Graph Neural Networks (GNNs)",
    "text": "4.7 Graph Neural Networks (GNNs)\n\n4.7.1 Recommended GNN Architectures for Spatiotemporal Ozone Modeling\nThe literature suggests a few Graph Neural Network (GNN) architectures for effectively modeling spatiotemporal relationships in air pollution, particularly for ozone and related pollutants. Among these, diffusion-based GNNs have demonstrated notable performance. Specifically, the Diffusion Convolutional Recurrent Neural Network (DCRNN) was successfully applied to model hourly ozone data from extensive air-quality monitoring networks, showing superior prediction accuracy compared to baseline models (Wang et al. 2022). The efficacy of the DCRNN is further enhanced through the incorporation of meteorological features; a directed graph variant of the DCRNN, which explicitly encodes wind direction, was shown to perform better in 24-hour predictions of pollutant concentrations than undirected graph models. This highlights the benefit of GNN architectures that can integrate physical processes influencing pollutant dispersion.\nOther GNN variants identified for their utility in air pollution monitoring include spectral-based approaches like ChebyNet and inductive methods such as the Inductive Graph Neural Network Kriging (IGNNK). IGNNK, characterized as a 2-layer diffusion convolution neural network, is particularly noted for its ability to reconstruct pollutant fields, such as O₃/NO₂, from incomplete data collected by sensor networks (Ferrer-Cid 2023). The capacity of these architectures to handle complex spatial dependencies and reconstruct data at unmonitored locations makes them valuable for creating comprehensive exposure surfaces.\n\n\n4.7.2 Relevance of GNN Approaches to Cardiovascular Epidemiology\nThe spatiotemporal modeling capabilities of GNNs hold significant relevance for cardiovascular epidemiology, primarily by enabling more accurate and higher-resolution exposure assessments for pollutants like ozone. Epidemiological evidence indicates that associations between air pollutant exposure and cardiovascular outcomes can be sensitive to the spatial resolution of the exposure data. For instance, a study assessing cardiovascular hospitalizations found a slightly larger effect size for PM₁₀ exposure when assessed at a 200-meter resolution compared to a 1-kilometer resolution (Fasola et al. 2021). This finding underscores the potential benefit of employing GNNs, which can generate finer-scale exposure estimates, thereby potentially reducing exposure misclassification and improving the precision of risk estimates in cardiovascular studies.\nGiven the established epidemiological links between ozone exposure and cardiovascular physiological markers, such as blood pressure (Tang et al. 2024), the need for accurate O₃ exposure modeling is paramount. While GNNs like the DCRNN have demonstrated superior performance in forecasting ozone concentrations compared to conventional models, their application in directly modeling cardiovascular disease outcomes or a comparative evaluation against traditional spatial statistical methods in such epidemiological contexts was not observed in the reviewed literature. Nevertheless, the enhanced ability of GNNs to capture complex spatiotemporal pollutant dynamics suggests their potential to significantly improve exposure inputs for studies investigating the cardiovascular impacts of ozone.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#convolutional-neural-networks-cnns",
    "href": "03-literature-review.html#convolutional-neural-networks-cnns",
    "title": "4  Literature Review",
    "section": "4.8 Convolutional Neural Networks (CNNs)",
    "text": "4.8 Convolutional Neural Networks (CNNs)\n\n4.8.1 Previous Applications of CNNs in Air Pollution Exposure Modeling\nConvolutional Neural Networks (CNNs), including variants such as convolutional autoencoders, have been documented in environmental epidemiology primarily for the purpose of air pollution exposure modeling and forecasting. These applications leverage the inherent strengths of CNNs in processing grid-like or rasterized spatial data. For instance, Rautela and Goyal (2024) employed convolutional autoencoders for predicting pollutant concentrations, including ozone (O₃), using image-like inputs, with performance evaluated using image-based metrics such as the Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR). This study demonstrated the utility of CNNs in generating fine spatio-temporal resolution maps for pollutants like O₃ and PM₂.₅, showcasing their capability to learn complex multi-scale spatial patterns from gridded data. The primary output of these CNN applications in the reviewed literature is the prediction or forecasting of ambient pollutant concentrations rather than the direct modeling of health outcomes.\n\n\n4.8.2 CNN Applications Directly Linking Air Pollution to Cardiovascular Outcomes\nDespite the successful application of CNNs in air pollution exposure assessment, their direct integration into models assessing cardiovascular health outcomes from such exposures appears to be largely absent or significantly limited within the provided evidence. Epidemiological studies investigating cardiovascular health endpoints, such as cardiovascular hospitalizations or the incidence of major cardiovascular diseases, have predominantly relied on other methodologies, such as Random Forests, for their exposure assessment components, rather than CNNs. For health-effect modeling in these studies, traditional statistical models like Cox regression or other regression techniques were employed (Fasola et al. (2021); Y. Liu et al. (2022)).\nWhile one study did extend its CNN-based ozone prediction (using a convolutional autoencoder on gridded data) to quantify health impact metrics, these were specifically odds ratios for respiratory symptoms, not cardiovascular outcomes (Fasola et al. 2021). Furthermore, in analyses where various machine learning models were compared for predicting pollution-related metabolic diseases (including hypertension, a cardiovascular risk factor), tree-based ensembles like XGBoost demonstrated high predictive accuracy (AUROC 0.710 for hypertension), with CNNs not being reported as the preferred or applied method for this direct health outcome prediction (J. Liu et al. 2025). Consequently, the reviewed literature indicates a notable gap in studies that combine CNN-based air pollution exposure modeling with the direct epidemiological analysis of cardiovascular disease outcomes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#identifying-high-risk-areas",
    "href": "03-literature-review.html#identifying-high-risk-areas",
    "title": "4  Literature Review",
    "section": "4.9 Identifying High-Risk Areas",
    "text": "4.9 Identifying High-Risk Areas\n\n4.9.1 Statistical Methods and Models for Area-Level Risk Estimation\nPrevious epidemiological studies have employed a range of statistical models to estimate area-specific risks of cardiovascular mortality or morbidity associated with ozone and related criteria air pollutants. Hierarchical statistical approaches are prominent in this context. For example, Chen, Teyton, and Benmarhnia (2024) utilized a modified two-stage Bayesian hierarchical model, in conjunction with quasi-Poisson models and natural cubic spline functions, to estimate county-specific risks of circulatory outcomes from PM₂.₅ and ozone exposure in California. This methodology allowed for the pooling of risks across counties and the subsequent identification of those with the highest excess acute-care utilization. Another sophisticated approach involved a three-stage time-series framework combining distributed lag non-linear models (DLNM) with mixed-effects meta-analysis to assess cardiovascular disease risk linked to nitrogen dioxide exposure across urban and rural areas in China (Zhang et al. 2024). This framework generated area-specific effect estimates and attributable fractions, thereby enabling the identification of regions (urban versus rural counties) with elevated cardiovascular events associated with pollutant exposure.\n\n\n4.9.2 Spatial Analytical Approaches for High-Risk Area Identification\nTo delineate specific administrative units or geographic areas with elevated cardiovascular risk related to air pollution, studies have frequently applied various spatial analytical techniques. Global and local spatial autocorrelation metrics are commonly used to identify non-random spatial patterns in exposure or health outcomes. J. Liu et al. (2025) employed Moran’s I to test for global spatial autocorrelation and Local Indicators of Spatial Association (LISA) maps to pinpoint “high-high” and “low-low” clusters, thereby identifying prefectural-level “hot-spots” of hypertension linked to ozone and other pollutants. This study also incorporated unsupervised clustering methods such as k-means and Principal Component Analysis (PCA) to further refine the identification of these high-risk areas. Similarly, Clark et al. (2024) assessed spatial autocorrelation using Moran’s I following land-use regression and Generalized Additive Model (GAM) modeling to map pollutants, a technique routinely used to delineate neighborhoods or census tracts with unusually high pollutant levels that can drive cardiovascular risk.\nOther spatial statistical methods for hotspot detection include the use of spatial GAMs to map and identify statistically significant elevated (hot) or reduced (cold) odds of model error or risk on a continuous surface (Girguis et al. 2019). Geostatistical interpolation methods, particularly inverse-distance weighting (IDW) and kriging, are widely employed to create continuous concentration surfaces for pollutants like ozone from monitoring data. Kriging, by generating both estimates and their standard errors, supports the delineation of areas with statistically significant high concentrations, which can be considered exposure hotspots (Xie et al. 2017). These diverse spatial analytical tools provide crucial means for identifying and mapping areas where populations may be at heightened risk of pollution-related cardiovascular outcomes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#quantifying-ozone-impact",
    "href": "03-literature-review.html#quantifying-ozone-impact",
    "title": "4  Literature Review",
    "section": "4.10 Quantifying Ozone Impact",
    "text": "4.10 Quantifying Ozone Impact\n\n4.10.1 Empirical Estimates from Short-Term Exposure\nSelected epidemiological studies provided quantitative estimates of the increased risk for cardiovascular events associated with short-term elevations in ambient ozone levels. A case-crossover study conducted within the Pisan Longitudinal Study in Tuscany, Italy, investigated the association between daily ozone concentrations and cardiovascular hospitalizations. This research found that a 10 µg/m³ increment in daily ozone exposure was associated with an approximately 12% higher odds of cardiovascular hospitalization six days following the exposure, with a reported Odds Ratio (OR) of 1.122 (95% Confidence Interval [CI] 0.890–1.415). Although this estimate suggests a potential adverse effect, the confidence interval encompassed the null value, indicating statistical uncertainty (Fasola et al. 2021).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#empirical-estimates-from-mortality-or-long-term-exposure",
    "href": "03-literature-review.html#empirical-estimates-from-mortality-or-long-term-exposure",
    "title": "4  Literature Review",
    "section": "4.11 Empirical Estimates from Mortality or Long-Term Exposure",
    "text": "4.11 Empirical Estimates from Mortality or Long-Term Exposure\nStudies quantifying the impact of ozone exposure on cardiovascular mortality have utilized quasi-experimental designs to assess the effects of changes in ambient ozone concentrations. Burns et al. (2019), in a study evaluating large-scale traffic-related interventions, employed a difference-in-differences (DiD) framework, supplemented by a triple-difference sensitivity analysis. By comparing intervention areas with control areas before and after the implementation of traffic measures, this study linked concomitant reductions in ozone levels to changes in cardiovascular mortality rates. The regression-based DiD analysis (DiD estimator = -1.557, SE 0.813; adjusted Relative Risk ≈ 0.97) indicated a 5.9% decline in cardiovascular disease deaths in the vehicular intervention areas associated with the decrease in ozone. Beyond this intervention-based approach, the provided dataset did not contain further time-series, case-crossover, or cohort studies that quantified the association between ambient ozone exposure and cardiovascular mortality outcomes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#exploring-non-linearity",
    "href": "03-literature-review.html#exploring-non-linearity",
    "title": "4  Literature Review",
    "section": "4.12 Exploring Non-Linearity",
    "text": "4.12 Exploring Non-Linearity\n\n4.12.1 Studies Reporting Non-linear Relationships\nWithin the selected articles for review, the explicit observation of non-linear relationships between ozone exposure and cardiovascular outcomes is not clearly detailed, even when analytical methods capable of detecting such patterns were employed. For instance, Tang et al. (2024) investigated the association between ozone exposure and blood pressure indices, a cardiovascular risk marker in a large cohort of middle-aged and older Chinese adults. This study utilized a multivariable-adjusted Distributed Lag Model (DLM) combined with Linear Mixed Models (LMM) to assess cumulative effects of O₃ exposure for periods up to 30 days. While DLMs are inherently capable of modeling non-linear exposure-response functions and non-linear lag structures, the provided summaries primarily highlight the application of this methodology and the finding of inverse associations, without specifying if the observed dose-response relationship itself was non-linear for cardiovascular outcomes. Further review of the evidence suggests that this study observed linear changes in blood pressure. Beyond this, the selected studies did not contain other details reporting observed non-linear dose-response associations between ozone exposure and clinically defined cardiovascular disease events.\n\n\n4.12.2 Studies Reporting No Observed Non-linear Relationships\nFew studies explicitly investigated potential non-linear effects, particularly threshold effects, of ozone on cardiovascular health but did not find evidence supporting such relationships. In a large prospective cohort study of 0.5 million adults in China, C. Liu et al. (2022) found that long-term exposure to ambient ozone (mean exposure range approximately 43–60 ppb) was not associated with the incidence of major cardiovascular diseases, with the hazard ratio remaining essentially null (HR ≈ 1.00, 95% CI 1.00–1.01) across the exposure distribution. This suggests no evidence of a concentration threshold for major cardiovascular diseases within the observed ambient ozone levels. Similarly, a case-crossover study conducted in Pisa, Italy, Fasola et al. (2021) examined short-term effects of air pollution on 137 cardiovascular hospitalizations and found no significant association for ozone at multiple lags (lag 0 OR 0.896, 95% CI 0.710–1.130). The authors concluded there was no significant association for O₃, implying the absence of an acute threshold effect across an inter-quartile range of approximately 30 µg/m³. Furthermore, the repeated-measurement study by Tang et al. (2024) in a Chinese population, which used DLMs, reported that each 10 µg/m³ increase in 30-day moving-average ozone was associated with small, apparently linear, reductions in systolic and diastolic blood pressure, with no mention of a non-linear cut off or threshold effect within typical ambient concentrations. Collectively, these studies suggest that within the exposure ranges and populations investigated, non-linear threshold effects of ozone on the assessed cardiovascular endpoints were not observed; instead, relationships were often found to be linear or null.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#spatial-spillover",
    "href": "03-literature-review.html#spatial-spillover",
    "title": "4  Literature Review",
    "section": "4.13 Spatial Spillover",
    "text": "4.13 Spatial Spillover\nInvestigations identified within the selected articles for review which quantify spatial spillover effects of ozone or related criteria air pollutant exposure on cardiovascular outcomes in adjacent counties or similar administrative areas using formal spatial-lag or spatial-Durbin models are not apparent. However, some research highlights the existence of spatial dependencies in health outcomes linked to air pollution across administrative units. For instance, a study by J. Liu et al. (2025) using a machine learning pipeline detected significant spatial correlations between various ambient pollutants and hypertension at the prefectural-city level in China. These findings underscore that the health impacts associated with air pollutant exposure can exhibit regional variability and are not strictly confined within local administrative boundaries, thereby suggesting the potential for effects to propagate spatially. The Fasola et al. (2021) study compared exposure surfaces at different spatial resolutions (200 m vs 1 km) for acute cardiovascular hospitalizations but reported no measurable effect specifically for ozone, focusing more on within-area spatial resolution rather than inter-area spillover. Collectively, while the concept of regional dependence is acknowledged, direct evidence from the reviewed literature employing explicit spatial spillover models to assess how ozone exposure in one administrative area affects cardiovascular outcomes in adjacent areas remains limited, indicating a notable gap in the current reviewed literature.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#validation-and-predictive-accuracy",
    "href": "03-literature-review.html#validation-and-predictive-accuracy",
    "title": "4  Literature Review",
    "section": "4.14 Validation and Predictive Accuracy",
    "text": "4.14 Validation and Predictive Accuracy\n\n4.14.1 Validation Metrics for CNN and GNN Models\nPrevious epidemiological and environmental studies employing Convolutional Neural Networks (CNN) or Graph Neural Networks (GNN) for predicting air quality outcomes, including ozone and similar criteria pollutants, have consistently reported a set of standard validation metrics to assess model performance. These commonly include error-based regression metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), which quantify the average magnitude of prediction errors (Wang et al. (2022); Krishna E T and R (2024)). Goodness-of-fit measures are also frequently utilized, prominently the Pearson correlation coefficient (r) and the coefficient of determination (R²), to evaluate the correspondence between predicted and observed concentrations. For instance, Ferrer-Cid (2023) reported RMSE of 11.96 µg/m³, MAE of 9.51 µg/m³, and R² of 0.66 for graph- and CNN-based sensor network models. In studies involving image-like or gridded data, particularly with CNNs, image-similarity and error metrics such as the Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE) have also been adopted to quantify predictive quality (Rautela and Goyal 2024). Some investigations additionally report overall model accuracy, particularly when classification tasks are involved.\n\n\n4.14.2 Predictive Accuracy of Random Forest Models\nRandom Forest models have been widely applied in epidemiological studies for predicting ozone and related criteria air pollutant concentrations, with reported predictive accuracy varying across different geographic locations, temporal resolutions, and specific pollutants. In a national-scale comparison in Scotland for monthly predictions, an “out-of-cell” Random Forest (RFoc) model demonstrated high accuracy, achieving an RMSE of 4.36 µg m⁻³ for NO₂, 1.52 µg m⁻³ for PM₁₀, and 0.56 µg m⁻³ for PM₂.₅, with 94-95% coverage of 95% prediction intervals (Zhu, Lee, and Stoner 2024). For daily PM₂.₅ prediction at a national scale in Malaysia, a Random Forest model yielded a validation R² of 0.62 and an RMSE of 11.40 µg m⁻³ (Zaman et al. 2021). In Taiwan, a Random Forest model integrated into a Spark-based big data framework for sub-hourly PM₂.₅ forecasting achieved an RMSE of 10.90 µg m⁻³ and an R² of 0.85 (Shih et al. (2021)). These examples illustrate that while Random Forest models are versatile, their predictive accuracy, as indicated by metrics like RMSE and R², can differ substantially depending on the specific application context, including the spatial scale, temporal aggregation, and regional atmospheric conditions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#discussion",
    "href": "03-literature-review.html#discussion",
    "title": "4  Literature Review",
    "section": "4.15 Discussion",
    "text": "4.15 Discussion\nThis literature review, facilitated by an automated ETL and RAG pipeline, has systematically synthesized current research at the intersection of ambient ozone exposure, cardiovascular disease (CVD) outcomes, and the application of geospatial and machine learning methodologies, with a particular focus on approaches like Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs). The overarching narrative emerging from the reviewed literature confirms ozone’s impact on cardiovascular health, evidenced by associations with physiological markers like blood pressure and, at a population level, with cardiovascular mortality following exposure reductions. Mechanistic pathways involving lipid oxidation and inflammation are also increasingly understood. However, the evidence base, particularly for direct ozone-CVD mortality links and consistent short-term effects, remains somewhat varied, and epidemiological investigations often grapple with methodological complexities in exposure assessment and disentangling pollutant-specific effects. A clear trend identified is the evolution of geospatial epidemiological methods from traditional statistical models like land-use regression (LUR) and kriging towards the integration of machine learning for enhanced exposure estimation and the identification of high-risk areas. While conventional machine learning models such as Random Forest and XGBoost are frequently applied both for exposure prediction and, to some extent, for health outcome analysis (predicting hypertension risk, etc.), a significant disconnect persists when it comes to more advanced deep learning models.\nCritically evaluating the reviewed literature reveals common methodological strengths, such as the increasing adoption of high-resolution spatiotemporal exposure modeling and the application of robust validation metrics for these models, particularly for Random Forest and initial GNN-based exposure forecasts. The diverse array of geospatial techniques, from LUR to hotspot analysis using Moran’s I and LISA maps, also demonstrates a mature toolkit for characterizing spatial patterns. However, significant weaknesses and gaps were also evident. Many studies on ozone’s cardiovascular impact are limited by inconsistent short-term findings, the omission of ozone in some large cohort studies focusing on other pollutants like PM₂.₅, and a predominant focus on intermediate outcomes rather than direct mortality. Methodological challenges in geospatial epidemiology persist, including inaccuracies in exposure assessment stemming from sparse monitoring networks, the prevalent use of single spatial scales neglecting multi-scale effects, oversimplification of meteorological drivers in some forecasting models, and issues with multicollinearity in multi-pollutant health models.\nThe most striking gap, central to this review’s objectives, is the limited direct integration of deep learning methods like CNNs and GNNs into cardiovascular health outcome modeling, despite their documented success in air quality forecasting and sensor data processing (as seen in studies by Wang et al. (2022) and Rautela and Goyal (2024)). While these models are recommended for spatiotemporal ozone modeling due to their ability to capture complex dependencies and incorporate features like wind direction (DCRNN variants, etc.), their application stops short of linking these refined exposures to CVD event prediction or risk quantification within the reviewed epidemiological studies. Potential reasons for this disconnect may include the inherent complexity of these models, which can be perceived as “black boxes” hindering the etiological interpretation favored in epidemiology, the higher computational resources and specialized expertise required, challenges in aligning highly granular exposure predictions with often coarser health data registries, and the established reliance on traditional regression frameworks (linear mixed models, distributed lag models, etc.) for health effect estimation, as observed in the works of Tang et al. (2024) and Fasola et al. (2021). This gap implies that the full potential of AI-driven exposure science is not yet being leveraged to deepen our understanding of ozone-CVD relationships or to predict health outcomes with potentially greater accuracy and spatial specificity. Furthermore, the explicit modeling of spatial spillover effects of ozone on cardiovascular outcomes in adjacent administrative areas using formal spatial econometric models also remains an under-explored domain.\nThis review set out to provide a comprehensive overview of the research landscape concerning ozone, heart disease, and the use of geospatial/ML methods (particularly CNNs/GNNs), as identified via our automated pipeline; to detail methodological trends, practices, and gaps; and thereby to lay a robust foundation for subsequent empirical research. The synthesis presented has achieved these objectives by detailing current applications, highlighting for example, the strengths of GNNs like DCRNN and CNNs with autoencoders for exposure modeling, while contrasting this with the continued reliance on conventional methods for direct health outcome analysis, and explicitly identifying the critical gap in integrating these advanced approaches for ozone-CVD risk assessment. The identified underutilization of CNNs and GNNs for modeling county-level spatiotemporal dependencies between ozone exposure and cardiovascular outcomes, despite their demonstrated capabilities in related contexts such as air quality forecasting and their theoretical advantages in handling complex spatial and temporal data structures, directly motivates the analytical approach proposed for our subsequent empirical research (outlined in “#04-analysis.qmd”).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#future-work",
    "href": "03-literature-review.html#future-work",
    "title": "4  Literature Review",
    "section": "4.16 Future Work",
    "text": "4.16 Future Work\nThe planned empirical investigation will begin by establishing a baseline understanding of the relationships between various annual county-level ozone exposure metrics and cardiovascular disease (CVD) mortality rates using robust machine learning techniques such as Random Forests. This initial phase will also incorporate essential data preprocessing strategies, including transformations to address the skewed distribution commonly observed in CVD mortality rates and careful methodologies to manage the influence of specific data anomalies, such as the isolated, extreme ozone values recorded on any or over any given time period, potentially through outlier capping or sensitivity analyses to ensure model stability and the reliability of derived insights. Following this foundational analysis, the research will transition to applying spatiotemporal deep learning models, specifically CNN and GNN architectures, to the U.S. county-level ozone and CVD mortality data. This aims to directly address the identified literature gap by systematically exploring whether these models can provide improved prediction accuracy or enhanced interpretability compared to both traditional methods and the initial Random Forest benchmarks. Furthermore, to deconstruct the complex relationships potentially learned by these models, interpretability techniques such as SHAP (SHapley Additive exPlanations) analysis will be employed. It is hypothesized that SHAP analyses will not only affirm the importance of diverse ozone exposure metrics (e.g., mean, peak, variability) but will also elucidate the nuanced, often non-linear, nature of their associations with CVD mortality and reveal critical interactions, such as how the impact of average ozone concentrations might be modulated by population size or co-occurring high-peak exposure days. We anticipate that these model-agnostic explanations will demonstrate how specific ozone characteristics contribute to predicted risk in different contexts, thereby offering deeper insights than those typically achievable with conventional statistical approaches and underscoring the value of these methodologies in this epidemiological domain.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "03-literature-review.html#conclusion",
    "href": "03-literature-review.html#conclusion",
    "title": "4  Literature Review",
    "section": "4.17 Conclusion",
    "text": "4.17 Conclusion\nThis literature review, facilitated by an AI-powered ETL and RAG pipeline, aimed to systematically survey and synthesize current research at the intersection of ozone exposure, cardiovascular disease, and the application of geospatial and machine learning methodologies, with a particular focus on CNNs and GNNs. The primary conclusions drawn indicate that while ozone is increasingly recognized as a cardiovascular risk factor and geospatial methods are evolving, a significant translational gap exists: deep learning techniques like CNNs and GNNs, despite their proven efficacy in complex spatiotemporal air pollution modeling, have not yet been substantially integrated into epidemiological studies to directly model cardiovascular health outcomes related to ozone. Instead, health effect estimation largely continues to rely on more established statistical and conventional machine learning approaches. Key methodological challenges in exposure assessment and the analysis of multi-pollutant contexts also persist across the field.\nAddressing these identified gaps is crucial for advancing our understanding of the environmental determinants of cardiovascular disease. The limited application of sophisticated models like CNNs and GNNs represents a missed opportunity to potentially enhance the accuracy of risk prediction, identify complex interaction effects, and better characterize spatiotemporal health impacts. The analysis of literature, as demonstrated by the pipeline used herein, can effectively pinpoint such underutilized yet promising methodologies. Therefore, future research should prioritize the rigorous development, validation, and application of these integrated modeling frameworks in ozone-cardiovascular epidemiology to more fully harness their potential for improving public health insights and interventions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literature Review</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html",
    "href": "04-prelim-analysis.html",
    "title": "5  Preliminary Analysis",
    "section": "",
    "text": "6 Proposed Research Question:\n“Can convolutional neural networks (CNNs) and graph neural networks (GNNs) effectively model the spatiotemporal relationships between ozone exposure and cardiovascular disease (CVD) outcomes at the county level in the United States, and do these models provide improved prediction accuracy or interpretability compared to traditional statistical or machine learning approaches?”",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#graph-neural-networks-gnns",
    "href": "04-prelim-analysis.html#graph-neural-networks-gnns",
    "title": "5  Preliminary Analysis",
    "section": "6.1 Graph Neural Networks (GNNs)",
    "text": "6.1 Graph Neural Networks (GNNs)\n\nUse GNN to model counties as nodes in a graph, with edges based on adjacency or other meaningful spatial relationships.\nDefine counties as nodes and their adjacency (or distance-based proximity) as edges.\nNode features would be time series of ozone metrics, past CVD rates, and other covariates for each county\nGraph Convolutional Networks (GCN), Graph Attention Networks (GAT), GraphSAGE for static graph snapshots per year, or spatiotemporal GNNs like STGCN, DCRNN, GraphWaveNet if you structure data as time-series on nodes.\nPerform comparative analyses with traditional models (linear regression, random forests, etc.).\n\nQuestions Answered:\n\nCan GNNs effectively model the spatiotemporal dependencies in ozone exposure and CVD mortality, leveraging the spatial relationships between counties?\nDo GNNs improve the prediction of county-level CVD mortality rates compared to non-spatial models or traditional spatial statistics?\nCan GNNs help identify counties or regions where the influence of neighboring counties’ ozone levels is particularly important for local CVD rates?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#convolutional-neural-networks-cnns",
    "href": "04-prelim-analysis.html#convolutional-neural-networks-cnns",
    "title": "5  Preliminary Analysis",
    "section": "6.2 Convolutional Neural Networks (CNNs)",
    "text": "6.2 Convolutional Neural Networks (CNNs)\n\nRasterize the merged data (ozone, CVD rates, covariates) onto a regular spatial grid for each year. This creates a sequence of “images” or multi-channel images\n2D CNNs (if each year’s spatial map is an image), 3D CNNs (if using sequences of years as volumes), or ConvLSTM/CNN-RNN hybrids to capture spatiotemporal features\n\nQuestions Answered:\n\nCan CNNs learn meaningful spatial patterns from gridded ozone and CVD data to predict CVD mortality hotspots or future rates?\nHow do local spatial configurations of ozone exposure (as captured by convolutional filters) relate to CVD patterns?\nDo CNNs offer advantages in predictive accuracy over other methods for this gridded spatiotemporal data?\n\nIdentifying High-Risk Areas:\n\nWhich counties consistently show high CVD mortality in conjunction with high ozone levels, even after accounting for other factors? (small population counties but for analytical reason).\n\nanalytical plan (CNN or GNN architectures, datasets to use).\nQuantifying Ozone Impact:\n\nWhat is the estimated increase in CVD mortality risk associated with an increase in annual mean/median/max ozone? (GLMs, Mixed Models, GAMs for effect size; ML for prediction importance).\n\nExploring Non-Linearity:\n\nIs there a threshold effect for ozone, or does the risk increase differently at different ozone levels? (GAMs, tree-based ML, NNs).\n\nSpatial Spillover:\n\nDoes ozone in one county affect CVD in neighboring counties? (GNNs, some advanced spatial statistical models).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#proposed-research-question-for-aim-3",
    "href": "04-prelim-analysis.html#proposed-research-question-for-aim-3",
    "title": "5  Preliminary Analysis",
    "section": "6.3 Proposed Research Question for Aim 3:",
    "text": "6.3 Proposed Research Question for Aim 3:\n“Can a multi-modal deep learning framework, integrating high-resolution optical satellite imagery (processed with CNNs) and methane monitoring satellite data, accurately identify and classify the activity status of unconventional oil and gas wells at a national scale? Furthermore, can this resulting spatiotemporal inventory serve as a foundational dataset to explore potential associations between UOG site density and county-level cardiovascular mortality?”",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#data-preprocessing-and-cleaning",
    "href": "04-prelim-analysis.html#data-preprocessing-and-cleaning",
    "title": "5  Preliminary Analysis",
    "section": "7.1 Data Preprocessing and Cleaning",
    "text": "7.1 Data Preprocessing and Cleaning\nInitial exploratory data analysis of the daily census-tract ozone prediction data (DS_O3_pred) for the 2001-2005 period revealed a set of anomalously high values, with a maximum approaching 500 ppb. Further investigation, including time series analysis (as shown in Figure 7.1) and spatial visualization (see Figure 7.2 for geographical distribution), pinpointed these extreme values to a single day, September 13, 2001, predominantly within census tracts in Missouri (State FIPS ‘29’). While daily mean and median ozone levels for Missouri on this date were elevated, they were substantially lower than these localized maximums, as illustrated in Figure 7.1.\nGiven that these peak values were several orders of magnitude higher than the 99th percentile of the broader dataset for that period (83.2 ppb vs. ~491 ppb) and their extreme magnitude could disproportionately influence county-level annual average calculations, a data cleaning step was implemented. To mitigate this potential bias while retaining information about a significant, albeit localized, high ozone event, DS_O3_pred values for census tracts in Missouri on September 13, 2001, that exceeded 150 ppb were capped at this threshold. This 150 ppb level was chosen as it still represents a very high ozone concentration (categorized as “Unhealthy” by EPA standards for an 8-hour average) but prevents extreme, potentially unrepresentative modeled outputs from dominating subsequent statistical aggregations. This adjustment affected 45 data points on that specific day. The DS_O3_pred data for the other periods (2006-2010 and 2011-2014), which had maximum observed values around 125-127 ppb, did not exhibit such extreme singular-day outliers and was therefore used after standard quality checks without similar capping.\n\n\n\n\n\n\nFigure 7.1: Anomalous ozone event in Missouri, September-October 2001. The graph shows daily mean, median, and maximum DS_O3_pred values. Note the exceptional spike in maximum predicted ozone on September 13, 2001, which reached values significantly above typical daily maximums observed during this period.\n\n\n\n\n\n\n\n\n\nFigure 7.2: Geographic Distribution of Extreme Ozone Event in Missouri (2001-09-13)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#construction-of-the-county-year-analytical-dataset",
    "href": "04-prelim-analysis.html#construction-of-the-county-year-analytical-dataset",
    "title": "5  Preliminary Analysis",
    "section": "7.2 Construction of the County-Year Analytical Dataset",
    "text": "7.2 Construction of the County-Year Analytical Dataset\nThe final analytical dataset, merged_df_clean, was constructed through a series of preprocessing, standardization, aggregation, and merging steps applied to the source ozone and cardiovascular disease (CVD) mortality data. The daily census-tract level ozone data, originally sourced from three separate files covering 2001-2005, 2006-2010, and 2011-2014, underwent multiple preprocessing stages: For each data segment, state (statefips), county (countyfips), and census tract (ctfips) identifiers were converted to standardized string formats. A consistent 5-digit county identifier (GEOID10) was derived for each record by combining the 2-digit state FIPS and the 3-digit county FIPS (for the 2006-2010 dataset where countyfips represented the 3-digit portion) or by standardizing the existing 5-digit countyfips column (for the 2001-2005 and 2011-2014 datasets). This process ensured comprehensive county coverage, yielding approximately 3,109 unique GEOID10 values per year across all raw ozone data. The date column in each segment was converted to datetime64[ns] format, and a numeric year column (int32) was verified.\nAs detailed in the preceding “Data Preprocessing and Cleaning” section, anomalously high DS_O3_pred values (those &gt;150 ppb) occurring on September 13, 2001, in Missouri (State FIPS ‘29’) within the df_ozone_2001_2005 dataset were capped at 150 ppb. This affected 45 data points and resulted in a maximum DS_O3_pred value of 150 ppb in the subsequently combined dataset. Other ozone data periods did not exhibit such extreme singular-day outliers requiring capping. The three processed and cleaned ozone dataframes were concatenated into a single comprehensive dataframe (df_ozone_full) containing 412,812,344 daily census-tract level observations from 2001 to 2014. df_ozone_full was then aggregated by year and GEOID10. For each county-year, multiple summary statistics for DS_O3_pred were calculated: mean (mean_ozone_pred), median (median_ozone_pred), maximum (max_ozone_pred), 95th percentile (p95_ozone_pred), and standard deviation (std_ozone_pred). This resulted in the df_ozone_annual_county dataframe with 43,526 unique county-year observations. The GEOID10 column was then renamed to countyfips for merging.\nThe “Rates and Trends in Heart Disease and Stroke Mortality Among US Adults ≥35 by County, Age Group, Race/Ethnicity, and Sex, 2000-2019” dataset (5,770,240 initial rows) was processed as follows:\n\nMissing Value Handling: Rows with missing Data_Value (indicating suppressed mortality rates) were dropped.\nFIPS Standardization: The LocationID column (int32) was converted to a 5-digit string and renamed to countyfips to serve as the merge key.\nYear Cleaning: The Year column (originally object type, containing single years and year ranges) was processed to extract single numeric years. Records were subsequently filtered to include only years between 2001 and 2014, aligning with the ozone data’s temporal extent.\n\nThe df_ozone_annual_county (containing annual county-level ozone metrics) was merged with the df_cvd_processed (containing annual county-level CVD mortality rates and associated information) using an inner join on year and countyfips.\nCharacteristics of the Final Merged Dataset (merged_df_clean)\nThe resulting merged_df_clean dataset contains 2,056,590 rows. Each row represents a unique county-year-CVD Topic combination for which both ozone exposure data and CVD mortality data were available.\n\nTemporal Range: The dataset spans years 2001 through 2014.\nSpatial Coverage: It includes approximately 3,056 to 3,057 unique US counties per year.\nKey Variables for Analysis:\n\nyear (int32): Observation year.\ncountyfips (object): 5-digit county FIPS code.\nOzone Metrics:\n\nmean_ozone_pred\nmedian_ozone_pred\nmax_ozone_pred\np95_ozone_pred\nstd_ozone_pred\n\nCVD Outcome:\n\nData_Value: representing age-standardized, spatiotemporally smoothed mortality rate per 100,000.\nTopic: Specific cardiovascular disease category (‘All heart disease’, ‘All stroke’, ‘Coronary heart disease (CHD)’).\n\nDescriptive CVD columns\n\nLocationAbbr\nLocationDesc\nGeographicLevel\nDataSource\nClass\nData_Value_Unit\nData_Value_Type\n\nCVD Confidence Intervals:\n\nConfidence_limit_Low\nConfidence_limit_High\n\nStratification Details:\n\nStratificationCategory1: Age-specific groups: 35-64 years vs. 65+ years\nStratificationCategory2: Sex-specific groups: Men vs. Women\nStratificationCategory3: Race-specific groups: Exploring disparities among different races.\n\n\nData Quality: All key analytical columns (ozone metrics, Data_Value for CVD rate) in merged_df_clean have no missing values.\n\nDataset Summary (in merged data):\nAll key analytical columns (ozone metrics, Data_Value for CVD rate) in merged_df_clean have no missing values. The mean_ozone_pred in merged_df_clean has a mean of approximately 39.7 ppb, a standard deviation of 4.2 ppb, a minimum of 20.3 ppb, and a maximum of 54.9 ppb (reflecting annual county averages after outlier capping at the daily tract level). The dataset includes multiple CVD Topics, allowing for analyses focused on specific outcomes. The original stratification columns are retained, offering pathways for future stratified analyses. This meticulously cleaned and merged dataset is designed specifically to support robust spatiotemporal modeling efforts using convolutional neural networks (CNNs) and graph neural networks (GNNs), thereby directly addressing the research question’s methodological and analytical requirements.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#validation-metrics",
    "href": "04-prelim-analysis.html#validation-metrics",
    "title": "5  Preliminary Analysis",
    "section": "7.3 Validation Metrics",
    "text": "7.3 Validation Metrics\n\n7.3.1 Metrics for Regression Tasks (Predicting cvd_mortality_rate)\nThese assess how close the model’s predictions (\\(\\hat{y}_i\\)) are to the actual values (\\(y_i\\)).\n\nScale-Dependent Errors (in the units of ppb or rate per 100,000):\n\nMean Absolute Error (MAE): Average absolute difference between predicted and actual values. It’s easy to understand and less sensitive to extreme outliers than MSE/RMSE. Formula: \\[\n\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left| y_i - \\hat{y}_i \\right|\n\\]\nwhere \\(n\\) is the total number of data points (county-year observations) being evaluated, \\(i\\) is the index for each data point, from 1 to n, representing each specific county-year observation, \\(y_i\\) is the observed value for the \\(i\\)-th data point, specifically cvd mortality rate for a given year and county, \\(\\hat{y}_i\\) is the value predicted by the model for the \\(i\\)-th data point for cvd mortality of that same \\(i\\)-th observation (the specific county-year), \\(\\left( y_i - \\hat{y}_i \\right)\\) is the prediction error for the \\(i\\)-th observation and represents the difference between the actual cvd mortality rate and the models predicted cvd mortality, \\(\\left| y_i - \\hat{y}_i \\right|\\) is the absolute error between the actual value and the prdicted value for the \\(i\\)-th data point, \\(\\sum_{i=1}^{n}\\) is the summation of all the individual absolute errors across all \\(n\\) data points, \\(\\frac{1}{n}\\) is the average of the absolute difference,\nThe MAE will tell us, on average, how many units (deaths per 100,000 population since thats the unit of cvd_mortality_rate) the models predictions of county-level annual CVD mortality rates are off from the actual rates. A lower MAE indicates the models predictions are, on average, closer to the true values.\nMean SquaredError (MSE):\nAverage of the squared differences. It penalizes larger errors more heavily. Formula: \\[\n\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left( y_i - \\hat{y}_i \\right)^2\n\\]\nRoot Mean Squared Error (RMSE):\nThe square root of MSE. Its in the same units as tge outcome variable and is a common metric, penalizing large errors. Formula:\n\\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left( y_i - \\hat{y}_i \\right)^2}\n\\]\n\nScale-Independent Errors (Percentage-based):\n\nMean Absolute Percentage Error (MAPE):\nAverage percentage difference. Useful for comparing across different scales, but can be problematic if actual values (\\(y_i\\)) are zero or very close to zero. Formula:\n\\[\n\\text{MAPE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left|\\frac {y_i - \\hat{y}_i}{y_i} \\right|\\times 100\\%\n\\]\n\nGoodness-of-Fit / Correlation:\n\nR-squared (R² or Coefficient of Determination):\nIndicates the proportion of the variance in the cvd_mortality_rate that is predictable from the model (ozone and other features). Values range from 0 to 1 (higher is better).\n\\[\n\\text{R}^2 = 1 - \\frac{\\sum_{i=1}^{n}\\left( y_i - \\hat{y}_i \\right)^2}{\\sum_{i=1}^{n}\\left( y_i - \\bar{y}_i \\right)^2}\n\\]\nPearson Correlation Coefficient:\nMeasures the linear relationship between the model’s predictions and the true mortality rates",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#preliminary-analysis",
    "href": "04-prelim-analysis.html#preliminary-analysis",
    "title": "5  Preliminary Analysis",
    "section": "7.4 Preliminary analysis",
    "text": "7.4 Preliminary analysis\n\n7.4.1 Preliminary Random Forest Analysis Data-preprocessing\nFollowing the merging of ozone, cardiovascular disease (CVD) mortality, and census-derived population data, the resulting analytical dataset (merged_df_clean) contained 2,056,590 county-year observations. Initial examination of the primary outcome variable, the age-standardized and spatiotemporally smoothed CVD mortality rate (per 100,000 population), revealed a pronounced right-skewed distribution. The rates ranged from a minimum of 1.5 to a maximum of 4,139.0 per 100,000, with a mean of approximately 500.2 and a median of 188.9. This distribution exhibited a skewness of 1.56 (as shown in Figure 7.3, left panel), indicating that a direct application of this variable in modeling could be unduly influenced by extreme high values, potentially stemming from counties with smaller populations where rates are less stable. To address this and create a more symmetrically distributed outcome variable suitable for regression modeling, a natural logarithm transformation was applied. The resulting log-transformed CVD mortality rate exhibited a substantially reduced skewness of -0.24, with a mean of approximately 5.13 and a median of 5.24, indicating a much more symmetrical distribution (Figure 7.3, right panel).\nSimilarly, the county population estimates within the dataset also demonstrated significant right skewness (11.77), with values ranging from approximately 1,123 to over 10 million, and a mean (~138,602) considerably larger than the median (~35,386). Such a wide and skewed distribution for a predictor variable can also impact model performance and stability. Therefore, a log transformation (specifically log(1+x)) was applied to the county population estimates. This transformation effectively reduced the skewness to 0.52 and resulted in a more symmetrically distributed variable (as depicted in Figure 7.4), rendering it more suitable for inclusion as a covariate in subsequent modeling efforts. These transformations of the outcome variable and a key covariate were performed prior to selecting the final feature set and splitting the data for model training and evaluation.\n\n\n\n\n\n\nFigure 7.3: The log transformation has compressed the very high mortality rates, so they no longer exert such an overwhelming influence on the mean or the scale of the variable. The maximum value is now ~8.33 on the log scale, which is much more manageable than ~4139.\n\n\n\n\n\n\n\n\n\nFigure 7.4: The log transformation of popestimate was highly effective in reducing the skewness of the population data, making the distribution of population estimates much more symmetrical. This effectively compressed the range and reduced the disproportionate influence of very large population counties.\n\n\n\n\n\n7.4.2 Model Performance and Evaluation\nFollowing data preprocessing and feature engineering for the selected age cohort (35-64 years) and the ‘All heart disease’ outcome, a Random Forest regression model was developed to predict log-transformed county-level annual cardiovascular disease (CVD) mortality rates. Hyperparameter tuning via 5-fold cross-validated Randomized Search identified an optimal model configuration (e.g., 450 estimators, no maximum depth, minimum samples per split of 4, minimum samples per leaf of 1, and ‘log2’ for maximum features), achieving a cross-validated Root Mean Squared Error (RMSE) of 0.2706 on the log-transformed scale during training. The tuned Random Forest model demonstrated strong predictive capability, explaining a substantial portion of the variance in the outcome. On the training set, the model achieved an R² of 0.8906 and a Pearson correlation of 0.9619 between predicted and actual log-transformed CVD mortality rates. More critically, on the held-out test set, the model yielded an R² of 0.4699 and a Pearson correlation of 0.6870, with an Out-of-Bag (OOB) R² score of 0.4786, indicating reasonable generalization to unseen data despite some overfitting observed when comparing training and test performance.\nTo contextualize these findings, model performance was also evaluated on the original scale of CVD mortality rates (deaths per 100,000 population) by back-transforming the predictions. On the test set, the Random Forest model exhibited a Mean Absolute Error (MAE) of approximately 24.29 deaths per 100,000 and an RMSE of 33.39 deaths per 100,000, corresponding to a Mean Absolute Percentage Error (MAPE) of 21.32%. In comparison, a baseline Linear Regression model trained on the same features achieved a test set R² of 0.1370 (log scale), with an MAE of 31.75, RMSE of 42.39, and MAPE of 28.14% on the original scale. The superior performance of the Random Forest model across all metrics (as visualized in Figure 7.5) suggests its ability to capture complex, non-linear relationships between the ozone exposure metrics, log-transformed population estimates, year, and the age-stratified CVD mortality outcome, which were not adequately addressed by the linear model. The scatter plot of actual versus predicted values on the original scale (Figure 7.6) further illustrates the Random Forest’s comparatively better fit, particularly in the mid-range of mortality rates.\n\n\n\n\n\n\nFigure 7.5: jg.\n\n\n\n\n\n\n\n\n\nFigure 7.6: jg.\n\n\n\nFurther validation using learning curves (Figure 7.7) provided insights into the model’s bias-variance characteristics and its response to varying training set sizes. The training RMSE remained consistently low, decreasing from approximately 0.135 to 0.124 (on the log-transformed mortality rate scale) as the number of training examples increased from roughly 2,700 to 27,000. The cross-validation RMSE, while higher, also showed improvement, decreasing from approximately 0.298 to 0.271 over the same range of training data. The persistent gap between the training and cross-validation RMSE curves suggests that the model exhibits some degree of variance, fitting the training data more closely than it generalizes to unseen validation folds. However, both curves began to plateau towards the larger training set sizes, indicating that while the model benefited from increased data up to a point, substantial further improvements in validation RMSE with this specific model configuration and feature set might be limited by simply adding more similar training instances. This plateauing, combined with the validation RMSE being close to the final test set RMSE (0.271 vs 0.267), suggests that the model has learned the generalizable patterns from the available data to a reasonable extent, despite the variance.\n\n\n\n\n\n\nFigure 7.7: jg.\n\n\n\n\n\n7.4.3 Preliminary Results\nPremutation-Based Feature Importance\nTo further assess the contribution of each predictor to the Random Forest model’s performance on unseen data, permutation feature importance was calculated on the test set. This method evaluates the decrease in model score (negative Root Mean Squared Error) when a single feature’s values are randomly shuffled, thereby breaking its relationship with the outcome. The results, averaged over 10 repeats (Figure 7.8), indicated that metrics characterizing ozone exposure were the most influential predictors for the log-transformed CVD mortality rate within the modeled age group. Specifically, the annual standard deviation of daily 8-hour maximum ozone concentrations emerged as the top feature (mean importance drop ≈ 0.052), closely followed by the annual 95th percentile of daily ozone (mean importance drop ≈ 0.047), the annual median daily ozone (mean importance drop ≈ 0.043), and the annual mean daily ozone (mean importance drop ≈ 0.042).\n\n\n\n\n\n\nFigure 7.8: Higher\n\n\n\nLog-transformed county population estimate also demonstrated substantial predictive importance (mean importance drop ≈ 0.039), ranking among the key ozone metrics. The annual maximum daily ozone concentration followed with a mean importance drop of approximately 0.028. The one-hot encoded year indicators, representing temporal trends, generally showed lower importance values (ranging from approximately 0.001 to 0.006) compared to the primary ozone exposure metrics and population size. These findings suggest that after accounting for broad temporal variations and population size, different aspects of ozone exposure—including its variability, peak levels, and central tendency—are key drivers of the model’s predictive accuracy for CVD mortality within this age cohort.\nSHapley Additive exPlanations\nWhile permutation importance provides a global ranking of feature influence based on model performance degradation, SHAP (SHapley Additive exPlanations) analysis was subsequently employed to elucidate the contributions and directionality of individual predictors to the Random Forest model’s output for log-transformed cardiovascular disease (CVD) mortality rates. The global feature importance, as depicted in the SHAP summary (beeswarm) plot (Figure 7.9), reveals that log-transformed county population size is the most influential feature, exhibiting the widest distribution of SHAP values. This indicates that variations in county population have the largest average impact on model predictions. This ranking, based on the average magnitude of impact on model output, is further clarified by the SHAP feature importance bar plot (Figure 7.10), which also positions log-transformed county population size at the top, followed by annual metrics of daily 8-hour maximum ozone concentrations, specifically the 95th percentile, the mean, the standard deviation, the median, and the maximum—demonstrate substantial importance, each showing a significant spread of SHAP values in the beeswarm plot and high mean absolute SHAP values in the bar plot. One-hot encoded year indicators generally exhibit less impact compared to population and the primary ozone metrics.\nThe SHAP summary plot further clarifies the directionality of these effects. For log-transformed county population size, higher values (indicating more populous counties, shown as red points) predominantly correspond to negative SHAP values, suggesting that larger populations tend to push the model’s prediction of log-transformed CVD mortality rates lower. Conversely, lower population values (blue points) are associated with positive SHAP values, pushing mortality predictions higher. Regarding the ozone metrics, a general pattern emerges where higher values of these exposure indicators (red points)—such as higher 95th percentile ozone, higher mean ozone, or greater standard deviation of daily ozone—are typically associated with positive SHAP values, indicating they contribute to an increase in predicted log-transformed CVD mortality. Conversely, lower ozone metric values (blue points) tend to have negative SHAP values, pushing predictions lower. The year indicators show varying directional impacts, reflecting year-specific adjustments to the baseline mortality prediction.\n\n\n\n\n\n\nFigure 7.9: for log_popestimate, high values (red dots) tend to have negative SHAP values (pushing the prediction of log_cvd_mortality_rate lower), while low values (blue dots) tend to have positive SHAP values (pushing the prediction higher). This suggests that, after accounting for other factors, counties with larger populations tend to have lower predicted log CVD mortality rates, and vice-versa. This finding that might relate to urbanicity, access to care, or other factors correlated with population size that aren’t fully captured by other variables. For Ozone metrics, it appears that higher feature values (red dots) generally correspond to positive SHAP values (pushing the prediction of log_cvd_mortality_rate higher), and lower feature values (blue dots) correspond to negative SHAP values. This is the direction we would generally expect if higher ozone is associated with higher CVD mortality. The spread indicates the magnitude of this push. The impact of ‘year’ dummies is more about specific years being associated with higher or lower rates compared to the baseline year (2001). For example, 2010 has many red dots (high value, meaning it is 2010) pushing predictions slightly higher.\n\n\n\n\n\n\n\n\n\nFigure 7.10: This plot confirms the ranking from the beeswarm plot based on the mean absolute SHAP value (average impact on model output magnitude). log_popestimate is at the top, ozone metrics (p95_ozone_pred, mean_ozone_pred, std_ozone_pred, median_ozone_pred, max_ozone_pred) are the next most important group of features, all showing substantial impact. The year dummies have lower mean absolute SHAP values. log_popestimate is more prominent in SHAP importance here than it was in permutation importance where std_ozone_pred was first. This can happen as they measure importance differently.\n\n\n\nPopulation Size and Ozone Interactions\nCounty population size, after log-transformation, emerged as a highly influential predictor of log-transformed CVD mortality rates, as depicted in its SHAP dependence plot Figure 7.11. The plot illustrates a generally negative association: as the log-transformed population estimate increases, indicating more populous counties, the corresponding SHAP values tend to decrease. This suggests that, after accounting for ozone exposure metrics and year, larger county population size contributes to a lower predicted CVD mortality rate within the model. The relationship exhibits non-linearity, with a steeper decrease in SHAP values observed for initial increases in log-population, followed by a flattening or more variable trend at very high population levels. This pattern indicates that while an increase in population size generally has a protective association in the model, the marginal benefit may diminish for the most populous counties.\nThe interaction with the annual median of daily 8-hour maximum ozone concentrations (indicated by the color scale) further refines this interpretation. For counties with lower log-transformed population estimates (less populous counties, represented by bluer points on the interaction scale if median ozone is low, or redder if median ozone is high), higher median ozone levels appear to be associated with higher SHAP values for population (pushing the overall mortality prediction higher, or attenuating the protective effect of being a small county if the SHAP values for population are positive for small populations). Conversely, for more populous counties, the interaction with median ozone is less distinct, though the overall trend of larger populations being associated with lower predicted mortality persists. This suggests that the generally observed lower predicted CVD mortality in more populous counties might be less pronounced or even counteracted when median ozone levels are concurrently high, particularly in less populous areas where other unmeasured factors correlated with both rurality and higher median ozone could be at play. Understanding this interaction is crucial, as population size often serves as a proxy for various socioeconomic, environmental, and healthcare access factors that can modify the impact of environmental exposures like ozone.\n\n\n\n\n\n\nFigure 7.11: High values (red dots) tend to have negative SHAP values (pushing the prediction of the log of CVD mortality lower), while low values (blue dots) tend to have positive SHAP values (pushing the prediction higher). This suggests that, after accounting for other factors, counties with larger populations tend to have lower predicted log CVD mortality rates, and vice-versa. This finding that might relate to urbanicity, access to care, or other factors correlated with population size that aren’t fully captured by other variables.\n\n\n\nPeak Ozone Exposure\nThe influence of peak ozone exposures, represented by the annual 95th percentile of daily 8-hour maximum ozone concentrations, on predicted log-transformed cardiovascular disease (CVD) mortality rates was examined using SHAP dependence plots (Figure Figure 7.12). A generally positive association was observed: as the 95th percentile ozone levels increased from approximately 40 ppb up to around 65-70 ppb, their corresponding SHAP values tended to increase. This indicates that higher peak ozone exposures in this range contribute to higher predicted CVD mortality rates within the age group modeled. Beyond approximately 70-75 ppb, where fewer data points exist, the trend in SHAP values became less distinct and more variable, suggesting a potential plateau or a more complex relationship at very high peak exposure levels.\nThe analysis further revealed an interaction between peak ozone exposure and county population size (log-transformed). For counties experiencing lower to moderate levels of peak ozone (50-65 ppb), those with larger populations (indicated by higher log_popestimate values, shown as redder points on the plot) generally exhibited SHAP values for peak ozone that were closer to zero or slightly lower than those for less populous counties (bluer points). This suggests that, for a given level of peak ozone within this range, the model attributes a somewhat attenuated increase in predicted CVD mortality risk if the county is more populous, or a comparatively larger increase if it is less populous. At the highest observed levels of peak ozone ( &gt;70 ppb), while the interaction was less clearly defined, many of the instances where peak ozone had the largest positive impact on predicted mortality (highest SHAP values) were associated with counties of lower to moderate population sizes. This complex interaction implies that the impact of high peak ozone days on CVD mortality risk, as learned by the model, may be modulated by county population size or unmeasured characteristics correlated with it.\n\n\n\n\n\n\nFigure 7.12: Higher peak ozone exposures (p95_ozone_pred) are generally associated with an increased predicted risk of CVD mortality. This effect might be somewhat more pronounced or have a higher baseline impact in less populous counties. As p95_ozone_pred increases from around 40 ppb to about 65-70 ppb, there’s a clear positive trend in its SHAP value.\n\n\n\nDaily Ozone Variability\nSimilarly, the variability of daily ozone, represented by the annual standard deviation of daily 8-hour maximum ozone concentrations, demonstrated a notable influence on predicted log-transformed CVD mortality rates (Figure Figure 7.13). Generally, an increase in the standard deviation of daily ozone was associated with higher SHAP values, indicating that greater day-to-day variability in ozone concentrations tends to elevate the predicted CVD mortality risk. This relationship appeared somewhat non-linear, with the impact potentially steepening at lower levels of variability and becoming more dispersed at higher standard deviations.\nAn interaction was also observed between ozone variability and the annual mean daily 8-hour maximum ozone concentration. For any given level of ozone variability, instances where the mean ozone concentration was also high (indicated by redder points on the plot) typically corresponded to higher positive SHAP values, signifying a greater push towards increased predicted mortality. Conversely, when mean ozone levels were low (bluer points), the positive impact of ozone variability on predicted mortality was generally attenuated. The largest positive SHAP values for ozone variability (around 0.2) were observed when both the standard deviation of daily ozone and the annual mean ozone concentration were high (standard deviation &gt;17.5 ppb). This suggests that the health risk associated with ozone exposure is not solely dependent on average concentrations but is also amplified by the instability or fluctuation of daily ozone levels, particularly when average levels are already elevated.\n\n\n\n\n\n\nFigure 7.13: Higher variability in daily ozone (std_ozone_pred) is associated with increased predicted CVD mortality risk, and this effect appears to be amplified when the average ozone level (mean_ozone_pred) is also high. This suggests that not just the average exposure, but also the fluctuation or instability of ozone levels, is important.\n\n\n\nMedian Ozone Non-linearity\nThe relationship between the annual median of daily 8-hour maximum ozone concentrations and predicted log-transformed CVD mortality also exhibited non-linearity and interaction effects ( Figure 7.14). From approximately 20 ppb up to around 40-42 ppb, an increase in median ozone levels generally corresponded to higher SHAP values, indicating a positive association with predicted CVD mortality. However, beyond this range (approximately &gt;42-45 ppb), the trend appeared to reverse or become considerably more scattered, with higher median ozone values sometimes associated with negative SHAP values, suggesting a complex, non-monotonic relationship.\nThis pattern was further modulated by the annual 95th percentile of daily ozone (representing peak exposure days). In the range where median ozone showed a positive association with predicted mortality (30-42 ppb), the presence of high peak ozone days (indicated by redder points) tended to amplify this positive impact, resulting in larger SHAP values. Interestingly, in the region where very high median ozone levels (&gt;45 ppb) began to show a less clear or even negative association with predicted mortality, many of these instances also corresponded to high peak ozone days (red or purple points). This complex interplay suggests that while moderate increases in typical (median) ozone levels are associated with increased predicted CVD risk, particularly when extreme peak days are also common, the effect of very high median ozone levels might be saturated or interact differently with other factors when peak exposures are concurrently high.\n\n\n\n\n\n\nFigure 7.14: Moderate increases in median_ozone_pred are associated with increased predicted CVD mortality risk, especially when peak ozone days (p95_ozone_pred) are also frequent/high. However, the effect of very high median ozone levels is less clear and might even be negative in the model, particularly when peak exposures are also high, suggesting a complex interplay.\n\n\n\nAnnual Maximum Ozone Impacts\nFurthermore, the impact of the annual maximum of daily 8-hour maximum ozone concentrations on predicted log-transformed CVD mortality rates also showed a dependency on county population size (Figure 7.15). A general positive trend was observed where an increase in maximum ozone levels, from approximately 50 ppb up to around 90-100 ppb, corresponded to an increase in SHAP values, thereby pushing predicted CVD mortality higher. Beyond 100 ppb, this relationship became more variable, with the positive impact potentially diminishing, although data points in this higher range were less frequent.\nThe interaction with county population size (log-transformed) indicated that counties with larger populations (redder points) generally exhibited SHAP values for maximum ozone that were closer to zero or slightly negative, even at higher maximum ozone levels. In contrast, less populous counties (bluer points) displayed a clearer positive trend between maximum ozone and its SHAP value, suggesting a stronger contribution to increased predicted mortality in these areas. This implies that the model perceives the risk associated with high maximum daily ozone exposures as being more pronounced or apparent in counties with smaller populations, while more populous counties appear to have a dampening effect on this specific risk factor.\n\n\n\n\n\n\nFigure 7.15: Higher maximum daily ozone exposures are generally associated with increased predicted CVD mortality risk, but this effect appears to be more pronounced in counties with smaller populations.\n\n\n\nAnnual Mean Ozone Concentrations\nThe influence of the annual mean of daily 8-hour maximum ozone concentrations on predicted log-transformed CVD mortality rates is central to the primary research question (Figure 7.16). The SHAP dependence plot reveals a generally positive trend for this variable: as mean ozone levels increase from approximately 25 ppb to around 40 ppb, the corresponding SHAP values tend to rise, indicating that higher average ozone exposures contribute to an increase in the model’s prediction of CVD mortality risk. Beyond approximately 40-45 ppb, this positive relationship becomes less distinct, with SHAP values becoming more scattered and potentially showing a slight decrease or plateau. This suggests a non-linear effect, where the impact of increasing mean ozone on predicted mortality might diminish or become more variable at higher average concentration levels, possibly due to other factors becoming more influential or a saturation in the exposure-response relationship as captured by the model.\nThis relationship is further nuanced by interaction with the annual 95th percentile of daily ozone (representing peak exposure days), indicated by the color scale on the plot. Instances where peak ozone days are more frequent or intense (redder points) tend to be associated with higher SHAP values for mean ozone, particularly in the mid-range of mean ozone concentrations (e.g., 35-45 ppb). This observation implies an amplification effect: the adverse impact associated with a given level of mean ozone on predicted CVD mortality appears to be greater when a county also experiences more extreme peak ozone days. This interaction highlights the importance of considering not only average ozone exposure but also the occurrence of high-pollution episodes when assessing CVD risk.\n\n\n\n\n\n\nFigure 7.16: From ~25 ppb to ~40 ppb, there’s a general trend of increasing SHAP values, meaning higher mean ozone tends to push the predicted log_cvd_mortality_rate higher. Beyond ~40-45 ppb, the relationship becomes less clear or might even slightly decrease or become more variable. This suggests a potential non-linear effect or that other factors become more dominant at very high mean ozone levels\n\n\n\n\n\n7.4.4 Summary of Preliminary Random Forest Findings\nThis preliminary analysis, employing a Random Forest regressor, aimed to establish a baseline understanding of the relationships between various annual county-level ozone exposure metrics and log-transformed cardiovascular disease (CVD) mortality rates for adults aged 35-64 years, while controlling for county population size (log-transformed) and year. The model demonstrated notable predictive capability, achieving a test set R² of approximately 0.470 and an Out-of-Bag R² of 0.479. On the original scale of mortality rates (deaths per 100,000), this translated to a Mean Absolute Error (MAE) of approximately 24.3 on the test set. Learning curve analysis indicated that while the model effectively learned from the training data (training RMSE ≈ 0.124), its generalization performance (cross-validation RMSE ≈ 0.271) plateaued, suggesting that the current feature set captures a substantial portion of the explainable variance for this model architecture. The Random Forest significantly outperformed a baseline linear regression model (Test R² ≈ 0.137), underscoring the presence of non-linear relationships or interactions among the predictors.\nFeature importance analyses, including both permutation importance and SHAP values, consistently identified metrics of ozone exposure as primary drivers of the model’s predictions within this age-stratified context. Permutation importance highlighted the annual standard deviation of daily maximum ozone concentrations as the most influential feature (mean importance drop ≈ 0.052), followed by the 95th percentile, median, and mean ozone concentrations. SHAP analysis further elucidated these relationships, confirming log-transformed county population size as having the largest average impact on predictions globally, followed closely by the suite of ozone metrics. SHAP dependence plots revealed that higher values across various ozone exposure characterizations (mean, median, 95th percentile, maximum, and standard deviation) generally corresponded to an increased predicted log-transformed CVD mortality rate, often exhibiting non-linear patterns and interactions with population size or other ozone metrics. For instance, the adverse impact associated with mean ozone levels appeared to be amplified in the presence of more frequent high-peak ozone days. Similarly, the impact of peak and maximum ozone exposures on predicted mortality was more pronounced in less populous counties. These preliminary findings establish a statistical association between ozone exposure and CVD mortality and highlight the complex, non-linear nature of these associations, providing a crucial benchmark for the subsequent application of more advanced spatiotemporal deep learning models like CNNs and GNNs, which are hypothesized to better capture these intricate dependencies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "04-prelim-analysis.html#stratification",
    "href": "04-prelim-analysis.html#stratification",
    "title": "5  Preliminary Analysis",
    "section": "9.1 Stratification",
    "text": "9.1 Stratification\n\nOverall Population (baseline)\nAge-specific groups: 35-64 years vs. 65+ years\nSex-specific groups: Men vs. Women\nRace-specific groups: Exploring disparities among different races.\nlink back to 02-data-collection.qmd.\nDescribe selected data subset.\nDetailed description of analytical approach (CNN/GNN models, software, computational resources).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Preliminary Analysis</span>"
    ]
  },
  {
    "objectID": "05-aim3.html",
    "href": "05-aim3.html",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "",
    "text": "7 Introduction and Foundational Acknowledgement\nThis chapter outlines the proposed methodology for Specific Aim 3: the development and application of a novel deep learning framework to identify, classify, and monitor unconventional oil and gas (UOG), or “fracking,” sites. The successful completion of the preceding project aims; engineering a sophisticated ETL+RAG pipeline and conducting a baseline machine learning analysis, directly highlighted the critical need for more advanced spatiotemporal models in environmental health. This aim is designed to address that identified gap head-on.\nThe conceptualization and design of the methodologies presented here were made possible by the foundational knowledge acquired in the “AI with Geospatial Big Data” (GOG 594) course at the University at Albany. The course, taught not only the theoretical underpinnings of advanced neural networks but also the practical, hands-on skills necessary to apply them to complex, real-world geospatial problems. Its approach to data processing, model implementation, and the use of tools like Quarto for reproducible research provided the essential toolkit for envisioning and structuring this final stage of the dissertation.\nThe objective of this aim is twofold. First, to create a high-resolution, actionable dataset of UOG infrastructure and activity by applying state-of-the-art computer vision techniques to satellite imagery. Second, to leverage this dataset to monitor associated environmental indicators, specifically methane emissions, providing a new layer of data for future epidemiological studies. To ensure methodological depth and feasibility within the doctoral timeline, this chapter will develop and validate this entire framework through a case study of UOG development in the Commonwealth of Pennsylvania, a region with a long history of activity and excellent data availability.\nThe scientific novelty of this research is threefold. First, it pioneers a multi-modal deep learning framework that moves beyond static well counts to create a dynamic, activity-based UOG exposure inventory from satellite imagery. Second, by linking this high-resolution exposure data to census-tract level health microdata, it directly confronts and mitigates the core methodological challenges of ecological fallacy and exposure misclassification that have limited previous studies. Finally, it integrates these components within a hybrid analytical framework, using AI for predictive discovery and established statistical models for formal inference, providing a comprehensive and robust model for environmental health risk assessment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#software-tools-and-libraries",
    "href": "05-aim3.html#software-tools-and-libraries",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "14.1 Software Tools and Libraries",
    "text": "14.1 Software Tools and Libraries\nThis project leverages open-source software tools to ensure reproducibility, efficiency, and ease of collaboration:\n\nPyTorch / PyTorch Lightning: efficient neural network training and inference\nUltralytics YOLOv8: framework for CNN-based object detection\nHugging Face Model Hub: Source of pre-trained models, including YOLOv8 checkpoints for transfer learning\nCVAT (Computer Vision Annotation Tool): software for bounding box annotation and iterative labeling workflows\nGDAL, rasterio, geopandas: Tools for geospatial data acquisition, preprocessing, and spatial masking workflows\n\nThese tools ensure reproducibility, efficiency, and ease of collaboration, enabling integration of complex components within the proposed methodological framework.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#data-acquisition-and-preprocessing",
    "href": "05-aim3.html#data-acquisition-and-preprocessing",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "14.2 Data Acquisition and Preprocessing",
    "text": "14.2 Data Acquisition and Preprocessing\nSatellite Imagery:\nWe will acquire high-resolution multispectral satellite imagery (from Sentinel-2, Planet Labs, or NAIP) for regions with known UOG development.\nTraining Data Scaffolding (Permit-Guided):\nTo efficiently generate a dataset for model training, this project will leverage publicly available oil and gas well permit databases from state regulatory agencies. The latitude and longitude coordinates from these official records will be used to programmatically clip thousands of small image “snippets” from the satellite imagery. This approach ensures the manual annotation effort is focused on relevant imagery containing known positive examples, forming the foundation for the training process described in the next section.\nInference Search Area Definition (Tax Parcel-Guided):\nFor the final model deployment phase, where the goal is to discover all sites (both known and unknown), a broad search area will be defined using county tax parcel data. This data will create a “mask” to programmatically exclude areas where UOG development is highly improbable (e.g., dense residential zones, protected lands). Running the model over this large, plausible search area is what enables the discovery of unrecorded or legacy sites.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#training-data-creation",
    "href": "05-aim3.html#training-data-creation",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "14.3 Training Data Creation",
    "text": "14.3 Training Data Creation\nCreating a large, accurate, and diverse training dataset is critical, yet manual annotation at a national scale is not possible for a single researcher. Therefore, a modern, “human-in-the-loop” workflow will be employed to make this large-scale labeling task manageable and to produce a highly robust final training set. The process is structured as follows:\n\nSeed Dataset Creation (Manual Annotation):\nA manageable initial subset of image snippets (2,000–5,000) will be labeled by hand in the CVAT annotation tool to create the initial high-quality ground truth.\nInitial Model Training (V1):\nThis seed dataset will be used to train a preliminary version (V1) of the YOLO/EfficientDet object detection model.\nSemi-Automated Labeling:\nThe V1 model will then be deployed on the remaining tens of thousands of unlabeled snippets to generate “pre-labels.” The human annotator’s role then shifts to that of a reviewer, rapidly confirming correct predictions and correcting inaccurate ones.\nFinal “Expert” Model Training:\nThe full, high-quality dataset comprising both the manually labeled seed set and the much larger set of human-corrected semi-automated labels will be used to train the final, “expert” version of the model, likely by fine-tuning the V1 model.\n\nThis entire iterative process will be used to draw bounding boxes around key features, including:\n\nWell Pads: The overall footprint of the development site.\nStorage Tanks: Identifying individual or clusters of tanks.\nActive Equipment: Such as drilling rigs or vehicle fleets, which serve as a proxy for site activity.\nAbandoned Sites: Identifying sites with visible infrastructure but no signs of recent activity.\n\nTo ensure the objectivity and reproducibility of these labels, an Inter-Annotator Agreement (IAA) analysis will be performed on a subset of the data. By having a second independent annotator label the same set of images and calculating metrics such as Intersection over Union (IoU) for bounding boxes and Cohen’s Kappa for categorical labels, the reliability of the annotation process will be quantitatively validated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#model-architecture-and-training",
    "href": "05-aim3.html#model-architecture-and-training",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "14.4 Model Architecture and Training",
    "text": "14.4 Model Architecture and Training\nDetailed information on the specialized computational hardware used for training and inference (multi-GPU workstation with 48GB GPUs) is provided in Appendix C: Computational Hardware. Leveraging these dedicated computational resources, we will employ a transfer learning approach, which is standard practice for complex object detection tasks. Specifically, a state-of-the-art YOLO (You Only Look Once) variant, such as YOLOv8 pre-trained on the COCO dataset, will be fine-tuned on our annotated UOG site dataset. This approach enables the model to leverage its existing knowledge of general shapes and textures, adapting rapidly to the specific task of identifying UOG infrastructure. Utilizing our robust computational setup significantly accelerates training times, facilitates larger batch sizes, and ensures high-quality, generalizable model performance.\n\nData Curation:\nThe training dataset will be constructed using stratified sampling to ensure representation from different regions within Pennsylvania’s Appalachian Basin (the wet gas southwestern region vs. the dry gas northeastern region) and across different seasons (leaf-on vs. leaf-off). This approach ensures the model is robust for the case study area, while the methodology itself is designed to be directly scalable to other basins in future work.\nData Augmentation:\nThe training data will be programmatically augmented by applying random geometric (flips, rotations) and photometric (brightness, contrast, hue) transformations. This forces the model to learn the fundamental features of UOG infrastructure rather than superficial characteristics tied to a specific region or season.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#methane-data-acquisition",
    "href": "05-aim3.html#methane-data-acquisition",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "15.1 Methane Data Acquisition",
    "text": "15.1 Methane Data Acquisition\nWe will acquire publicly available methane concentration data from sources such as the Sentinel-5P TROPOMI instrument or the more recent MethaneSAT. This provides regular, global coverage of atmospheric methane levels.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#spatiotemporal-analysis-with-convlstm",
    "href": "05-aim3.html#spatiotemporal-analysis-with-convlstm",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "15.2 Spatiotemporal Analysis with ConvLSTM",
    "text": "15.2 Spatiotemporal Analysis with ConvLSTM\nTo model the movement and concentration of methane plumes over time, we will use a spatiotemporal deep learning model such as a ConvLSTM (Convolutional Long Short-Term Memory) network. This architecture is ideal for this task because:\n\nThe Convolutional (CNN) component analyzes the spatial structure of the methane plume in each individual satellite image.\nThe Recurrent (LSTM) component analyzes the sequence of these images over time, learning how plumes emerge, move with wind patterns, and dissipate.\n\nThis model will be trained to predict methane concentrations across our study region.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#leak-detection-and-site-attribution",
    "href": "05-aim3.html#leak-detection-and-site-attribution",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "15.3 Leak Detection and Site Attribution",
    "text": "15.3 Leak Detection and Site Attribution\nBy overlaying the outputs from the ConvLSTM model with the geolocated UOG site inventory from Phase 1, we can systematically:\n\nIdentify Anomalous Methane Plumes: Detect methane concentrations that are significantly higher than the regional baseline.\nAttribute Emissions: Correlate these plumes with specific active UOG sites to monitor their emissions.\nDetect Leaks from Abandoned Sites: Critically, we can also identify significant methane sources originating from sites previously classified as abandoned, providing a powerful tool for environmental monitoring and public health risk assessment.\n\nThe final output of this aim will be a dynamic, spatially explicit dataset linking UOG infrastructure to its real-world methane emissions, creating a novel and valuable resource for the environmental health sciences community.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#health-outcome-data-acquisition-of-restricted-access-microdata",
    "href": "05-aim3.html#health-outcome-data-acquisition-of-restricted-access-microdata",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "16.1 Health Outcome Data: Acquisition of Restricted-Access Microdata",
    "text": "16.1 Health Outcome Data: Acquisition of Restricted-Access Microdata\nTo overcome the limitations of publicly available aggregated data, this research will use individual-level, anonymized health data containing fine-scale geographic identifiers. The target dataset is the National Center for Health Statistics (NCHS) National Vital Statistics System (NVSS) Mortality Data. This dataset contains individual death records, including cause of death and, critically, the census tract of residence.\nAcquiring this data is a formal process that must be conducted through the New York Federal Statistical Research Data Center (NYRDC), of which the University at Albany is a member. The process involves the following key steps:\n\nProposal Development:\nA detailed research proposal will be submitted to the NYRDC. This proposal will outline the research questions, the specific data variables required (year of death, census tract, cause of death, age, race, sex), the statistical methodology, and a plan for ensuring data confidentiality.\nInstitutional Review Board (IRB) Review:\nThe proposal must be approved by the University at Albany’s IRB. This ensures the research is conducted ethically and that all protocols for protecting sensitive human subjects’ data are in place.\nExecution of a Data Use Agreement (DUA):\nUpon provisional approval, a DUA from NCHS will be processed through the University’s Office of Research and Regulatory Compliance. This legal agreement governs the terms of data use, storage, and security.\nSecure Data Analysis:\nAll analyses using the restricted microdata will be performed within the secure environment of an NYRDC facility to ensure compliance with all federal data protection requirements.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#analytical-approach-prediction-and-inference",
    "href": "05-aim3.html#analytical-approach-prediction-and-inference",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "16.2 Analytical Approach: Prediction and Inference",
    "text": "16.2 Analytical Approach: Prediction and Inference\nTo fully address the research questions, a hybrid analytical framework will be employed, leveraging the distinct strengths of deep learning for prediction and discovery, and established statistical models for inference. This two-pronged strategy allows the project to predict where health risks may be highest and quantify the magnitude of those risks in a more traditional statistically rigorous manner.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#predictive-modeling-and-pattern-discovery",
    "href": "05-aim3.html#predictive-modeling-and-pattern-discovery",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "16.3 Predictive Modeling and Pattern Discovery",
    "text": "16.3 Predictive Modeling and Pattern Discovery\nThe primary objective of this phase is to achieve the highest possible predictive accuracy for health outcomes and to uncover complex, non-linear spatiotemporal patterns that traditional models often miss.\n\nMethodology: The high-resolution UOG and methane exposure datasets (from Phases 1 & 2) will be used as inputs for the trained Convolutional Neural Network (CNN) and Graph Neural Network (GNN) architectures. These models are specifically designed to learn from the spatial grid and network structures of the data, capturing localized patterns and inter-county dependencies (i.e., spatial spillover effects).\nKey Outputs:\n\nPredictive Risk Maps: Generation of spatially explicit maps identifying census tracts predicted to have the highest health risk in future time periods.\n\nAI-Driven Feature Importance: Application of model interpretation techniques (SHAP) to visualize the non-linear effects and complex interactions discovered by the deep learning models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#statistical-modeling-for-inference-and-effect-size-estimation",
    "href": "05-aim3.html#statistical-modeling-for-inference-and-effect-size-estimation",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "16.4 Statistical Modeling for Inference and Effect Size Estimation",
    "text": "16.4 Statistical Modeling for Inference and Effect Size Estimation\nThe objective of this phase is to complement the predictive findings by quantifying the specific, interpretable effect size of UOG exposure on mortality while controlling for a wide range of confounders.\n\n16.4.1 Methodology: Negative Binomial Regression\nThe relationship between UOG exposure and health outcomes (mortality counts) will be analyzed using Negative Binomial regression. This type of Generalized Linear Model (GLM) is standard for analyzing count data that exhibits overdispersion (i.e., where the variance in the data is greater than the mean), which is common in epidemiological data.\nThe selection of a Negative Binomial model over more conventional methods like simple Linear Regression or ANOVA is a deliberate choice dictated by the fundamental nature of the outcome data. Linear models, including ANOVA, are inappropriate for this analysis as they assume a continuous, normally distributed outcome with constant variance.\nApplying them to mortality counts which are discrete, non-negative, and exhibit increasing variance with the mean, would violate these core assumptions and could lead to biased estimates and impossible predictions, such as negative death counts. The Negative Binomial framework is purpose-built to overcome these challenges, properly handling count data and its characteristic overdispersion while allowing for the estimation of interpretable risk ratios.\nA Negative Binomial model is defined by two key components:\n\nThe Probability Distribution\nThe number of deaths \\((y)\\) in a given census tract is assumed to follow a Negative Binomial distribution. This distribution is defined by its mean \\((\\mu)\\) and a dispersion parameter \\((\\theta)\\) that models the extra variation not captured by a simpler Poisson model. The probability of observing exactly \\((y)\\) deaths is given by the Probability Mass Function (PMF):\n\\[\nP(Y=y \\mid \\mu,\\theta) = \\frac{\\Gamma(y+\\theta)}{\\Gamma(y+1)\\,\\Gamma(\\theta)} \\left(\\frac{\\theta}{\\theta+\\mu}\\right)^{\\theta} \\left(\\frac{\\mu}{\\theta+\\mu}\\right)^{y}\n\\]\nwhere \\(y\\) is the observed mortality count, \\(\\mu\\) is the expected mean count for an observation, \\(\\theta\\) is the dispersion parameter, and \\(\\Gamma()\\) is the gamma function.\nThe Link Function and Regression Structure\nThe model connects the expected mean count \\((\\mu_i)\\) for each census tract \\((i)\\) to a set of predictor variables \\((X)\\) using a natural logarithm link function. This ensures the predicted counts are always positive and that the predictors have a multiplicative effect on the outcome. The general form of the regression equation is:\n\\[\n\\log(\\mu_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip}\n\\]\nwhere \\(\\beta_0\\) is the intercept and \\(\\beta_1, \\dots, \\beta_p\\) are the regression coefficients representing the change in the log of the mean count for a one-unit change in each predictor.\n\nInsights from the AI model’s SHAP analysis will directly inform the structure of this regression model. For example, identified non-linear exposure-response relationships will be modeled using flexible terms such as restricted cubic splines to ensure the inferential model is as robust and realistic as possible.\n\n\n16.4.2 Confounder Control\nTo account for other factors that influence health, census-tract level covariates will be incorporated from the CDC’s Social Vulnerability Index (SVI) (data on poverty, education, income). Individual-level demographic variables available in the restricted health data (age, race, sex) will be included directly in the model.\n\n\n16.4.3 Full Model Specification\nTo properly analyze mortality rates rather than raw counts, the model must account for the population size in each census tract. This is accomplished by including the logarithm of the population as an offset term, which effectively acts as a predictor with a coefficient fixed at 1. The full conceptual model for this analysis is:\n\\[\n\\log(\\mu_i) = \\beta_0 + \\beta_1\\,s(\\text{UOG Exposure}_i) + \\sum_{j=2}^{p} \\beta_j\\,X_{ij} \\;+\\; \\text{offset}\\!\\bigl(\\log(\\text{Population}_i)\\bigr)\n\\]\nHere, \\(s(\\text{UOG Exposure}_i)\\) represents the potentially non-linear term for UOG exposure, and \\(\\sum_{j=2}^{p} \\beta_j\\,X_{ij}\\) represents all other confounders (demographic and socioeconomic). This formulation is equivalent to modeling the mortality rate \\((\\mu_i/\\text{Population}_i)\\), and the resulting coefficients \\((\\beta_j)\\) are exponentiated \\((e^{\\beta_j})\\) to yield interpretable Risk Ratios (RR).\n\n\n16.4.4 Key Outputs\n\nRisk Ratios (RR): A quantifiable measure of association (the percent increase in mortality risk for each unit increase in UOG exposure), complete with 95% confidence intervals and p-values for formal hypothesis testing.\n\nBy integrating these two approaches, this research will not only predict where risk is highest but also explain how much risk is attributable to UOG exposure, providing a comprehensive and actionable set of findings.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#phase-1-cnn-object-detector-quality",
    "href": "05-aim3.html#phase-1-cnn-object-detector-quality",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "17.1 Phase 1: CNN Object Detector Quality",
    "text": "17.1 Phase 1: CNN Object Detector Quality\nThe quality of the UOG site detector will be assessed using standard computer vision metrics on a held-out test set:\n\nIntersection over Union (IoU): Measures the accuracy of predicted bounding boxes against ground-truth boxes.\nPrecision & Recall: Measure the rates of false positives and false negatives, respectively.\nMean Average Precision (mAP): The primary, single-number metric that summarizes overall performance across all object classes and detection thresholds.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#phase-2-methane-model-quality",
    "href": "05-aim3.html#phase-2-methane-model-quality",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "17.2 Phase 2: Methane Model Quality",
    "text": "17.2 Phase 2: Methane Model Quality\nThe spatiotemporal ConvLSTM model for methane prediction is a regression task and will be evaluated using:\n\nRoot Mean Squared Error (RMSE) & Mean Absolute Error (MAE): To quantify the average prediction error in parts-per-billion (ppb).\nR-squared (R²): To measure the proportion of variance in the real methane data explained by the model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#phase-3-epidemiological-model-quality",
    "href": "05-aim3.html#phase-3-epidemiological-model-quality",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "17.3 Phase 3: Epidemiological Model Quality",
    "text": "17.3 Phase 3: Epidemiological Model Quality\nThe two prongs of the health analysis require different evaluation methods:\n\nPredictive AI Models (CNN/GNN): Performance will be measured using RMSE, MAE, R², and Pearson Correlation between the predicted and actual health outcomes on a held-out test set of census tracts/years.\nInferential Statistical Model (Negative Binomial): The focus is on model fit and the validity of the conclusions. Quality will be assessed via:\n\nGoodness-of-Fit Statistics (AIC/BIC): To compare model variations and select the best-fitting specification.\nSignificance and Confidence Intervals: The p-value and 95% CI for the UOG exposure term will determine the statistical significance and precision of the primary finding.\nResidual Analysis: To validate the model’s underlying statistical assumptions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#year-1-focus-on-phase-1---cnn-model-development-and-site-inventory-creation",
    "href": "05-aim3.html#year-1-focus-on-phase-1---cnn-model-development-and-site-inventory-creation",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "18.1 Year 1: Focus on Phase 1 - CNN Model Development and Site Inventory Creation",
    "text": "18.1 Year 1: Focus on Phase 1 - CNN Model Development and Site Inventory Creation\n\nMonths 1–4: Data Acquisition and Preparation. This period will be dedicated to acquiring high-resolution satellite imagery and the state-level UOG permit databases. The primary milestone will be the programmatic generation of tens of thousands of image snippets centered on permitted well locations.\n\nMonths 4–8: Iterative Dataset Annotation This phase involves the multi-step process of creating the seed dataset, training the V1 model, and performing semi-automated labeling and review to build the full training dataset for the Pennsylvania case study. The IAA analysis will also be conducted.\nMonths 8–12: “Expert” Model Training and Large-Scale Deployment. This final phase involves training the expert model on the full dataset, validating its performance, and deploying it over the tax-parcel-masked search area to generate the final, comprehensive UOG inventory.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "05-aim3.html#year-2-spatiotemporal-analysis-health-data-acquisition-and-epidemiological-linkage",
    "href": "05-aim3.html#year-2-spatiotemporal-analysis-health-data-acquisition-and-epidemiological-linkage",
    "title": "6  Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)",
    "section": "18.2 Year 2: Spatiotemporal Analysis, Health Data Acquisition, and Epidemiological Linkage",
    "text": "18.2 Year 2: Spatiotemporal Analysis, Health Data Acquisition, and Epidemiological Linkage\n\nMonths 1–9 (Parallel Track 1): Methane & Exposure Modeling\nAcquiring and processing methane data and integrating it with the UOG inventory from Phase 1 to create the final, dynamic exposure dataset.\nMonths 1–9 (Parallel Track 2): Health Data Acquisition\nthe formal application process for the restricted-access NCHS mortality data will be undertaken through the NYRDC. This includes proposal writing, IRB submission, and DUA processing, which can take months.\nMonths 9–12: Final Integration and Analysis.\n\nLinking the final exposure dataset with the approved NCHS health microdata.\n\nExecuting the hybrid analytical plan: running the predictive GNN/CNN models and the inferential Negative Binomial models.\n\nGenerating final results, including predictive maps and risk ratios.\n\nCompleting the initial draft of the chapter’s findings.\n\n\nThe final output of this aim will culminate in a comprehensive environmental health assessment. This includes both the creation of a novel, high-resolution dataset linking UOG infrastructure to methane emissions and its application in a hybrid analytical framework. By integrating deep learning for predictive risk mapping with rigorous statistical modeling for inferential risk quantification, this research will provide a multi-faceted understanding of the potential public health consequences of UOG development, offering a valuable new resource for researchers, policymakers, and communities.\n\nAs the methodology is developed, this chapter will be updated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span>"
    ]
  },
  {
    "objectID": "06-technical-report.html",
    "href": "06-technical-report.html",
    "title": "7  Technical Report",
    "section": "",
    "text": "7.1 The ETL Pipeline: From Raw Literature to Structured Knowledge\nThis chapter will server as the technical report for the ETL+HySemRAG methodology and is a work in progress. This chapter will be updated once the draft is complete to describe all phases and theories of operation of the system.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Technical Report</span>"
    ]
  },
  {
    "objectID": "06-technical-report.html#the-etl-pipeline-from-raw-literature-to-structured-knowledge",
    "href": "06-technical-report.html#the-etl-pipeline-from-raw-literature-to-structured-knowledge",
    "title": "7  Technical Report",
    "section": "",
    "text": "Figure 7.1: Graphical Abstract\n\n\n\n\n7.1.1 Corpus Size and Variability\nThe final corpus size in each ETL run is not fixed, rather emerges naturally based on the availability of open-access full-text PDFs via the Unpaywall API. Typically, from an initial candidate pool of approximately 3,400 metadata records retrieved from PubMed, OpenAlex, and Scopus, the number of successfully retrieved full-text documents ranges from around 500 to 1,300. This inherent variability reflects realistic constraints encountered during automated scholarly literature retrieval, specifically the reliance on open-access publications. Importantly, this variability underscores the pipeline’s robustness and adaptability, as subsequent processing stages maintain stable performance metrics (accuracy of data extraction, knowledge graph quality, semantic consistency) regardless of the exact corpus size. This adaptability positions the ETL pipeline as particularly suited to real-world research scenarios where comprehensive subscription-based access may not always be available.\nDemonstration Video\nHere is a video demonstrating the system’s capabilities:\n\n\n\n\n7.1.2 Stage 1: Multi-Source Data Acquisition and Enrichment\nThe initial and most foundational stage of the ETL pipeline is the acquisition of a comprehensive, relevant, and clean corpus of scholarly literature. The objective of this phase is to systematically gather article metadata from multiple disparate sources, each with unique strengths, and then intelligently fuse them into a single, deduplicated, and enriched dataset. This process is orchestrated by a series of high-performance Python modules designed for scalability and robustness, ensuring that the subsequent stages of the pipeline operate on a high-quality foundation.\n\n7.1.2.1 Parallelized Metadata Fetching:\nTo achieve a comprehensive corpus, the system employs a multi-source fetching strategy, simultaneously querying three major scholarly databases: PubMed, for its depth in biomedical and life sciences literature; OpenAlex, for its broad multidisciplinary coverage and rich citation network data; and Elsevier’s Scopus API, for its extensive abstract and citation database, particularly for STEM fields. The GUI allows the user to select any combination of these sources for a given run.\nRecognizing that fetching metadata for thousands of articles via sequential API calls would present a significant performance bottleneck, the system was architected for high-throughput, concurrent data retrieval. The implementation is tailored to the specific architecture and rate-limiting policies of each API. For PubMed, the scripts/fast_pubmed.py module first performs a single, efficient esearch call to retrieve up to 10,000 PubMed IDs (PMIDs) matching a given query. Subsequently, it uses a ThreadPoolExecutor to create a pool of worker threads that fetch the full article metadata in parallel batches using efetch, while adhering to NCBI’s polite API usage policy of approximately 3 queries per second. For OpenAlex and Scopus, the system leverages modern asynchronous I/O via the asyncio and aiohttp libraries within the scripts/fast_openalex.py and scripts/etl_elsevier.py modules. This approach allows the system to initiate hundreds of concurrent HTTP requests, managing them efficiently without being blocked by network latency. An asyncio.Semaphore is used as a throttling mechanism to ensure the number of requests per second does not exceed the official rate limits for each API, preventing errors and ensuring reliable access.\n\n\n7.1.2.2 Data Fusion and Deduplication:\nOnce the parallel fetching processes are complete, the metadata from each source, now held in separate pandas DataFrames, must be combined into a single, unified dataset. This is handled by the merge_and_deduplicate function within the scripts/etl_utils_refined.py module. The function first concatenates the list of DataFrames into one large, heterogeneous dataset.\n\n\nCode\ndef merge_and_deduplicate(from_pubmed_all: List[pd.DataFrame], \n                          from_openalex_all: List[pd.DataFrame], \n                          from_elsevier_all: Optional[List[pd.DataFrame]] = None) -&gt; pd.DataFrame:\n    all_dfs = []\n    if from_pubmed_all: all_dfs.extend(from_pubmed_all)\n    if from_openalex_all: all_dfs.extend(from_openalex_all)\n    if from_elsevier_all: all_dfs.extend(from_elsevier_all)\n    if not all_dfs:\n        logging.warning(\"No DataFrames provided to merge_and_deduplicate.\")\n        return pd.DataFrame()\n    combined_df = pd.concat(all_dfs, ignore_index=True)\n    if combined_df.empty:\n        logging.info(\"Combined DataFrame is empty before deduplication.\")\n        return combined_df\n    deduplicated_df = deduplicate_by_doi_title(combined_df)\n    return deduplicated_df\n\n\nThe subsequent critical step is deduplication, as the same article may be retrieved from multiple databases. A simple row-based drop would be insufficient. Therefore, the system implements a robust, hierarchical deduplication algorithm. First, it normalizes all Digital Object Identifiers (DOIs) by converting them to a consistent lowercase format and removing whitespace. It then identifies and retains the first occurrence of each unique DOI.\n\n\nCode\ndef deduplicate_by_doi_title(df: pd.DataFrame) -&gt; pd.DataFrame:\n    # Create normalized columns\n    if \"DOI\" not in df.columns: df[\"DOI\"] = \"\"\n    if \"Title\" not in df.columns: df[\"Title\"] = \"\"\n    df[\"DOI\"] = df[\"DOI\"].fillna(\"\")\n    df[\"Title\"] = df[\"Title\"].fillna(\"\")\n\n    df[\"DOI_clean\"] = df[\"DOI\"].astype(str).str.lower().str.strip()\n    df[\"Title_clean\"] = df[\"Title\"].astype(str).str.lower().str.strip()\n    df.sort_values(by=[\"DOI_clean\", \"Title_clean\"], inplace=True, kind='stable')\n    indices_to_keep = set()\n    seen_dois = set()\n    for index, doi in df.loc[df[\"DOI_clean\"] != \"\", \"DOI_clean\"].items():\n        if doi not in seen_dois:\n            indices_to_keep.add(index)\n            seen_dois.add(doi)\n    empty_doi_mask = (df[\"DOI_clean\"] == \"\") & (~df.index.isin(indices_to_keep))\n    seen_titles_empty_doi = set()\n    for index, title in df.loc[empty_doi_mask, \"Title_clean\"].items():\n        if title not in seen_titles_empty_doi:\n            indices_to_keep.add(index)\n            seen_titles_empty_doi.add(title)\n    df_deduplicated = df.loc[list(indices_to_keep)].copy()\n    df_deduplicated.drop(columns=[\"DOI_clean\", \"Title_clean\"], inplace=True)\n    df_deduplicated.reset_index(drop=True, inplace=True)\n    return df_deduplicated\n\n\nFor the remaining records that may lack a DOI, the algorithm performs a secondary deduplication based on a similarly normalized article title. This two-pass strategy ensures that each unique scholarly work is represented by a single, canonical record in the final dataset, preventing data redundancy in all downstream analyses.\n\n\n7.1.2.3 Metadata Enrichment:\nThe initial, merged dataset, while comprehensive, may contain records with incomplete information (e.g., a record from PubMed might be missing a PDF link that OpenAlex has). The final step in the acquisition phase is therefore a dedicated enrichment process, orchestrated by scripts/metadata_enrichment.py. This module takes the entire list of unique DOIs from the deduplicated dataset and once again uses an asynchronous approach to concurrently query both the OpenAlex and Crossref APIs for every article.\n\n\nCode\ndef enrich_metadata_sync(df: pd.DataFrame, crossref_email: Optional[str] = None) -&gt; pd.DataFrame:\n    email = crossref_email or MAILTO\n    dois = df[\"DOI\"].dropna().unique().tolist()\n    enrichment = asyncio.run(enrich_metadata(dois, email))\n\n    for idx, row in df.iterrows():\n        doi = row[\"DOI\"]\n        metadata = enrichment.get(doi, {})\n        if metadata:\n            if pd.isna(row.get(\"Abstract\")) or not str(row[\"Abstract\"]).strip():\n                df.at[idx, \"Abstract\"] = metadata.get(\"Abstract\", row[\"Abstract\"])\n            if pd.isna(row.get(\"ItemType\")) or not str(row[\"ItemType\"]).strip() or row[\"ItemType\"] == \"N/A\":\n                df.at[idx, \"ItemType\"] = metadata.get(\"ItemType\", row.get(\"ItemType\", \"N/A\"))\n            citation_count = metadata.get(\"CitationCount\", row.get(\"CitationCount\"))\n            df.at[idx, \"CitationCount\"] = int(citation_count) if citation_count is not None else 0\n            primary_topic = metadata.get(\"primary_topic\")\n            if primary_topic:\n                df.at[idx, \"primary_topic\"] = json.dumps(primary_topic)\n            else:\n                df.at[idx, \"primary_topic\"] = pd.NA\n            df.at[idx, \"is_published\"] = metadata.get(\"is_published\", row.get(\"is_published\"))\n            df.at[idx, \"is_retracted\"] = metadata.get(\"is_retracted\", row.get(\"is_retracted\"))\n            df.at[idx, \"OpenAlexID\"] = metadata.get(\"OpenAlexID\", row.get(\"OpenAlexID\"))\n    df[\"CitationCount\"] = df[\"CitationCount\"].fillna(0).astype(int)\n\n    return df\n\n\nThe results are then intelligently merged back into the main DataFrame. This process is non-destructive; it is designed to backfill missing information rather than overwrite existing data. For instance, an abstract retrieved from Crossref will only be added to a record if that record’s Abstract field is currently empty. Similarly, fields like ItemType or CitationCount are updated only if the existing data is null or a placeholder. This ensures that the final output is the most complete possible version of each record, combining the strengths of all queried databases.\n\n\n\n\n7.1.3 Stage 2: Asynchronous Full-Text PDF Retrieval\nFollowing the creation of a unified and deduplicated list of scholarly articles, the next critical stage of the pipeline is the acquisition of the full-text content for each entry. This is essential for the deep content analysis, entity extraction, and topic modeling performed in subsequent stages. The system orchestrates this process by leveraging the Digital Object Identifier (DOI) of each article to query the Unpaywall API, a service that indexes legally available open-access versions of scholarly publications. Given that this stage involves potentially thousands of network requests, a high-throughput, asynchronous architecture was implemented to manage this I/O-bound task efficiently.\nThe core retrieval logic, encapsulated in scripts/async_unpaywall.py, is built upon Python’s asyncio and aiohttp libraries. This approach allows the system to initiate and manage thousands of concurrent HTTP requests, preventing the pipeline from being blocked by network latency from any single request. To ensure compliance with Unpaywall’s API usage policies and to maintain system stability, an asyncio.Semaphore is employed as a robust rate-limiting mechanism. The semaphore is initialized with the API_RATE_LIMIT value specified in the project’s configuration file (e.g., 8 queries per second). Each asynchronous worker task must acquire the semaphore before making an API call, guaranteeing that the overall request rate does not exceed the defined limit. The worker function first queries Unpaywall for a given DOI to get a list of potential PDF locations, prioritizing the best_oa_location, and then iteratively attempts to download the file from each candidate URL until a valid PDF is secured.\n\n\nCode\n# In scripts/async_unpaywall.py\n\nasync def _worker(row_idx, doi, title, session):\n    \"\"\"Return (row_idx, pdf_path|None, status_string).\"\"\"\n    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n    # _fetch_json is rate-limited by a semaphore\n    data = await _fetch_json(session, api_url)\n    if not data:\n        return row_idx, None, \"Unpaywall_fail\"\n\n    # Build a list of candidate URLs, prioritizing the best_oa_location\n    candidates = []\n    if (b := data.get(\"best_oa_location\")): candidates.append(b)\n    candidates += data.get(\"oa_locations\", [])\n\n    for loc in candidates:\n        pdf_url = loc.get(\"url_for_pdf\")\n        if not pdf_url:\n            continue\n        # Attempt to download the PDF from the candidate URL\n        pdf_bytes = await _download_pdf(session, pdf_url)\n        if pdf_bytes:\n            # Sanitize filename and save the file\n            safe_name = re.sub(r'[\\\\/*?:\"&lt;&gt;|()]+', '', title)[:100] or doi\n            fname = f\"{safe_name}_{row_idx}.pdf\"\n            path  = os.path.join(PDF_SAVE_FOLDER, fname)\n            with open(path, \"wb\") as f:\n                f.write(pdf_bytes)\n            return row_idx, path, \"Saved\"\n            \n    return row_idx, None, \"No_PDF\"\n\n\nUpon successful download, the PDF is saved to a local directory defined in the configuration, and the main DataFrame is updated with the local file path for that article record. Records for which a full-text PDF could not be successfully retrieved are filtered out at the conclusion of this stage. The resulting DataFrame, now containing only articles with corresponding local PDF files, is then passed to the next stage of the pipeline for content extraction.\n\n\n\n7.1.4 Stage 3: Bibliographic Management and Citation Rendering\nFollowing the acquisition and deduplication of article metadata and the retrieval of corresponding full-text PDFs, the pipeline proceeds to a crucial data management and enrichment stage. The objective here is twofold: first, to create a permanent, centralized, and queryable library of all processed literature using the Zotero reference management system; and second, to pre-generate authoritative citation metadata for each article. This pre-rendered citation data serves as a verifiable “ground truth” for the downstream Retrieval-Augmented Generation (RAG) agent, ensuring the academic rigor and traceability of the final synthesized output. This entire process is orchestrated by the scripts/fast_zotero_gui.py module.\nThe integration with Zotero is architected for high-throughput processing. The integrate function within the module receives the DataFrame of articles with valid PDF paths and uses the pyzotero library to communicate with the Zotero API. To maximize efficiency, new items are created in batches of up to 50 articles per API call. Subsequently, the system utilizes a ThreadPoolExecutor to upload the associated PDF files to the newly created Zotero entries in parallel, significantly reducing the time required for this I/O-bound task.\nA key technical innovation in this module is the method of citation generation. Rather than relying on slow, repeated API calls to Zotero for formatting each citation, the system leverages the citeproc-py library to perform this rendering locally. This approach uses standard Citation Style Language (CSL) files (e.g., apa.csl, ieee.csl), which are selectable in the GUI, to instantly format both the full bibliographic citation and the corresponding in-text citation for every article. The module also includes a robust fallback mechanism that can query the Zotero API for the citation if the local citeproc rendering encounters an error for a specific entry.\n\n\nCode\n# In scripts/fast_zotero_gui.py\n\n# ... inside the integrate function loop ...\nfor idx, zkey in attach.items():\n    df.at[idx, \"ZoteroKey\"] = zkey\n    meta = {c: df.at[idx, c] for c in (\n        \"Title\", \"DOI\", \"Authors\", \"Date\",\n        \"Journal\", \"Volume\", \"Issue\", \"Pages\"\n    )}\n    try:\n        # Attempt local citation rendering first\n        in_text, full = _render_citations(meta, csl_path)\n    except Exception as exc:\n        # If local rendering fails, use the fallback\n        logging.error(f\"citeproc rendering failed for row {idx}: {exc}\")\n        in_text, full = fallback(zkey, csl_path) if callable(fallback) else (\"\", \"\")\n    df.at[idx, \"InTextCitation\"] = in_text or \"\"\n    df.at[idx, \"FullCitation\"] = full or \"\"\n\n\nThe impact of this stage on the overall system is critical. The generated ZoteroKey, InTextCitation, and FullCitation generated in this stage are not merely for user reference; they are programmatically injected back into the DataFrame. This enriched data is subsequently passed as part of the context to the downstream Retrieval-Augmented Generation (RAG) agent.\nThis architectural choice is fundamental to ensuring the factuality and academic rigor of the final synthesized output. The RAG agent is explicitly instructed to populate its findings into a structured JSON object that requires these specific metadata fields for each piece of evidence it presents. By providing this pre-generated, validated citation data directly within the context, the system constrains the Large Language Model. This prevents the LLM from attempting to generate its own citations, a task for which they are notoriously unreliable and prone to hallucination.\nInstead of showing the entire internal prompt, the following illustrates the structure of the metadata object that the RAG agent must populate for each observation it makes. The presence of the Zotero-derived fields highlights their essential role in the validation process.\n\n\nCode\n// Example Metadata Schema for a single RAG Observation\n{\n  \"metadata\": {\n    \"PDF_DocIndex\": \"148\",\n    \"PDF_ChunkIndex\": \"0\",\n    \"Struct_DocIndex\": \"N/A\",\n    \"Struct_ChunkIndex\": \"N/A\",\n    \"DOI\": \"10.1016/j.jobe.2021.103722\",\n    \"ZoteroKey\": \"ABC123DE\",\n    \"InTextCitation\": \"(Author et al., 2021)\",\n    \"FullCitation\": \"Author, A., Author, B., & Author, C. (2021). Title of the article. Journal Name, 1(2), 100-110. https://doi.org/10.1016/j.jobe.2021.103722\"\n  }\n}\n\n\nThis architectural choice constrains the Large Language Model, preventing it from generating its own, potentially hallucinated, citations. It ensures that every piece of evidence presented in the final answer is verifiably and accurately linked back to a specific source document managed within the Zotero library, creating a fully traceable and academically sound output.\n\n\n\n7.1.5 Stage 4: Content Extraction via Document Layout Analysis\nObjective: High-Fidelity Extraction from Scholarly PDFs\nA foundational requirement for the ETL pipeline is the ability to accurately and efficiently extract content from thousands of scholarly articles in PDF format. This task is notoriously challenging due to the format’s complex, variable layouts and the need to preserve structural context. The objective for this stage was to move beyond simple text extraction and reliably parse not only the full textual body of each article but also complex embedded elements crucial for scientific analysis, such as tabular data and mathematical formulas formatted in LaTeX.\n\n7.1.5.1 Core Technology and Identified Limitations\nTo meet this objective, the system leverages IBM’s Docling library as its core document analysis engine, a powerful framework that uses machine learning for sophisticated page layout detection. However, initial implementation and testing revealed several significant limitations with the out-of-the-box library that necessitated a substantial re-engineering effort:\n\nPerformance Bottleneck: The framework lacked native support for efficient parallelization, making the processing of thousands of documents a prohibitively time-consuming, single-threaded operation.\nModel Inefficiency: TThe original formula recognition model, based on the SamOPTForCausalLM architecture, was found to be excessively resource-intensive, requiring approximately 30 GB of VRAM per instance, constraining the ability to parallelize effectively.\nLayout Inaccuracies: The layout detection model frequently produced suboptimal bounding boxes. This manifested in two primary ways:\n\nFormula Fragmentation: Single, multi-line mathematical formulas were often incorrectly segmented into several independent clusters.\nLayout Misclassification: Pages containing structured elements like line numbers were often misclassified in their entirety as a single, large TABLE cluster, losing all other semantic information.\n\nStability Issues: Persistent glyph-parsing errors within the core library caused instability during large-scale processing runs.\n\nTo address these significant challenges, a substantial re-engineering of the Docling library was undertaken. The resulting modified codebase, along with testing scripts, sample data, and a validation notebook, has been made publicly available in a dedicated GitHub repository to ensure transparency and allow for independent validation of the enhancements.\nGithub Repository\n\n\n7.1.5.2 Custom Enhancements and Implementation\nTo overcome these challenges, the system was transformed into a high-performance, accurate, and efficient processing system through a series of targeted enhancements, primarily orchestrated by scripts/docling_multi_mp_gui.py.\nThe performance bottleneck was addressed first by designing a custom architecture around Python’s concurrent.futures.ProcessPoolExecutor. A key innovation was the implementation of a custom worker_initializer function that facilitates true parallel processing across multiple GPUs. This initializer uses a locking mechanism to assign each new worker process to a specific GPU from the available hardware pool, effectively converting Docling into a distributed application and dramatically reducing processing time.\nThe accuracy and inefficiency issues of the formula recognition model were solved by replacing it entirely. The core code_formula_predictor.py was reconfigured to integrate a more efficient and accurate model, “SmolDocling,” which reduced VRAM requirements to ~8GB.\nFinally, the layout inaccuracies and stability issues were addressed through direct modifications to the Docling codebase. This included engineering a sophisticated post-processing step in docling/utils/layout_postprocessor.py with rule-based heuristics to merge fragmented formulas and re-classify entire misidentified pages. The persistent glyph errors required low-level debugging by reverse-engineering the compiled C-extension library (pdf_parsers.cpython-312-x86_64-linux-gnu.so) using the Ghidra software reverse engineering suite to diagnose and patch the root cause.\nThese extensive modifications are reflected in the numerous library files that were changed during development:\n\nPrimary Docling Library Modifications:\n\ndocling/backend/docling_parse_v4_backend.py\ndocling/datamodel/base_models.py\ndocling/datamodel/pipeline_options.py\ndocling/models/code_formula_model.py\ndocling/models/layout_model.py\ndocling/pipeline/vlm_pipeline.py\ndocling/utils/layout_postprocessor.py\n\nIBM Model Predictor Modifications:\n\ndocling_ibm_models/code_formula_model/code_formula_predictor.py\ndocling_ibm_models/layoutmodel/layout_predictor.py\n\n\n\n\n7.1.5.3 Initial Challenge: Inaccurate Bounding Boxes and Model Inefficiency\nThe first major issue identified was the precision of the layout analysis model. The bounding boxes generated for mathematical formulas were often too tight or included extraneous visual artifacts from surrounding text. When these imperfect image snippets were passed to the original formula recognition model (ds4sd/CodeFormula), it frequently failed, resulting in generative feedback loops that produced repetitive, malformed LaTeX output (as exemplified in Figure 7.2, showing a poorly cropped snippet).\n\n\n\n\n\n\nFigure 7.2: Bad Formula\n\n\n\nFurthermore, the original model, based on the SamOPTForCausalLM architecture, was found to be excessively resource-intensive, requiring approximately 30 GB of VRAM per instance. This made large-scale parallelization across multiple GPUs prohibitively expensive and inefficient.\n\n\n7.1.5.4 Bounding Box and Image Preprocessing Modifications\nThe initial attempt to resolve the formula recognition issue focused on improving the quality of the image snippets before they were sent to the model. This involved significant modifications to docling/models/code_formula_model.py.\n\nDynamic Bounding Box Expansion: Logic was introduced in the prepare_element method to dynamically expand the bounding boxes of detected formulas. Instead of using the original, often tightly-cropped box, the new implementation expands the box by a configurable ratio of the snippet’s height and width. This ensures more visual context is captured as demonstrated in Figure 7.3.\n\n\n\n\n\n\n\nFigure 7.3: Good Formula\n\n\n\n\nRatio-Based Image Padding: The original *_pad_with_most_frequent_edge_color* function, which used static pixel padding, was entirely replaced. The new version accepts floating-point ratios, allowing it to apply padding that is proportional to the image’s dimensions. This creates a more consistent and contextually appropriate input for the model, regardless of the formula’s original size.\nConditional Image Masking: To further isolate formulas from surrounding text, conditional logic was added to use Docling’s page.get_masked_image() method exclusively for items labeled as DocItemLabel.FORMULA.\n\n\n\n7.1.5.5 Implementation of Selective Content Masking\nThe most critical enhancement was the development of a method to isolate formula elements from distracting adjacent text. When the layout model produced imperfect bounding boxes, snippets would often contain partial lines of text from the main body, confusing the formula recognition model. To solve this, a selective masking capability was engineered directly into the Page class within docling/datamodel/base_models.py.\nThis was achieved by introducing two new methods, get_masked_image and the internal *_create_masked_image. The _create_masked_image* function operates by first taking a complete, high-resolution image of a document page. It then iterates through all layout clusters previously identified by the layout model. For every cluster that is not labeled as a DocItemLabel.FORMULA, the function programmatically draws a white rectangle over its bounding box.\n\n\nCode\ndef _create_masked_image(self, scale: float, pdf_identifier: Optional[str] = None) -&gt; Optional[Image]:\n    try:\n        original_image = self.get_image(scale=scale)\n        if original_image is None or self.size is None:\n            _log.warning(\"Original image or size is None for page %s.\", self.page_no)\n            return None\n        masked = original_image.copy()\n        draw = ImageDraw.Draw(masked)\n        scale_x = masked.width / self.size.width\n        scale_y = masked.height / self.size.height\n        top_expansion_factor = 0.045\n        bottom_expansion_factor = 0.045\n        cluster_count = len(self.predictions.layout.clusters)\n        _log.debug(\"Creating masked image for page %s with %d clusters.\", self.page_no, cluster_count)\n        # Mask non-formula clusters explicitly here with correct scaling\n        for cl in self.predictions.layout.clusters:\n            if cl.label != DocItemLabel.FORMULA:\n                bbox = cl.bbox.to_top_left_origin(page_height=self.size.height)\n                bbox_height = bbox.b - bbox.t\n                expanded_bbox = BoundingBox(\n                    l=bbox.l,\n                    t=bbox.t - bbox_height * top_expansion_factor,\n                    r=bbox.r,\n                    b=bbox.b + bbox_height * bottom_expansion_factor,\n                    coord_origin=bbox.coord_origin,\n                )               \n                scaled_bbox = BoundingBox(\n                    l=expanded_bbox.l * scale_x,\n                    t=expanded_bbox.t * scale_y,\n                    r=expanded_bbox.r * scale_x,\n                    b=expanded_bbox.b * scale_y,\n                    coord_origin=expanded_bbox.coord_origin\n                )\n                draw.rectangle(scaled_bbox.as_tuple(), fill=\"white\")\n        return masked\n\n\nThe result is a new, “masked” image of the page where all non-formula content has been completely obscured, as demonstrated in the comparison between an original page layout (Figure 7.4) and its masked counterpart (Figure 7.5).\n\n\n\n\n\n\nFigure 7.4: Unmasked Page\n\n\n\n\n\n\n\n\n\nFigure 7.5: Masked Page\n\n\n\nWhen the downstream prepare_element method in code_formula_model.py is called, it now uses this get_masked_image function instead of the standard get_image. By cropping the formula’s bounding box from this pre-masked page, the system guarantees that the final image snippet sent to the SmolDocling model contains only the formula itself, significantly improving the signal-to-noise ratio and preventing recognition errors caused by textual artifacts.\n\n\n7.1.5.6 Heuristic-Based Merging of Fragmented Layout Clusters\nBeyond improving the quality of individual image snippets, a second significant challenge was identified in the output of the base layout analysis model: the fragmentation of single, cohesive document elements into multiple, distinct bounding boxes. This issue was particularly prevalent with complex, multi-line mathematical formulas, which were often incorrectly segmented into several independent FORMULA clusters, as exemplified in Figure 7.6. This fragmentation would lead to the downstream formula recognition model receiving only partial equations, resulting in incomplete and unusable LaTeX output.\n\n\n\n\n\n\nFigure 7.6: Fragmented Formulas\n\n\n\nTo resolve this, a sophisticated post-processing step was engineered within docling/utils/layout_postprocessor.py. The purpose of this step is to apply a series of rule-based heuristics to refine and correct the raw output of the initial layout model before the final document structure is assembled.\n\n\nCode\ndef _merge_vertically_adjacent_formulas(\n    self,\n    clusters,\n    vertical_threshold_factor=1.8,\n    horizontal_overlap_threshold=0.7,\n    padding=50,\n    alignment_threshold=20,  # pixel-based threshold\n    max_alignment_ratio=0.2,  # ratio-based threshold (e.g., 0.2 means 20% mismatch)\n):\n    \"\"\"\n    merge vertically adjacent FORMULA clusters using Union-Find.\n    New logic:\n    1) Compute a dynamic vertical threshold based on median formula height.\n    2) Expand bounding boxes horizontally using padding to compute the horizontal overlap.\n    3) Compute both absolute (pixel) differences and ratio-based alignment.\n    4) Extract formula numbers from cell texts. If both clusters have numbers and they differ, skip merge.\n    5) If neither cluster has a formula number, require that the vertical gap is very small (&lt;= 10 px)\n        and that the horizontal overlap is nearly complete (&gt;= 0.9).\n    6) Otherwise, use the normal criteria based on vertical gap (and derived thresholds).\n    \"\"\"\n    def compute_alignment_factor(c1, c2):\n        # Compute the average width and return the maximum normalized edge difference.\n        w1 = c1.bbox.r - c1.bbox.l\n        w2 = c2.bbox.r - c2.bbox.l\n        avg_width = max((w1 + w2) / 2.0, 1e-6)\n        left_diff = abs(c1.bbox.l - c2.bbox.l)\n        right_diff = abs(c1.bbox.r - c2.bbox.r)\n        return max(left_diff / avg_width, right_diff / avg_width)\n    # Only process clusters labelled as FORMULA.\n    formula_clusters = [c for c in clusters if c.label == DocItemLabel.FORMULA]\n    non_formula_clusters = [c for c in clusters if c.label != DocItemLabel.FORMULA]\n    if not formula_clusters:\n        return clusters\n    # Sort by the top coordinate.\n    formula_clusters.sort(key=lambda c: c.bbox.t)\n    heights = [c.bbox.b - c.bbox.t for c in formula_clusters]\n    median_height = np.median(heights) if heights else 0\n    vertical_threshold = median_height * vertical_threshold_factor\n    # Build a mapping from cluster id to cluster instance.\n    id_to_cluster = {c.id: c for c in formula_clusters}\n    uf = UnionFind(list(id_to_cluster.keys()))\n    n = len(formula_clusters)\n    for i in range(n):\n        for j in range(i + 1, n):\n            c1 = formula_clusters[i]\n            c2 = formula_clusters[j]\n            # Compute vertical gap (c2 is assumed to be below c1)\n            vertical_gap = c2.bbox.t - c1.bbox.b\n            if vertical_gap &lt; 0 or vertical_gap &gt; vertical_threshold:\n                continue\n            # Expand bounding boxes horizontally using the given padding.\n            expanded_bbox_c1 = type(c1.bbox)(\n                l=c1.bbox.l - padding,\n                t=c1.bbox.t,\n                r=c1.bbox.r + padding,\n                b=c1.bbox.b\n            )\n            expanded_bbox_c2 = type(c2.bbox)(\n                l=c2.bbox.l - padding,\n                t=c2.bbox.t,\n                r=c2.bbox.r + padding,\n                b=c2.bbox.b\n            )\n            horizontal_overlap = min(expanded_bbox_c1.r, expanded_bbox_c2.r) - max(expanded_bbox_c1.l, expanded_bbox_c2.l)\n            min_width = min(expanded_bbox_c1.r - expanded_bbox_c1.l, expanded_bbox_c2.r - expanded_bbox_c2.l)\n            overlap_ratio = horizontal_overlap / min_width if min_width &gt; 0 else 0\n            left_diff = abs(c1.bbox.l - c2.bbox.l)\n            right_diff = abs(c1.bbox.r - c2.bbox.r)\n            alignment_factor = compute_alignment_factor(c1, c2)\n            # Extract formula numbers using _extract_formula_number.\n            num1 = self._extract_formula_number(c1)\n            num2 = self._extract_formula_number(c2)\n            # Branch 4) If both have numbers and differ, skip\n            if num1 and num2 and num1 != num2:\n                continue\n            # Normal merging criteria based on vertical gap and alignment.\n            # Pre-calculate default geometry-based threshold\n            if vertical_gap &lt;= (0.5 * vertical_threshold):\n                if left_diff &lt;= alignment_threshold and right_diff &lt;= alignment_threshold:\n                    required_overlap = horizontal_overlap_threshold\n                else:\n                    required_overlap = 0.85\n            elif vertical_gap &lt;= vertical_threshold:\n                if alignment_factor &lt;= max_alignment_ratio:\n                    required_overlap = 0.85\n                else:\n                    continue\n            else:\n                continue\n            # New branch: if no formula number is detected in either cluster,\n            # then enforce a very small vertical gap and nearly complete horizontal overlap.\n            # Branch 2) Both None =&gt; require vertical_gap ≤10 and overlap ≥0.9\n            if num1 is None and num2 is None:\n                if vertical_gap &gt; 3:  # Change this constant if needed\n                    continue\n                # Require a higher overlap; allow slight relaxation if the normal criterion is higher.\n                required_overlap = max(required_overlap, 0.9)\n            # Branch 3) One number missing =&gt; require vertical_gap ≤5 and overlap ≥0.95\n            elif (num1 is None) != (num2 is None):\n                # i.e. exactly one side is missing a number\n                if vertical_gap &gt; 12.8:\n                    continue\n                required_overlap = max(required_overlap, 0.95)\n            # Branch 1) If both exist and match =&gt; use normal geometry (already in required_overlap)\n            # Final merge check\n            if overlap_ratio &gt;= required_overlap:\n                uf.union(c1.id, c2.id)\n    groups = uf.get_groups()\n    merged_formula_clusters = []\n    for group_ids in groups.values():\n        group_clusters = [id_to_cluster[g] for g in group_ids]\n        merged_bbox = type(group_clusters[0].bbox)(\n            l=min(c.bbox.l for c in group_clusters),\n            t=min(c.bbox.t for c in group_clusters),\n            r=max(c.bbox.r for c in group_clusters),\n            b=max(c.bbox.b for c in group_clusters)\n        )\n        combined_cells = []\n        for c in group_clusters:\n            combined_cells.extend(c.cells)\n        merged_cluster = group_clusters[0]\n        merged_cluster.bbox = merged_bbox\n        merged_cluster.cells = self._sort_cells(self._deduplicate_cells(combined_cells))\n        merged_formula_clusters.append(merged_cluster)\n    return non_formula_clusters + merged_formula_clusters\n\n\nThe core of this enhancement is the new *_merge_vertically_adjacent_formulas* method. This algorithm intelligently identifies and merges clusters that likely belong to the same formula. It operates on a set of heuristics designed to distinguish between separate, adjacent equations and multiple lines of a single equation:\n\nDynamic Proximity Threshold: Rather than using a fixed pixel distance, the method first calculates the median height of all detected formulas on a page. It then defines a dynamic vertical proximity threshold based on a factor of this median height, allowing it to adapt to documents with different font sizes and line spacing.\nFormula Number Extraction: A helper function, _extract_formula_number, uses regular expressions to find equation numbers (e.g., (1), (2a), (A5)) within the text of each formula cluster.\nContextual Merging Logic: The algorithm iterates through pairs of vertically adjacent formula clusters and applies a decision-making process:\n\nIf both clusters have distinct formula numbers (e.g., (1) and (2)), they are correctly identified as separate equations and are not merged.\n\nIf neither cluster has a formula number, they are only merged if the vertical gap between them is minimal and their horizontal overlap is nearly complete, which is characteristic of multi-line equations without a single encompassing number.\n\nIf one cluster has a number and the adjacent one does not, a stricter set of proximity and overlap rules is applied.\n\n\nOnce candidate clusters for merging are identified, a Union-Find data structure is used to efficiently group them. The final step involves creating a new, single bounding box that encompasses all the merged clusters and combining their constituent text cells. This new, unified cluster is then passed to the next stage of the pipeline, ensuring the entire formula is processed as a single unit as demonstrated in Figure 7.7. Additional heuristics, such as the *_filter_tables_containing_page_footer* method, were also added to this module to correct other common layout analysis errors, further improving the overall quality of the parsed document structure.\n\n\n\n\n\n\nFigure 7.7: Unfragmented Formulas\n\n\n\n\n\n7.1.5.7 Misclassification of Page Layouts\nThis analysis details further enhancements made to the layout post-processing logic to address specific, recurring classification errors observed in certain document types, such as pre-prints or articles with line numbers.\nA common failure mode was observed where the layout detection model would misclassify an entire page or large sections of a page as a single, large TABLE cluster. This issue was particularly prevalent in documents that featured line numbers running down the left-hand margin, as seen in Figure 7.8. The presence of this repeating, structured numerical column appeared to mislead the model into perceiving the entire text block as tabular data. This resulted in the loss of all other semantic distinctions for that page (e.g., paragraphs, headers, lists), rendering the content unusable for downstream extraction.\n\n\n\n\n\n\nFigure 7.8: Table Missclassification\n\n\n\n\n\n7.1.5.8 Heuristic-Based Re-classification of Misidentified Clusters\nTo correct this, a new heuristic-based filtering method, *_filter_tables_containing_page_footer*, was engineered and integrated into the main postprocess pipeline. While the method name suggests a focus on page footers, its logic was designed more broadly to identify and re-classify any cluster that was erroneously labeled as a table-like structure when it was, in fact, the main body text of the page.\n\n\nCode\n# In scripts/docling/utils/layout_postprocessor.py\n\ndef _filter_tables_containing_page_footer(self, clusters: List[Cluster], \n                                          min_area_ratio: float = 0.70, \n                                          min_cells_threshold: int = 50,\n                                          min_density_threshold: float = 0.001) -&gt; List[Cluster]:\n    \"\"\"\n    Enhanced logic to avoid reclassifying legitimate large tables.\n    \"\"\"\n    large_labels = {DocItemLabel.TABLE, DocItemLabel.DOCUMENT_INDEX, \n                    DocItemLabel.KEY_VALUE_REGION, DocItemLabel.FORM}\n    page_area = self.page_size.width * self.page_size.height\n    reclassified_count = 0\n\n    for cluster in clusters:\n        if cluster.label in large_labels:\n            cluster_area_ratio = cluster.bbox.area() / page_area\n            if cluster_area_ratio &gt;= min_area_ratio:\n                all_cells = self._collect_all_cells(cluster)\n                cell_density = len(all_cells) / cluster.bbox.area()\n\n                # Reclassify if the cluster is large but sparsely populated with text cells\n                if len(all_cells) &lt; min_cells_threshold or cell_density &lt; min_density_threshold:\n                    cluster.label = DocItemLabel.TEXT\n                    reclassified_count += 1\n    return clusters\n\n\nThe algorithm operates on a set of carefully tuned heuristics to distinguish between a legitimate, large table and a misclassified text block:\n\nTarget Selection: The function first identifies any cluster labeled as a TABLE (or other large “wrapper” types like FORM) for inspection.\nArea Threshold: It only considers clusters that are exceptionally large, occupying a significant portion of the page (e.g., min_area_ratio of 70%). This prevents the algorithm from affecting smaller, legitimate tables.\nCell Count & Density Check: This is the core heuristic. It recursively collects all the individual text cells within the large cluster and calculates two metrics: the absolute number of cells and the “cell density” (number of cells per unit of bounding box area).\nRe-classification Logic: If a cluster is very large but contains a low number of text cells or has a very low cell density, it is highly unlikely to be a real table. In such cases, the algorithm re-classifies the cluster’s label from TABLE to TEXT.\n\nBy adding this function to the postprocess pipeline, the system can now correctly identify and relabel these large, sparse, misclassified text blocks, preserving the semantic integrity of the page structure as demonstrated by the corrected output in Figure 7.9.\n\n\n\n\n\n\nFigure 7.9: Table Correction\n\n\n\n\n\n7.1.5.9 Pre-emptive Filtering of Page-Level Artifacts\nIn addition to the post-processing of layout clusters, further enhancements were engineered upstream in the docling/backend/docling_parse_v4_backend.py module to improve the initial quality of the data fed to the layout analysis model. This module serves as a low-level backend that interacts directly with the docling-parse C++ library, which is responsible for the initial extraction of raw text cells and their coordinates from the PDF document. It was observed that certain document formats, particularly pre-prints, often contain page-level artifacts like line numbers in the margins. These structured, non-substantive elements were found to frequently mislead the layout model, leading to catastrophic classification errors where an entire page of text would be misidentified as a single, large TABLE cluster.\nTo mitigate this failure mode, the primary modification was made to the get_text_cells method within the DoclingParseV4PageBackend class. The original implementation of this method simply transformed the coordinate system of all extracted text cells and returned the complete, unfiltered list. The enhanced version introduces a new helper method, *_is_left_margin_line_number*, which applies a series of geometric heuristics to identify and pre-emptively filter these line-number artifacts before they are passed to the layout model.\nThe heuristic function determines if a given text cell is a line number by evaluating three specific spatial properties. First, it confirms the cell is located within a narrow vertical band on the far-left of the page, defined by a LEFT_MARGIN_THRESHOLD (e.g., the leftmost 8% of the page width). Second, it verifies that the cell’s bounding box width is less than this MAX_WIDTH_THRESHOLD, characteristic of short numerical strings. Finally, it ensures the cell has a reasonable height via MIN_HEIGHT_THRESHOLD to avoid incorrectly filtering other small page markings like footnote symbols. A cell is only flagged and removed if it satisfies all three conditions. By filtering the cell list with this function, the layout model receives a much cleaner representation of the page’s semantic content, significantly reducing classification errors and preserving the integrity of the document structure for all downstream processing.\nOriginal Implementation (get_text_cells)\n\n\nCode\ndef get_text_cells(self) -&gt; Iterable[TextCell]:\n    page_size = self.get_size()\n\n    # Applies coordinate transformation but returns all cells\n    [tc.to_top_left_origin(page_size.height) for tc in self._dpage.textline_cells]\n\n    return self._dpage.textline_cells\n\n\nModified Implementation with Heuristic Filtering\n\n\nCode\ndef get_text_cells(self) -&gt; Iterable[TextCell]:\n    page_size = self.get_size()\n    for tc in self._dpage.textline_cells:\n        tc.to_top_left_origin(page_size.height)\n\n    # Filter out cells identified as line numbers by the heuristic\n    filtered_cells = [\n        cell for cell in self._dpage.textline_cells\n        if not self._is_left_margin_line_number(cell, page_size)\n    ]\n    return filtered_cells\n\ndef _is_left_margin_line_number(self, cell: TextCell, page_size: Size) -&gt; bool:\n    \"\"\"\n    Identifies if a cell is a line number based on geometric properties.\n    \"\"\"\n    LEFT_MARGIN_THRESHOLD = 0.08\n    MIN_HEIGHT_THRESHOLD = 5\n    MAX_WIDTH_THRESHOLD = page_size.width * LEFT_MARGIN_THRESHOLD\n\n    bbox = cell.rect.to_bounding_box()\n\n    is_within_left_margin = bbox.l &lt; MAX_WIDTH_THRESHOLD\n    is_small_horizontal = bbox.width &lt; MAX_WIDTH_THRESHOLD\n    is_not_too_short = bbox.height &gt;= MIN_HEIGHT_THRESHOLD # Corrected logic\n\n    return is_within_left_margin and is_small_horizontal and is_not_too_short\n\n\n\n\n7.1.5.10 Refinements to the Underlying Layout Predictor\nComplementing the logical post-processing of layout clusters, targeted modifications were also made to the underlying docling_ibm_models/layoutmodel/layout_predictor.py module. This module is responsible for the initial, low-level detection of layout elements on a page image by executing the RTDetrForObjectDetection model. While the primary logic of the model itself was not altered, key refinements were made to its data handling and execution to ensure high-performance and stable operation within the custom parallelized framework.\nThe most critical modification addresses a performance bottleneck related to hardware utilization. In the predict method, the target_sizes tensor, which is required by the post_process_object_detection function to correctly rescale bounding boxes to the original image dimensions was originally created on the CPU by default.\n\n\nCode\n@torch.inference_mode()\ndef predict(self, orig_img: Union[Image.Image, np.ndarray]) -&gt; Iterable[dict]:\n    results = self._image_processor.post_process_object_detection(\n        outputs,\n        target_sizes=torch.tensor([page_img.size[::-1]]), # Tensor created on CPU by default\n        threshold=self._threshold,\n    )\n    w, h = page_img.size\n    result = results[0]\n    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n# ...\n        # Manual clamping of coordinates\n        l = min(w, max(0, bbox_float[0]))\n        t = min(h, max(0, bbox_float[1]))\n        r = min(w, max(0, bbox_float[2]))\n        b = min(h, max(0, bbox_float[3]))\n        yield { # ...\n        }\n\n\nWhen the model was running on a GPU, this mismatch forced an expensive and unnecessary cross-device data transfer during post-processing. The implementation was corrected to explicitly create this tensor on the same device as the model (device=self._device), eliminating the synchronization penalty and ensuring more efficient CUDA utilization.\n\n\nCode\n# ...\n@torch.inference_mode()\ndef predict(self, orig_img: Union[Image.Image, np.ndarray]) -&gt; Iterable[dict]:\n# ...\n    # Explicitly create the tensor on the same device as the model (e.g., CUDA)\n    target_sizes = torch.tensor([[original_height, original_width]], device=self._device)\n    results = self._image_processor.post_process_object_detection(\n        outputs, target_sizes=target_sizes, threshold=self._threshold\n    )\n    result = results[0]\n    # Directly use the results after converting to NumPy arrays\n    boxes = result[\"boxes\"].cpu().numpy()\n    scores = result[\"scores\"].cpu().numpy()\n    labels = result[\"labels\"].cpu().numpy()\n    for score, label_id, box in zip(scores, labels, boxes):\n# ...\n        # No more manual clamping\n        l, t, r, b = box\n        yield { # ...\n        }\n\n\nAdditionally, the code was modernized by removing redundant manual logic for clamping bounding box coordinates. The original implementation manually ensured that the box coordinates did not exceed the page dimensions after being returned by the post-processing function. The refined version removes this step, properly delegating the responsibility for coordinate clamping to the transformers library’s post_process_object_detection function. This change makes the code cleaner, more maintainable, and adheres more closely to the intended use of the library’s API. While subtle, these enhancements to the core predictor were essential for ensuring the stability and high throughput required by the system’s parallel architecture.\n\n\n7.1.5.11 Final Implementation in the ETL Pipeline\nThe re-engineered Docling system is integrated as a distinct stage in the ETL pipeline, orchestrated by the do_docling_extraction function.\n\n\nCode\n# In scripts/docling_multi_mp_gui.py\n\ndef do_docling_extraction(df: pd.DataFrame, progress_callback=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes DataFrame rows in parallel using config settings.\n    \"\"\"\n    global MAX_WORKERS_DOCLING \n    num_records = len(df)\n    # Use the worker count from config, ensuring it's at least 1\n    max_workers_to_use = max(1, MAX_WORKERS_DOCLING)\n    \n    output_cols = [\"FullText\", \"TablesJson\", \"EquationsJson\", \"TokenCount\", \"Error\"]\n    for col in output_cols:\n        if col not in df.columns:\n            if col == \"TokenCount\": df[col] = 0\n            elif col == \"Error\": df[col] = None\n            else: df[col] = \"\" # Default to empty string for text/JSON string columns\n    futures = {}\n    processed_count = 0\n    with ProcessPoolExecutor(max_workers=max_workers_to_use, initializer=worker_initializer) as executor:\n        for idx, row in df.iterrows():\n            pdf_path = row.get(\"PDFPath\", \"\")\n            if not pdf_path or not os.path.exists(pdf_path):\n                df.loc[idx, output_cols] = [\"PDF_PATH_ERROR\", \"[]\", \"[]\", 0, \"PDF_PATH_ERROR\"]\n                processed_count += 1\n                continue\n            future = executor.submit(extract_pdf_with_docling, pdf_path)\n            futures[future] = idx # Map future to DataFrame index\n\n        # Process results as they complete\n        for future in as_completed(futures):\n            irow = futures[future] # Get the original DataFrame index\n            try:\n                result = future.result() # Get the dict returned by worker\n                # Update DataFrame using .loc with index 'irow'\n                for col in output_cols:\n                    df.loc[irow, col] = result.get(col)\n            except Exception as e:\n                df.loc[irow, output_cols] = [\"FUTURE_ERROR\", \"[]\", \"[]\", 0, f\"FutureError: {type(e).__name__}\"]\n    return df\n\n\nThis function receives a pandas DataFrame containing paths to the downloaded PDF articles. It distributes the processing of each PDF to the pool of GPU-powered workers. Each worker executes the extract_pdf_with_docling function, which initializes a dedicated DocumentConverter instance configured for its assigned GPU. The worker processes its assigned PDF, extracts the full text (exported as Markdown to preserve structure), and specifically identifies and serializes all tabular data and mathematical formulas into JSON arrays. Upon completion, the structured outputs, FullText, TablesJson, EquationsJson, along with a TokenCount are returned to the main process and integrated back into the corresponding row of the DataFrame. The resulting enriched DataFrame is then saved as a new artifact (output.feather), ready for the subsequent LLM-based field extraction stage.\n\n\n\n\n7.1.6 Stage 5: Structured Field Extraction using Large Language Models\nFollowing the extraction of raw text via document layout analysis, the pipeline proceeds to the structured field extraction stage, orchestrated by the scripts/field_extraction.py module. The objective of this component is to parse the unstructured text from each article and populate a predefined set of structured data fields. This process transforms dense, narrative content into a queryable, machine-readable format suitable for populating the knowledge graph and facilitating systematic analysis.\n\n7.1.6.1 Methodology: Iterative, Context-Aware Extraction with Local LLMs\nThe system employs a sophisticated extraction strategy centered around a locally deployed Large Language Model (Qwen/Qwen3-32B). This self-hosted approach, leveraging the project’s multi-GPU hardware, ensures data privacy, eliminates reliance on external API costs and latency, and provides greater control over the inference process. The extraction is not a single pass; rather, it is an iterative process designed to build a rich, cumulative record for each document.\nThe core of the methodology is the *_process_one_row* function, which operates on each article’s full text. To manage the extensive length of academic papers and stay within the LLM’s context window, the chunk_text_for_extraction utility first splits the full text into overlapping chunks of a configured token size (e.g., 8000 tokens with a 500-token overlap). The system then iterates through these chunks, performing an LLM call for each one.\nA key innovation is the use of a dynamic, context-aware prompt. For each chunk, the LLM is provided not only with the text of that chunk but also with the current state of the metadata that has been extracted from all previous chunks of the same document. This allows the model to incrementally enrich the data, fill in missing fields, and use previously extracted information as context for interpreting the current text chunk. The prompt dynamically generates a list of fields to be extracted based on the SELECTED_COLS defined in the etl_config.json file, which are interactively selected via the GUI. Crucially, rather than just providing the field name (e.g., “LCA System Boundaries”), the prompt includes a detailed explanation from the FIELD_EXPLANATIONS dictionary, guiding the LLM with a precise definition of the information to look for (e.g., “Describe the scope and boundaries of any Life Cycle Assessment (LCA) mentioned, such as cradle-to-grave, cradle-to-gate…”).\n\n\nCode\n# In scripts/field_extraction.py, illustrating dynamic prompt generation\n\n# ... inside _process_one_row ...\n\n# The current state of extracted data is passed as context\nmetadata_context_json = json.dumps(final_merged_data, indent=2)\n\n# The prompt is formatted with the context, the new text chunk,\n# and the list of fields with their detailed explanations.\nprompt = extraction_template.format(\n    metadata_json=metadata_context_json,\n    chunk_text=chunk_txt,\n    field_list_placeholder=field_list_str\n)\n\n# ... LLM call is made with this enhanced prompt ...\n\n\n\n\n7.1.6.2 Parallelized Execution and Data Merging\nTo handle the processing of hundreds of articles efficiently, the entire operation is parallelized. The extract_additional_fields function uses a ThreadPoolExecutor to process multiple articles (rows in the DataFrame) concurrently. Within each of these threads, a nested ThreadPoolExecutor is used to process the individual text chunks for that article in parallel. This multi-level parallelism ensures maximum utilization of the available hardware.\nThe output from each LLM call is a JSON object containing only the fields found within that specific text chunk. A robust merging function, unify_fields, is then used to intelligently integrate this new data into the master record for the article. This function handles various data types, uniquely appending items to list-based fields (e.g., Pollutant Terms), overwriting simple text fields with more specific information, and carefully aggregating complex structured data like the Metrics field. To prevent performance degradation from repeated LLM calls on identical text, a cache (llm_response_cache) stores the output for each unique prompt, returning the cached result if the same text chunk is ever processed again. This entire process, from chunking to parallelized inference and intelligent merging, results in a richly detailed and structured dataset, which is then saved as an enriched Feather file, ready for the final stages of the ETL pipeline.\n\n\n\n\n7.1.7 Stage 6: Thematic Analysis via Topic Modeling\nFollowing the extraction of structured data, the pipeline performs a thematic analysis of the textual corpus to identify latent topics, which serves as a primary method for systematic research gap analysis. This is achieved through the implementation of a configurable and parallelized Latent Dirichlet Allocation (LDA) topic modeling workflow, orchestrated by the scripts/topic_modeling_gui.py module. The objective is to distill the vast, unstructured text from hundreds of articles into a set of coherent, interpretable topics whose prevalence and relationships can be quantitatively assessed.\n\n7.1.7.1 Methodology: Probabilistic Topic Modeling with LDA\nThe core of this stage is Latent Dirichlet Allocation (LDA), a generative probabilistic model for discrete data. The foundational assumption of LDA is that each document in a corpus is a mixture of various topics, and each topic is a distribution of words. The model does not know what the topics are in advance; it learns them by analyzing the patterns of word co-occurrence across the entire set of documents.\nMathematically, LDA models a document as being generated by the following process:\n\nFor each document \\((d)\\) in the corpus \\((D)\\), choose a distribution over topics \\(( \\theta_d \\sim \\text{Dir}(\\alpha))\\).\nFor each word \\((w_n)\\) in document \\((d)\\):\n\nChoose a topic \\(( z_n \\sim \\text{Categorical}(\\theta_d))\\).\nChoose a word \\(( w_n )\\) from \\((p(w_n \\mid z_n, \\beta))\\), the probability of word \\((w_n)\\) given topic \\((z_n)\\).\n\n\nThe goal of the model training is to infer the hidden variables: the topic distributions per document \\((\\theta_d)\\) and the word distributions per topic \\((\\beta_k)\\). The system uses scikit-learn’s implementation of LDA, which employs a variational Bayes algorithm to approximate these posterior distributions. The output for each topic \\((k)\\) is a probabilistic representation best described as a list of words that are most likely to belong to that topic.\n\n\n7.1.7.2 Data Preprocessing and Hyperparameter Optimization\nThe quality of an LDA model is highly dependent on both the cleanliness of the input text and the choice of model hyperparameters. The pipeline therefore begins with a rigorous, parallelized text preprocessing workflow (preprocess_pipeline function) that tokenizes, removes stopwords, and lemmatizes the text from each document.\nTo select the optimal hyperparameters for the corpus, the system implements a parallelized grid search, orchestrated by the _search_best_lda_params function. The user can define the search space in the etl_config.json file interactively via the GUI, specifying ranges for key parameters:\n\nnum_topics (k): The number of latent topics to discover. The system intelligently searches over a dynamic range to find the optimal granularity.\npasses / iterations: The number of passes the algorithm makes over the corpus during training.\nn-gram Parameters (bigram_threshold, trigram_threshold): These control the formation of common multi-word phrases (e.g., “cover crop,” “random forest model”), treating them as single tokens to generate more coherent topics.\nDictionary Filtering (no_below, no_above): These parameters prune the vocabulary by removing terms that are either too rare (no_below) or too common (no_above) to be thematically useful.\n\nThe grid search trains multiple LDA models in parallel using ProcessPoolExecutor, and each resulting model is evaluated using a Topic Coherence score.\n\n\n7.1.7.3 Model Evaluation: Topic Coherence\nTopic coherence measures the degree of semantic similarity between the high-scoring words within a topic, providing a quantitative way to assess how interpretable and “human-like” a topic is. This system uses the \\((C_v)\\) coherence measure, which is based on a sliding window and the normalized pointwise mutual information (NPMI) of word pairs.\nFor a set of top \\((N)\\) words \\((w_1, w_2, \\dots, w_N)\\) in a given topic, the \\((C_v)\\) score is calculated as the average NPMI of all unique word pairs:\n\\[\n\\text{Coherence}(V) = \\frac{1}{\\binom{N}{2}} \\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} \\text{NPMI}(w_i, w_j)\n\\]\nwhere the Normalized Pointwise Mutual Information (NPMI) is given by:\n\\[\n\\text{NPMI}(w_i, w_j) = \\frac{\\log \\frac{P(w_i, w_j)}{P(w_i)P(w_j)}}{-\\log P(w_i, w_j)}\n\\]\nHere, \\(P(w)\\) is the probability of seeing word \\(w\\) in a document, and \\(P(w_i, w_j)\\) is the probability of seeing both words in the same document. A score closer to 1 indicates a more coherent topic. The grid search selects the hyperparameter set that produces the model with the highest average \\(C_v\\) score across all its topics. Additionally, the calculate_topic_coherences function is used to compute the coherence score for each individual topic from the best model, allowing for a granular assessment of the quality of each thematic cluster.\n\n\n7.1.7.4 Interpretation and Visualization\nAfter the best model is identified, the system assigns a dominant topic to each document and uses an LLM (Qwen/Qwen3-32B) to generate a concise, human-readable label for each topic based on its top keywords. Finally, the generate_lda_visualization.py script leverages the pyLDAvis library to create an interactive visualization. This plot as shown, maps the topics into a 2D space, where the size of each topic’s circle represents its prevalence in the corpus and the distance between circles indicates their semantic dissimilarity. This provides a powerful visual tool for identifying prevalent themes and discovering potentially under-represented research areas.\n\n\n\n\n\n\n7.1.8 Stage 7: The Semantic Unification Pipeline\nThis section details the methodology used to resolve terminological ambiguity within the data extracted by the ETL pipeline, ensuring ontological consistency before ingestion into the knowledge graph. This core functionality is encapsulated within the scripts/dictionaries_gui.py module.\n\n7.1.8.1 The Challenge of Terminological Heterogeneity in Automated Extraction\nA primary challenge in the automated processing of scientific literature arises from terminological heterogeneity. The field_extraction.py module, which leverages a Large Language Model (LLM), extracts unstructured textual strings that often exhibit significant variation despite referring to the same underlying concept. For example, within the “Tillage Practices” field, semantically equivalent concepts may be described as “no-till,” “zero tillage,” or “direct drilling.” To construct a coherent and queryable knowledge graph, as orchestrated by kg_pipeline_gui.py, it is imperative that these synonymous variations are resolved into a single, canonical entity. The system addresses this challenge not through fragile string matching or rule-based heuristics, but through a robust process of semantic unification, which mathematically quantifies the contextual meaning of extracted terms. This core functionality is encapsulated within the scripts/dictionaries_gui.py module.\n\n\n7.1.8.2 A Vector Space Model for Semantic Representation\nThe foundational theory underpinning the unification process is the representation of language within a high-dimensional vector space, often referred to as word or phrase embedding. This approach posits that the meaning of a term can be captured by a dense numerical vector, where terms with similar meanings are located closer to each other in this geometric space. This system employs a pre-trained Sentence Transformer model (sentence-transformers/all-MiniLM-L6-v2) to perform this transformation. This specific model generates a 384-dimensional vector for any given textual input, effectively mapping each term to a unique coordinate in a 384-dimensional semantic space.\n\n\n7.1.8.3 Pre-computation of the Canonical Knowledge Base\nThe unification process is implemented through a sequence of operations designed for both accuracy and computational efficiency. At pipeline initialization, the precompute_all function within dictionaries_gui.py is executed.\n\n\nCode\ndef precompute_all(config_path=\"dictionaries_config.json\", device: str = DEVICE_STR):\n    \"\"\"\n    Loads synonym dicts from config, then builds embeddings for each.\n    \"\"\"\n    global _precomputed, _synonym_dictionaries, _precomputed_embeddings # Added _precomputed_embeddings\n    if _precomputed: return\n    # Load synonym dicts first\n    _synonym_dictionaries = load_synonym_dictionaries(config_path) # Use the loading function\n    if _synonym_dictionaries is None:\n         raise ValueError(\"Synonym dictionaries failed to load.\")\n    model = get_st_model(device=device)\n    # Initialize the dictionary to store computed embeddings\n    _precomputed_embeddings = {}\n    logger.info(f\"Starting embedding precomputation for {len(_synonym_dictionaries)} dictionaries...\")\n    # Dynamically build embeddings based on loaded dicts\n    for dict_key, syn_dict in _synonym_dictionaries.items():\n        logger.info(f\"Processing dictionary: {dict_key}\")\n        if syn_dict: # Check if dictionary is not empty\n            phrases, embeds, label_map = build_candidate_embeddings(syn_dict, model=model, device=device)\n            if embeds is not None: # Check if embedding generation was successful\n                 _precomputed_embeddings[dict_key] = {\n                      \"phrases\": phrases,\n                      \"embeds\": embeds.to(device), # Ensure embeddings are on the correct device\n                      \"label_map\": label_map\n                 }\n                 logger.info(f\"Finished {dict_key}, {len(phrases)} phrases, embedding shape: {embeds.shape}\")\n            else:\n                 logger.error(f\"Embedding generation failed for dictionary: {dict_key}\")\n        else:\n             logger.warning(f\"Skipping empty synonym dictionary: {dict_key}\")\n    _precomputed = True\n    logger.info(f\"All configured synonym embeddings pre-computed and stored on device '{device}'.\")\n\n\nThis function iterates through the synonym dictionaries defined in config/dictionaries_config.json. For each dictionary, such as TILLAGE_PRACTICES_SYNONYMS or ML_AI_METHODS_SYNONYMS, it compiles a comprehensive list of all canonical terms and their associated synonyms. Each of these phrases is then passed through the Sentence Transformer model to generate its corresponding 384-dimensional embedding. The resulting collection of vectors and a mapping that links each vector back to its canonical parent term are cached in memory. This pre-computation creates a static, numerically-indexed semantic map of the entire known vocabulary for each category, which allows subsequent matching operations to be performed as highly efficient numerical comparisons, rather than repeated, computationally expensive model inferences.\n\n\n7.1.8.4 Quantifying Semantic Similarity: The Cosine Similarity Metric\nOnce the field_extraction.py module provides a new, unstructured term for a given field (e.g., “conservation tillage” for the “Tillage Practices” category), the term is first converted into a 384-dimensional query vector, \\(q\\) . The _find_best_match function then systematically compares this query vector to every pre-computed candidate vector, \\(c\\), within the relevant semantic map.\n\n\nCode\ndef _find_best_match(query_emb, dict_key):\n    \"\"\"Internal helper to find best match using precomputed embeddings.\"\"\"\n    global _precomputed_embeddings # Use the dict storing computed data\n    if not _precomputed:\n        logger.error(f\"Embeddings not precomputed. Cannot unify for key '{dict_key}'. Call precompute_all() first.\")\n        # could trigger precomputation here, but better to do it explicitly at startup\n        # precompute_all()\n        # if not _precomputed: # Check again if precomputation failed\n        return None, 0.0 \n\n    if dict_key not in _precomputed_embeddings:\n        logger.warning(f\"No precomputed embeddings found for key '{dict_key}'.\")\n        return None, 0.0\n\n    data = _precomputed_embeddings[dict_key]\n    embeds = data.get(\"embeds\")\n    label_map = data.get(\"label_map\")\n    phrases = data.get(\"phrases\") # For logger\n\n    if embeds is None or label_map is None or phrases is None:\n         logger.error(f\"Precomputed data is incomplete for key '{dict_key}'.\")\n         return None, 0.0\n\n    try:\n        from sentence_transformers import util\n        scores = util.cos_sim(query_emb, embeds)[0]\n        best_score = float(scores.max())\n        best_idx = int(scores.argmax())\n        return best_idx, best_score\n    except Exception as sim_err:\n        logger.error(f\"Error during cosine similarity calculation for key '{dict_key}': {sim_err}\", exc_info=True)\n        return None, 0.0\n\n\nThe metric used for this comparison is the Cosine Similarity, which measures the cosine of the angle (θ) between the two vectors and serves as a measure of their orientation and semantic alignment, independent of their magnitude. The formula for this calculation is:\n\\[\n\\operatorname{sim}(\\mathbf q,\\mathbf c)\n  = \\cos\\theta\n  = \\frac{\\mathbf q \\!\\cdot\\! \\mathbf c}\n         {\\lVert \\mathbf q \\rVert \\,\\lVert \\mathbf c \\rVert}\n  = \\frac{\\displaystyle\\sum_{i=1}^{n} q_i\\,c_i}\n         {\\sqrt{\\displaystyle\\sum_{i=1}^{n} q_i^{2}}\\,\n          \\sqrt{\\displaystyle\\sum_{i=1}^{n} c_i^{2}}}\n\\]\nwhere:\n\n\\(n\\) is the embedding dimension (384 for all-MiniLM-L6-v2);\n\\(\\mathbf q \\in \\mathbb R^{n}\\) is the 384-dimensional vector representing the new, unstructured query term;\n\\(\\mathbf c \\in \\mathbb R^{n}\\) is a 384-dimensional candidate vector from the pre-computed knowledge base;\n\\(q_i\\) and \\(c_i\\) are the \\(i\\)-th components of the query and candidate vectors, respectively.\n\nThe cosine similarity ranges from \\(-1\\) (opposite) to \\(1\\) (identical), where \\(1\\) signifies that the vectors point in the exact same direction (a perfect semantic match), \\(0\\) indicates they are orthogonal (semantically unrelated), and \\(-1\\) indicates they are diametrically opposed.\n\n\n7.1.8.5 Threshold-Based Mapping for Ontological Consistency\nAfter calculating the similarity score between the query vector and all candidate vectors, the system identifies the maximum score, representing the “closest” known term in the semantic space. The final step of the unification process is to transform this probabilistic similarity score into a deterministic mapping. This is achieved by comparing the highest score against a pre-defined confidence threshold (e.g., 0.55). If the score is greater than or equal to this threshold, the match is accepted as valid, and the system uses the pre-computed label_map to retrieve the canonical term associated with the best-matching vector. This canonical term is then used for ingestion into the knowledge graph. If the highest score falls below the threshold, the query term is considered a non-match, ensuring that ambiguous or out-of-domain terms are rejected, thereby safeguarding the ontological integrity and consistency of the final knowledge graph.\n\n\n7.1.8.6 Knowledge Graph Integration\nThe final stage of the ETL pipeline transforms the processed and enriched tabular data into a highly interconnected knowledge graph using a Neo4j graph database. The primary objective is to model the extracted metadata not as isolated rows, but as a network of distinct entities (such as articles, authors, diseases, and methods) and the explicit relationships that connect them. This graph structure enables complex, multi-faceted queries that would be inefficient or impossible with standard relational tables. The entire process, from data preparation to graph ingestion, is orchestrated by scripts/kg_pipeline_gui.py and is driven by a declarative schema defined in the config/kg_pipeline.json file.\n\n\n7.1.8.7 Dynamic Graph Construction\nThe construction of the knowledge graph is a dynamic, two-step process for each article processed by the pipeline:\n\nArticle Node Creation: The process begins by creating or merging a central :Article node, using the article’s Digital Object Identifier (DOI) as its unique key. All primary metadata, such as the title, citation count, Zotero key, and the results of the topic modeling stage, are set as properties on this main node.\nEntity and Relationship Mapping The create_or_update_kg function then iterates through the field_mappings defined in the configuration file. For each field in the dataset (e.g., ml_methods_used), the system dynamically creates MERGE statements in the Cypher query language. For instance, for an article that used “Random Forest”, the system ensures a node (:MLMethod {name: ‘Random Forest’}) exists and then creates a [:USES_ML_METHOD] relationship from the :Article node to it. This procedure is applied across all mapped fields, creating a rich network of interconnected entities such as :HeartDisease, :PollutantTerm, and :StudyType, as visualized in the graph schema (Figure 7.10).\n\n\n\n\n\n\n\nFigure 7.10: Query example\n\n\n\n\n\n7.1.8.8 Querying for Methodological Insights\nThis graph-based structure allows for powerful analytical queries to uncover trends in the literature. For example, to identify the most frequently used machine learning methods specifically within the subset of empirical studies examining ozone and heart disease, the following Cypher query is executed. This query traverses multiple relationship types to precisely filter and aggregate the data, a task well-suited to a graph database.\nMATCH (a:Article)\n// Collect study type names per article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames\n// Filter for empirical studies only (exclude non-empirical types)\nWHERE SIZE(studyTypeNames) &gt; 0\n  AND NONE(stName IN studyTypeNames WHERE stName IN [\n    'review', 'systematic review', 'meta-analysis', 'expert opinion',\n    'scoping review', 'dissertation/thesis', 'short communication',\n    'methodological paper', 'theoretical study', 'report'\n  ])\n// Confirm articles explicitly have ozone, ML methods, and heart disease\nAND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nAND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nAND EXISTS {\n  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(oz:PollutantTerm)\n  WHERE toLower(oz.name) CONTAINS 'ozone'\n}\n// Exclude comment/reply-type articles explicitly\nAND NOT toLower(a.title) CONTAINS 'comment'\nAND NOT toLower(a.title) CONTAINS 'reply'\n// Retrieve ML methods and count their occurrences\nMATCH (a)-[:USES_ML_METHOD]-&gt;(ml:MLMethod)\nRETURN\n    ml.name AS `ML Method`,\n    COUNT(DISTINCT a.doi) AS `Number of Articles`\nORDER BY `Number of Articles` DESC\n╒════════════════════════════════════╤══════════════════╕\n│ML Method                           │Number of Articles│\n╞════════════════════════════════════╪══════════════════╡\n│\"Random Forest\"                     │20                │\n├────────────────────────────────────┼──────────────────┤\n│\"Gradient Boosting Machines\"        │14                │\n├────────────────────────────────────┼──────────────────┤\n│\"Support Vector Machines\"           │13                │\n├────────────────────────────────────┼──────────────────┤\n│\"Neural Networks\"                   │12                │\n├────────────────────────────────────┼──────────────────┤\n│\"K-Means Clustering\"                │9                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Recurrent Neural Networks\"         │7                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Deep Neural Networks\"              │7                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Convolutional Neural Networks\"     │5                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Geographically Weighted Regression\"│3                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Long Short-Term Memory Networks\"   │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Hierarchical Clustering\"           │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Markov Chain Monte Carlo\"          │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Autoencoders\"                      │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Prophet\"                           │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Principal Component Analysis\"      │1                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Extreme Learning Machines\"         │1                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Named Entity Recognition\"          │1                 │\n└────────────────────────────────────┴──────────────────┘\nThe execution of this query against the knowledge graph yields a clear distribution of methodologies employed across the 34 included studies. Tree-based ensemble methods are most prominent, with Random Forest being the predominant technique, utilized in 20 articles (58.8%). Gradient Boosting Machines and Support Vector Machines were also frequently applied, appearing in 14 and 13 studies, respectively. Neural network architectures showed considerable diversity, with the general category of Neural Networks reported in 12 studies, and more specific forms such as Recurrent and Deep Neural Networks each used in 7 studies. The data indicates a strong trend towards applying multiple ML techniques within a single study, with the 34 articles reporting 102 instances of ML method applications, averaging approximately three distinct methods per paper. This detailed, queryable insight into methodological trends is a direct result of structuring the extracted literature as a knowledge graph.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Technical Report</span>"
    ]
  },
  {
    "objectID": "06-technical-report.html#etl-pipeline-summary-and-output",
    "href": "06-technical-report.html#etl-pipeline-summary-and-output",
    "title": "7  Technical Report",
    "section": "7.2 ETL Pipeline Summary and Output",
    "text": "7.2 ETL Pipeline Summary and Output\nThe sequential execution of these seven stages transforms a collection of raw, unstructured PDF documents into a highly structured, semantically unified, and queryable knowledge graph. The process is designed to be both robust and efficient, leveraging parallel processing for I/O-bound tasks like API calls and GPU-bound tasks like document parsing. Sophisticated, configurable heuristics are employed at multiple stages, from pre-emptive artifact filtering and layout post-processing to semantic unification to ensure the quality and consistency of the final data product.\nThe ultimate output of this ETL pipeline is a Neo4j graph database. This database serves as the foundational “non-parametric memory” for the downstream Retrieval-Augmented Generation (RAG) system. Each :Article node in the graph is not simply a bibliographic entry, it is a rich object, connected through explicit relationships to its authors, its key concepts, its methodologies, its findings, and its thematic context within the broader literature. It is this interconnected structure, built upon layers of meticulous data processing and enhancement, that enables the RAG agent to perform nuanced, context-aware queries and synthesize high-fidelity, traceable information to address complex research questions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Technical Report</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "summary.qmd\nThis project centered on addressing the challenges of navigating and synthesizing vast scientific literature to identify novel research questions and methodological gaps. It involved the development, application, and demonstration of an AI-powered, semi-automated pipeline designed for large-scale academic literature analytics and structured data extraction.\n\n9 Brief Restatement of Objectives\nThe primary objectives of this dissertation are threefold:\n\nTo design and implement a robust, end-to-end AI-powered pipeline (Aim 1) combining Extract, Transform, Load (ETL) and Retrieval-Augmented Generation (RAG) to systematically analyze scientific literature and identify methodological research gaps.\nTo conduct a foundational environmental health analysis (Aim 2) by applying the pipeline to the domain of ozone and cardiovascular disease (CVD) and subsequently using established machine learning models to quantify this association, thereby setting a methodological baseline.\nTo develop and validate a state-of-the-art deep learning framework (Aim 3) that directly addresses an identified research gap by using Convolutional and Graph Neural Networks (CNNs/GNNs) to identify, monitor, and analyze unconventional oil and gas (UOG) development from multi-modal satellite data.\nTo lay the groundwork for subsequent empirical research by using the insights and identified gaps from the pipeline to inform the selection of advanced analytical techniques for real-world datasets.\n\n\n\n10 Key Insights and Findings\nThe project yielded several key insights and findings across system development and application:\nSuccessful ETL+RAG System Development:\n\nA modular and configurable pipeline was successfully created, integrating automated literature collection, advanced PDF content extraction, LLM-based field extraction, extensive synonym unification, knowledge graph construction, and an agentic RAG component for nuanced, QA-evaluated querying.\n\nIdentification of a Critical Methodological Gap (Aim 1 & 2):\n\nThe pipeline-driven review successfully identified a significant methodological gap across environmental epidemiology: the notable underutilization of advanced deep learning techniques (CNNs/GNNs) for modeling complex spatiotemporal exposures and their health impacts.\n\nBaseline Ozone-CVD Analysis Insights (Aim 2):\n\nA preliminary Random Forest analysis of county-level ozone and CVD mortality data (adults 35-64) revealed statistically significant associations between various ozone exposure metrics (mean, median, 95th percentile, standard deviation) and log-transformed CVD mortality rates, even after controlling for year and log-transformed population. This analysis highlighted complex, non-linear relationships and interactions, with the Random Forest model (Test R² ≈ 0.47) significantly outperforming a baseline linear model. SHAP analysis indicated that higher ozone exposure generally contributed to increased predicted mortality risk, with population size acting as a significant modulator.\n\nDemonstrated System Versatility:\n\nThe system’s adaptable architecture, particularly its configurable field extraction and rich dictionaries, was shown to be effective in analyzing literature for Life Cycle Assessment (LCA) of agricultural systems. It successfully aided in extracting LCI data and contextualizing empirical findings from a GWP (Global Warming Potential) Random Forest model focused on cover crop impacts, underscoring the pipeline’s interdisciplinary potential.\n\n\n\n11 Practical Implications\nThe outcomes of this dissertation have several practical implications:\n\nA New Tool for Accelerated Research Discovery:\n\n\nThe ETL+RAG system offers a significant acceleration over manual methods, enabling researchers to more efficiently identify and pursue novel research questions.\n\n\nA New Framework for Environmental Monitoring:\n\n\nThe deep learning framework developed in Aim 3 provides a scalable, near real-time method for monitoring industrial activity, creating a new paradigm for environmental regulation, corporate accountability, and exposure science.\n\n\nA Foundational Dataset for Future Health Research:\n\n\nThe UOG site inventory created in Aim 3 will be a critical public resource for epidemiologists seeking to investigate the health impacts of fracking, including potential links to cardiovascular, respiratory, or neurological outcomes..\n\n\nInforming Environmental Health Policy:\n\n\nThe combined insights ranging from understanding ozone’s impact to monitoring methane emissions provide a more holistic evidence base to inform public health policy and air quality standards.\n\n\nFacilitating Knowledge Synthesis in Complex Domains:\n\n\nThe RAG component, with its agentic reasoning and ability to synthesize information from curated, structured knowledge bases, streamlines the learning process when entering or reviewing complex or rapidly evolving fields.\n\n\n\n12 Brief Outlook for Future Work (PhD Dissertation Directions)\n\nThis project has been structured as a cohesive three-part dissertation. The future work is not only an outlook, but the execution of the primary research aims:\n\n\nComplete the Baseline Analysis (Aim 2):\n\n\nFinalize the Random Forest analysis of ozone-CVD associations, including a comprehensive write-up of the systematic literature review and the modeling results. This will form the initial analytical chapters of the dissertation.\n\n\nExecute the Novel Deep Learning Framework (Aim 3):\n\nThis constitutes the central, novel contribution of the dissertation. The primary work will be to:\n\nBuild and train the CNN model to identify UOG sites from high-resolution optical imagery, using geospatial data masks for optimization.\nIntegrate methane satellite data to classify site activity, creating a national UOG inventory.\nApply GNNs to explore spatial network effects and dependencies among the identified sites.\n\n\nSynthesize and Disseminate:\n\n\nThe final stage will involve comparing the findings from Aim 2 and Aim 3, discussing the value of applying advanced deep learning to problems identified by the AI-driven literature analysis. The full dissertation, along with the open-source pipeline and the UOG inventory dataset, will be prepared for publication and public release.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "9  References",
    "section": "",
    "text": "references.qmd\n\n\n\n\n\n\nAditya, G. 2024. “Understanding and Addressing AI Hallucinations in Healthcare and Life Sciences.” International Journal of Health Sciences 7 (3): 1–11. https://doi.org/10.47941/ijhs.1862.\n\n\nAl Ghadban, Yasmina, Huiqi Lu, Uday Adavi, Ankita Sharma, Sridevi Gara, Neelanjana Das, B. R. Kumar, Renu John, Devarsetty Praveen, and Jane E. Hirst. 2023. “Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building Using Retrieval-Augmented Generation.” medRxiv (Cold Spring Harbor Laboratory), December. https://doi.org/10.1101/2023.12.15.23300009.\n\n\nAlonso, Iñigo, Maite Oronoz, and Rodrigo Agerri. 2024. “MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering.” Artificial Intelligence in Medicine 155 (July): 102938–38. https://doi.org/10.1016/j.artmed.2024.102938.\n\n\nAuer, Christoph, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, et al. 2024. “Docling Technical Report.” arXiv. https://doi.org/10.48550/arXiv.2408.09869.\n\n\nBarnett, Scott, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. “Seven Failure Points When Engineering a Retrieval Augmented Generation System,” April. https://doi.org/10.1145/3644815.3644945.\n\n\nBurns, Jacob, Hanna Boogaard, Stephanie Polus, Lisa M. Pfadenhauer, Anke Rohwer, Annemoon M. van Erp, Ruth Turley, and Eva Rehfuess. 2019. “Interventions to Reduce Ambient Particulate Matter Air Pollution and Their Effect on Health.” Cochrane Library 2019 (5). https://doi.org/10.1002/14651858.cd010919.pub2.\n\n\nChen, Chen, Anaïs Teyton, and Tarik Benmarhnia. 2024. “The Temporal Trend and Disparity in Short-Term Health Impacts of Fine Particulate Matter in California (2006–2019).” Science of the Total Environment 954 (December): &lt;NA&gt;. https://doi.org/10.1016/j.scitotenv.2024.176543.\n\n\nClark, Sierra Nicole, Ryan Kulka, Stephane Buteau, Eric Lavigne, Joyce J. Y. Zhang, Christian Riel-Roberge, Audrey Smargiassi, Scott Weichenthal, and Keith Van Ryswyk. 2024. “High-Resolution Spatial and Spatiotemporal Modelling of Air Pollution Using Fixed Site and Mobile Monitoring in a Canadian City.” Environmental Pollution 356 (September): &lt;NA&gt;. https://doi.org/10.1016/j.envpol.2024.124353.\n\n\nDernbach, Stefan, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, and Sutanay Choudhury. 2024. “GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding.” Proceedings of the AAAI Symposium Series 3 (1): 82–89. https://doi.org/10.1609/aaaiss.v3i1.31186.\n\n\nDhole, Kaustubh D. 2025. “To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation.” arXiv. https://doi.org/10.48550/arXiv.2501.09292.\n\n\nDi Maria, Antonio, Lorenzo Bellomo, Fabrizio Billeci, Alfio Cardillo, Salvatore Alaimo, Paolo Ferragina, Alfredo Ferro, and Alfredo Pulvirenti. 2024. “NetMe 2.0: A Web-Based Platform for Extracting and Modeling Knowledge from Biomedical Literature as a Labeled Graph.” Bioinformatics 40 (5). https://doi.org/10.1093/bioinformatics/btae194.\n\n\nDuncan Morapedi, Tshepang, and Ibidun Christiana Obagbuwa. 2023. “Air Pollution Particulate Matter (PM2.5) Prediction in South African Cities Using Machine Learning Techniques.” Frontiers in Artificial Intelligence 6 (October). https://doi.org/10.3389/frai.2023.1230087.\n\n\nFan, Wenqi, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. “A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models.” Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, August, 6491–501. https://doi.org/10.1145/3637528.3671470.\n\n\nFasola, Salvatore, Sara Maio, Sandra Baldacci, Stefania La Grutta, Giuliana Ferrante, Francesco Forastiere, Massimo Stafoggia, et al. 2021. “Short-Term Effects of Air Pollution on Cardiovascular Hospitalizations in the Pisan Longitudinal Study.” International Journal of Environmental Research and Public Health 18 (3): 1164–64. https://doi.org/10.3390/ijerph18031164.\n\n\nFateh Ali, Nurshat, Md. Mahdi Mohtasim, Shakil Mosharrof, and T. Gopi Krishna. 2024. “Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation.” 2022 International Conference on Innovations in Science, Engineering and Technology (ICISET), October, 1–6. https://doi.org/10.1109/iciset62123.2024.10939517.\n\n\nFeng, Zhangyin, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. “Retrieval-Generation Synergy Augmented Large Language Models.” ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March, 11661–65. https://doi.org/10.1109/icassp48485.2024.10448015.\n\n\nFerrer-Cid, Pau. 2023. “On the Data Quality Improvement of Air Pollution Monitoring Low-Cost Sensor Networks Using Data-Driven Techniques,” May. https://doi.org/10.5821/dissertation-2117-399269.\n\n\nFleurence, Rachael, Jiang Bian, Xiaoyan Wang, Hua Xu, Dalia Dawoud, Mitch Higashi, and Jagpreet Chhatwal. 2024a. “Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations - an ISPOR Working Group Report.” Value in Health, November. https://doi.org/10.1016/j.jval.2024.10.3846.\n\n\n———. 2024b. “Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations - an ISPOR Working Group Report.” Value in Health, November. https://doi.org/10.1016/j.jval.2024.10.3846.\n\n\nGangemi, Aldo, and Andrea Giovanni Nuzzolese. 2025. “Logic Augmented Generation.” Journal of Web Semantics, January, 100859–59. https://doi.org/10.1016/j.websem.2024.100859.\n\n\nGirguis, Mariam, Lianfa Li, Fred Lurmann, Jun Wu, Robert Urman, Edward B. Rappaport, Carrie V. Breton, Frank D. Gilliland, Daniel O. Stram, and Rima Habre. 2019. “Exposure Measurement Error in Air Pollution Studies: A Framework for Assessing Shared, Multiplicative Measurement Error in Ensemble Learning Estimates of Nitrogen Oxides.” Environment International 125 (February): 97–106. https://doi.org/10.1016/j.envint.2018.12.025.\n\n\nGu, Bowen, Rishi Desai, Kueiyu Joshua Lin, and Jie Yang. 2024. “Probabilistic Medical Predictions of Large Language Models.” Npj Digital Medicine 7 (1). https://doi.org/10.1038/s41746-024-01366-4.\n\n\nGue, Celeste Ci Ying, Noorul Dharajath Abdul Rahim, William Rojas-Carabali, Rupesh Agrawal, Palvannan Rk, John Abisheganaden, and Wan Fen Yip. 2024. “Evaluating the OpenAI’s GPT-3.5 Turbo’s Performance in Extracting Information from Scientific Articles on Diabetic Retinopathy.” Systematic Reviews 13 (1): 135. https://doi.org/10.1186/s13643-024-02523-2.\n\n\nHuang, Lei, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, et al. 2024a. “A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions.” ACM Transactions on Office Information Systems, November. https://doi.org/10.1145/3703155.\n\n\n———, et al. 2024b. “A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions.” ACM Transactions on Office Information Systems, November. https://doi.org/10.1145/3703155.\n\n\nJ. Bolaños, F., Angelo A. Salatino, Francesco Osborne, and Enrico Motta. 2024. “Artificial Intelligence for Literature Reviews: Opportunities and Challenges.” Artificial Intelligence Review 57 (10). https://doi.org/10.1007/s10462-024-10902-3.\n\n\nKreimeyer, Kory, Jenna VanLiere Canzoniero, Maria Fatteh, Valsamo Anagnostou, and Taxiarchis Botsis. 2024. “Using Retrieval-Augmented Generation to Capture Molecularly-Driven Treatment Relationships for Precision Oncology.” Studies in Health Technology and Informatics, August. https://doi.org/10.3233/shti240575.\n\n\nKrishna E T, Sreya, and Kavitha R. 2024. “Impact of Urban Development on Air Quality and Predictive Analysis.” International Journal of Research Publication and Reviews 5 (3): 1994–2004. https://doi.org/10.55248/gengpi.5.0324.0727.\n\n\nLee, Ha-rin, and Seohyun Kim. 2024. “Bring Retrieval Augmented Generation to Google Gemini via External API: An Evaluation with BIG-Bench Dataset.” Research Square (Research Square), May. https://doi.org/10.21203/rs.3.rs-4394715/v1.\n\n\nLi, Po-hao, and Ya-yun Lai. 2024. “Augmenting Large Language Models with Reverse Proxy Style Retrieval Augmented Generation for Higher Factual Accuracy,” May. https://doi.org/10.31219/osf.io/ma6cq.\n\n\nLiu, Cong, Ka Hung Chan, Jun Lv, Hubert Lam, Katherine Newell, Xia Meng, Yang Liu, et al. 2022. “Long-Term Exposure to Ambient Fine Particulate Matter and Incidence of Major Cardiovascular Diseases: A Prospective Study of 0.5 Million Adults in China.” Environmental Science and Technology 56 (18): 13200–13211. https://doi.org/10.1021/acs.est.2c03084.\n\n\nLiu, Jingjing, Chang Liu, Zhangdaihong Liu, Yibin Zhou, Xiaoguang Li, and Yang Yang. 2025. “Spatial Analysis of Air Pollutant Exposure and Its Association with Metabolic Diseases Using Machine Learning.” BMC Public Health 25 (1): &lt;NA&gt;. https://doi.org/10.1186/s12889-025-22077-9.\n\n\nLiu, Rongqi Abbie, Yaguang Wei, Xinye Qiu, Anna Kosheleva, and Joel D. Schwartz. 2022. “Short Term Exposure to Air Pollution and Mortality in the US: A Double Negative Control Analysis.” Environmental Health: A Global Access Science Source 21 (1): &lt;NA&gt;. https://doi.org/10.1186/s12940-022-00886-4.\n\n\nLiu, Yaqi, Yi Jiang, Manyi Wu, Sunghar Muheyat, Dongai Yao, and Xiaoqing Jin. 2022. “Short-Term Effects of Ambient Air Pollution on Daily Emergency Room Visits for Abdominal Pain: A Time-Series Study in Wuhan, China.” Environmental Science and Pollution Research 29 (27): 40643–53. https://doi.org/10.1007/s11356-021-18200-z.\n\n\nLiu, Zhixin, Chaojie Liu, Yu Cui, Junping Liu, Huanyu Zhang, Yajie Feng, Nan Wang, et al. 2022. “Air Pollution and Refraining from Visiting Health Facilities: A Cross-Sectional Study of Domestic Migrants in China.” BMC Public Health 22 (1): &lt;NA&gt;. https://doi.org/10.1186/s12889-022-14401-4.\n\n\nLuu, Rachel K, and Markus J Buehler. 2023. “BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-Inspired Materials.” Advanced Science (Weinheim, Baden-Wurttemberg, Germany) 11 (10): e2306724. https://doi.org/10.1002/advs.202306724.\n\n\nPerryman, A. N., Hye‐Young Kim, Alexis Payton, Julia E. Rager, Erin E. McNell, Meghan E. Rebuli, Heather Wells, et al. 2023. “Plasma Sterols and Vitamin D Are Correlates and Predictors of Ozone-Induced Inflammation in the Lung: A Pilot Study.” PLoS ONE 18 (5): e0285721–21. https://doi.org/10.1371/journal.pone.0285721.\n\n\nRanjan Maharana, Piyush, and Kavita Joshi. 2024. “Retrieval Augmented Generation for Building Datasets from Scientific Literature,” November. https://doi.org/10.26434/chemrxiv-2024-qjx32.\n\n\nRautela, Kuldeep Singh, and Manish Kumar Goyal. 2024. “Transforming Air Pollution Management in India with AI and Machine Learning Technologies.” Scientific Reports 14 (1): &lt;NA&gt;. https://doi.org/10.1038/s41598-024-71269-7.\n\n\nRawson, Zane. 2024. “The Ngā Ara Whakamana Process: Conceptualising and Designing a Process to Engage with Māori Trust Factors in the Development of IT Artefacts,” December. https://doi.org/10.26686/wgtn.28028579.\n\n\nŞakar, Tolga, and Hakan Emekci. 2024. “Maximizing RAG Efficiency: A Comparative Analysis of RAG Methods.” Natural Language Processing., October, 1–25. https://doi.org/10.1017/nlp.2024.53.\n\n\nShah-Mohammadi, Fatemeh, and Joseph Finkelstein. 2024. “Utilizing RAG and GPT-4 for Extraction of Substance Use Information from Clinical Notes.” Studies in Health Technology and Informatics 321 (November): 94–98. https://doi.org/10.3233/SHTI241070.\n\n\nShi, Liuhua, Kyle Steenland, Haomin Li, Pengfei Liu, Yuhan Zhang, Robert H. Lyles, Weeberb J. Requia, et al. 2021. “A National Cohort Study (2000–2018) of Long-Term Air Pollution Exposure and Incident Dementia in Older Adults in the United States.” Nature Communications 12 (1): &lt;NA&gt;. https://doi.org/10.1038/s41467-021-27049-2.\n\n\nShih, Dong-Her, Thi Hien To, Ly Sy Phu Nguyen, Ting-Wei Wu, and Wen-Ting You. 2021. “Design of a Spark Big Data Framework for Pm&lt;inf&gt;2.5&lt;/Inf&gt; Air Pollution Forecasting.” International Journal of Environmental Research and Public Health 18 (13): &lt;NA&gt;. https://doi.org/10.3390/ijerph18137087.\n\n\nTan, Xi, Yun Qian, Han Wang, Jiayi Fu, and Jiansheng Wu. 2022. “Analysis of the Spatial and Temporal Patterns of Ground-Level Ozone Concentrations in the Guangdong–Hong Kong–Macao Greater Bay Area and the Contribution of Influencing Factors.” Remote Sensing 14 (22): &lt;NA&gt;. https://doi.org/10.3390/rs14225796.\n\n\nTang, Chen, Yiqin Zhang, Jingping Yi, Zhonghua Lu, Xianfa Xuan, Hanxiang Jiang, Dongbei Guo, et al. 2024. “The Association Between Ozone Exposure and Blood Pressure in a General Chinese Middle-Aged and Older Population: A Large-Scale Repeated-Measurement Study.” BMC Medicine 22 (1): &lt;NA&gt;. https://doi.org/10.1186/s12916-024-03783-4.\n\n\nTozuka, Ryota, Hisashi Johno, Akitomo Amakawa, Junichi Sato, Manabu Muto, Shogo Seki, Atsushi Komaba, and Hiroshi Onishi. 2024. “Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging.” Japanese Journal of Radiology, November. https://doi.org/10.1007/s11604-024-01705-1.\n\n\nTsai, Hsiao-Ching, Yueh-Fen Huang, and Chih-Wei Kuo. 2024. “Comparative Analysis of Automatic Literature Review Using Mistral Large Language Model and Human Reviewers.” Research Square (Research Square), March. https://doi.org/10.21203/rs.3.rs-4022248/v1.\n\n\nTu, Qingshi, Jing Guo, Nan Li, Jianchuan Qi, and Ming Xu. 2024. “Mitigating Grand Challenges in Life Cycle Inventory Modeling Through the Applications of Large Language Models.” Environmental Science & Technology 58 (44): 19595–603. https://doi.org/10.1021/acs.est.4c07634.\n\n\nTupayachi, Jose, Haowen Xu, Olufemi A. Omitaomu, Mustafa Can Camur, Aliza Sharmin, and Xueping Li. 2024. “Towards Next-Generation Urban Decision Support Systems Through AI-Powered Construction of Scientific Ontology Using Large Language Models—A Case in Optimizing Intermodal Freight Transportation.” Smart Cities 7 (5): 2392–2421. https://doi.org/10.3390/smartcities7050094.\n\n\nUpadhyay, Rishabh, and Marco Viviani. 2025. “Enhancing Health Information Retrieval with RAG by Prioritizing Topical Relevance and Factual Accuracy.” Deleted Journal 28 (1). https://doi.org/10.1007/s10791-025-09505-5.\n\n\nWang, Dongsheng, Hongwei Wang, Kai-Fa Lu, Zhong‐Ren Peng, and Juanhao Zhao. 2022. “Regional Prediction of Ozone and Fine Particulate Matter Using Diffusion Convolutional Recurrent Neural Network.” International Journal of Environmental Research and Public Health 19 (7): 3988–88. https://doi.org/10.3390/ijerph19073988.\n\n\nWang, Xiaohua, Zhenhua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, et al. 2024. “Searching for Best Practices in Retrieval-Augmented Generation.” Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, January, 17716–36. https://doi.org/10.18653/v1/2024.emnlp-main.981.\n\n\nXie, Xingzhe, Ivana Semanjski, Sidharta Gautama, Evaggelia Tsiligianni, Nikos Deligiannis, Raj Rajan, Frank Pasveer, and Wilfried Philips. 2017. “A Review of Urban Air Pollution Monitoring and Exposure Assessment Methods.” ISPRS International Journal of Geo-Information 6 (12): &lt;NA&gt;. https://doi.org/10.3390/ijgi6120389.\n\n\nXu, Haowen, Xueping Li, Jose Tupayachi, Jianming Lian, and Olufemi A. Omitaomu. 2024. “Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research,” October, 43–49. https://doi.org/10.1145/3681780.3697252.\n\n\nXu, Ruiyu, Ying Hong, Feifei Zhang, and Hongmei Xu. 2024. “Evaluation of the Integration of Retrieval-Augmented Generation in Large Language Model for Breast Cancer Nursing Care Responses.” Scientific Reports 14 (1): 30794. https://doi.org/10.1038/s41598-024-81052-3.\n\n\nYuan, Shuai, Fu Li, Matthew H. E. M. Browning, Mondira Bardhan, Kuiran Zhang, Olivia McAnirlin, Muhammad Mainuddin Patwary, and Aaron Reuben. 2024. “Leveraging and Exercising Caution with ChatGPT and Other Generative Artificial Intelligence Tools in Environmental Psychology Research.” Frontiers in Psychology 15 (April). https://doi.org/10.3389/fpsyg.2024.1295275.\n\n\nZaman, Nurul Amalin Fatihah Kamarul, Kasturi Devi Kanniah, Dimitris G. Kaskaoutis, and Mohd Talib Latif. 2021. “Evaluation of Machine Learning Models for Estimating Pm2.5 Concentrations Across Malaysia.” Applied Sciences (Switzerland) 11 (16): &lt;NA&gt;. https://doi.org/10.3390/app11167326.\n\n\nZhang, Xin, Huiyu Wang, and Chunyun Sun. 2024. “BiSpec Pairwise AI: Guiding the Selection of Bispecific Antibody Target Combinations with Pairwise Learning and GPT Augmentation.” Journal of Cancer Research and Clinical Oncology 150 (5): 237. https://doi.org/10.1007/s00432-024-05740-3.\n\n\nZhang, Yike, Mengxiao Hu, Bowen Xiang, Haiyang Yu, and Qing Wang. 2024. “Urban–Rural Disparities in the Association of Nitrogen Dioxide Exposure with Cardiovascular Disease Risk in China: Effect Size and Economic Burden.” International Journal for Equity in Health 23 (1): &lt;NA&gt;. https://doi.org/10.1186/s12939-024-02117-3.\n\n\nZhu, Qiangqiang, Duncan Lee, and Oliver Stoner. 2024. “A Comparison of Statistical and Machine Learning Models for Spatio-Temporal Prediction of Ambient Air Pollutant Concentrations in Scotland.” Environmental and Ecological Statistics 31 (4): 1085–1108. https://doi.org/10.1007/s10651-024-00635-5.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "update.html",
    "href": "update.html",
    "title": "Project Update",
    "section": "",
    "text": "Week 0",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-0",
    "href": "update.html#week-0",
    "title": "Project Update",
    "section": "",
    "text": "Started preliminary work on literature. Taking a lot of time, need to find faster solution.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-1",
    "href": "update.html#week-1",
    "title": "Project Update",
    "section": "Week 1",
    "text": "Week 1\n\nRead about retrieval augmented generation (RAG). This approach may drastically improve the literature-search step by embedding references and using an LLM to retrieve relevant passages.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-2",
    "href": "update.html#week-2",
    "title": "Project Update",
    "section": "Week 2",
    "text": "Week 2\n\nSpent significant time building an ETL pipeline: installed conda and FAISS for vector indexing.\n\nRealized PDF text extraction would be critical, tried PyPDF2 but it wasn’t good at extracting tables.\nStarted exploring Azure Document Intelligence as an alternative.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-3",
    "href": "update.html#week-3",
    "title": "Project Update",
    "section": "Week 3",
    "text": "Week 3\n\nSuccessfully tested Azure Document Intelligence. It can parse text and tables with higher accuracy than PyPDF2.\n\nBegan integrated script to pipeline: get_pdf_from_unpaywall(), AzurePDFExtractor().",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-4",
    "href": "update.html#week-4",
    "title": "Project Update",
    "section": "Week 4",
    "text": "Week 4\n\nDownloaded and extracted text from ~947 PDFs. Stored full text and tables in .feather format.\nAttempted a first pass at data extraction after chunking/embedding, which caused the LLM to shuffle fields incorrectly (in-text citation ended up in the “Title” field).\nReversed approach: performed data extraction before chunking. This fixed the mismatch issue. Confirmed with a sample of 20 PDFs.\nChain-of-thought (CoT) pipeline flagged repeated metadata assignment errors—a sign that the agentic QA steps are working.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-5-february-8---february-14",
    "href": "update.html#week-5-february-8---february-14",
    "title": "Project Update",
    "section": "Week 5 (February 8 - February 14)",
    "text": "Week 5 (February 8 - February 14)\n\nFeb 8\n\nGoal: Integrate LDA-based topic modeling to see if we can auto-categorize large numbers of articles before the RAG step.\n\nInstalled gensim for LDA experiments. Began experimenting with short pilot on 100 articles to cluster them by topic.\n\n\n\nFeb 9\n\nPipeline Enhancement:\n\nAdded a “topic modeling” function that runs after text extraction but before chunking. Currently operational.\n\nVerified that the topic assignment is stored in a new column (e.g., TopicLabel) for each article.\n\n\nPerformed initial tests on ~20 PDFs. Topics looked coherent, but some short abstracts were mislabeled.\n\n\n\nFeb 10\n\nRAG Implementation:\n\nIntegrated the new topic labels into the retrieval step. If user query references a domain (e.g. “epidemiology” or “machine learning”), we can first filter for relevant topics.\n\nRetried the “Impacts of cover crops on carbon sequestration?” query. The chain-of-thought reasoning is more direct now that we skip non-relevant topics.\n\n\n\n\nFeb 11\n\nAgentic QA Testing:\n\nPosed domain-specific questions, like “What is the best method for measuring greenhouse gas flux in cover crops?” The RAG system returned multiple references with almost no hallucination.\n\nAlso tested random or unrelated queries, like “Ferrari vs. Lamborghini top speed” or “Socioeconomic status of transgender clowns in rural Siberia.” System returned a “lack of relevant info” answer—demonstrating good out-of-scope handling.\n\n\n\n\nFeb 12\n\nDebugging:\n\nFound an edge case where the LLM would try to fabricate DOIs if there was no matching record. Added an explicit check so it returns “no data found.”, also made it invalidate the result if it cannot identify and retrieve four piece of metadata for the referenced articled in the generated response (DOI, in text citation, full citation, zoterokey). works perfectly",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-6-february-15---february-21",
    "href": "update.html#week-6-february-15---february-21",
    "title": "Project Update",
    "section": "Week 6 (February 15 - February 21)",
    "text": "Week 6 (February 15 - February 21)\n\nFeb 15\n\nLocal LLM Setup:\n\nExperimented with Qwen-7B and LLama2-13B locally for chunk embedding to reduce OpenAI API costs.\n\nPerformance is slower than OpenAI embeddings, but cost is zero. Considering a hybrid approach.\n\n\n\n\nFeb 16\n\nChain-of-Thought Refinement:\n\nEnhanced prompt to explicitly mention “If you cannot find a reference in the dataset, do not cite it.” This further reduced spurious citations.\n\nTweaked agentic evaluator to trigger REGENERATE: if any piece of required metadata (ZoteroKey, etc.) is missing.\n\n\n\n\nFeb 17\n\nLarge-Scale Test:\n\nRan pipeline on ~947 articles. Performance was stable, but Azure Document Intelligence costs soared.\n\nDecided to investigate open-source PDF extraction (Docling or Tesseract-based) as a fallback for basic text extraction.\n\n\n\n\nFeb 18\n\nCitation Management:\n\nVerified that the add_pdf_to_zotero() function actually attaches each PDF and auto-generates an item with correct metadata.\n\nConfirmed the get_citation_csl_json() yields accurate APA citations.\n\n\n\n\nFeb 19\n\nTopic Modeling Revisited:\n\nNeed to create a separate .qmd describing how i tested LDA to identify top 10 emergent topics.\n\nwill be writing up results in 02-data-collection.qmd.\nthis is what will be used to identify research gaps and research topcis\n\n\n\n\nFeb 20\n\nRAG Pipeline Finalization:\n\nPerformed final checks on chunking logic (overlap=200 tokens).\n\nImplemented a default “max_tokens=1000” for chunking. Verified no large chunk errors in Qdrant.\n\n\n\n\nFeb 21\n\nValidation:\n\nQueried random nonsense (“Does this set of PDFs mention UFO sightings?”). RAG responded “No relevant data found,” consistent with actual dataset content.\n\nAdded logs to highlight any user query that yields zero results\nCurrent system readiness: ETL & RAG are stable, partial local inference available. Live demo possible",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-7-february-22---february-23",
    "href": "update.html#week-7-february-22---february-23",
    "title": "Project Update",
    "section": "Week 7 (February 22 - February 23)",
    "text": "Week 7 (February 22 - February 23)\n\nFeb 22\n\nFinal Testing:\n\nConducted detailed test with domain questions (cover crops, greenhouse gas, climate change, etc.)\nObserved zero hallucinations, only direct references to text in the chunk embeddings.\n\n\n\n\nFeb 23\n\nConclusion:\n\nAll major pipeline components (ETL + RAG + topic modeling) are operational.\n\nNext steps: start looking for research gaps related to big data and geospatial analysis, sub-focus on environmental health or public health\n\n\n\n\n\nWeek 8 (February 24 - March 2, 2025)\nPipeline Integration Planning:\n\nBegan detailed planning for integrating the standalone ETL, topic modeling, and RAG modules into a cohesive, sequential automated pipeline.\n\nGUI Scoping:\n\nOutlined core requirements and desired functionalities for a PyQt-based Graphical User Interface (GUI) to manage pipeline configurations, execution, and monitoring.\n\nLocal LLM Strategy:\n\nFinalized strategy to prioritize local LLM deployment (Qwen series) for core reasoning tasks to ensure cost-effectiveness and control, reserving API calls for specialized needs or fallback. ___",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#weeks-9-12-march-3---march-30-2025",
    "href": "update.html#weeks-9-12-march-3---march-30-2025",
    "title": "Project Update",
    "section": "Weeks 9-12 (March 3 - March 30, 2025)",
    "text": "Weeks 9-12 (March 3 - March 30, 2025)\nSequential Pipeline Development:\n\nFocused on scripting the end-to-end workflow, ensuring smooth data handoffs between modules: PDF acquisition (async_unpaywall.py), text/table extraction (docling_extract_formulas_mp_multi.py), metadata enrichment (field_extraction.py), Zotero integration (fast_zotero.py), topic modeling (topic_modeling_culda.py), knowledge graph population (kg_pipeline.py), and RAG querying (agentic_pipeline.py).\n\nGUI Prototyping:\n\nDeveloped initial versions of key GUI tabs using PyQt, including the configuration tab (config_tab.py) for managing etl_config.json and kg_pipeline.json, and basic pipeline control elements.\n\nLocal LLM Deployment (Qwen-QwQ32B):\n\nSuccessfully deployed and began testing the Qwen/QwQ32B model locally. Started adapting the agentic_pipeline.py to utilize this model for its advanced reasoning and instruction-following capabilities, noting improved output quality over smaller local models. ___",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#weeks-13-16-march-31---april-27-2025",
    "href": "update.html#weeks-13-16-march-31---april-27-2025",
    "title": "Project Update",
    "section": "Weeks 13-16 (March 31 - April 27, 2025)",
    "text": "Weeks 13-16 (March 31 - April 27, 2025)\nGUI Enhancement & Integration:\n\nSignificantly advanced GUI development, enabling full pipeline orchestration from the interface. Implemented features for selecting specific fields for LLM extraction, initiating pipeline runs, and monitoring progress.\n\nQwen3-32B Integration & Agentic Pipeline Refinement:\nReplaced Qwen/QwQ32 model with Qwen3-32B. the reason is because the QwQ model thinks too hard. Integrated Qwen3-32B into the field_extraction.py and agentic_pipeline.py for all core LLM tasks (field extraction, CoT reasoning, agentic evaluation, QA checks). Conducted extensive testing, confirming superior instruction adherence and output quality compared to previous models, albeit with increased generation time. Developed robust error handling for local LLM inference.\nFine-tuning Data Logging:\n\nImplemented systematic logging of curated LLM interactions (prompts, agentic feedback, final outputs) from the agentic_pipeline.py into a structured JSON format, creating a valuable dataset for potential future fine-tuning of domain-specific LLMs.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#weeks-17-18-april-28---may-11-2025",
    "href": "update.html#weeks-17-18-april-28---may-11-2025",
    "title": "Project Update",
    "section": "Weeks 17-18 (April 28 - May 11, 2025)",
    "text": "Weeks 17-18 (April 28 - May 11, 2025)\nMilestone - Fully Integrated System Operational\n\nAchieved full operational status for the sequential ETL+RAG pipeline, orchestrated via the GUI and utilizing the locally deployed Qwen3-32B LLM.\n\nOzone-CVD Literature Review Application:\n\nCommenced the systematic application of the finalized pipeline to the target literature corpus on ozone exposure, cardiovascular disease, and analytical methodologies, as outlined for the 03-literature-review.qmd. This involved automated data extraction, knowledge graph population, and RAG-based synthesis to identify methodological gaps.\n\nLCA/GWP System Demonstration:\n\nConducted a focused application of the pipeline to a selection of literature on Life Cycle Assessment (LCA) and Global Warming Potential (GWP) in agricultural systems. This successfully demonstrates the system’s interdisciplinary adaptability, particularly its ability to extract LCI data using the configurable field_extraction.py and specialized dictionaries. ___",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-19-may-12---may-18-2025",
    "href": "update.html#week-19-may-12---may-18-2025",
    "title": "Project Update",
    "section": "Week 19 (May 12 - May 18, 2025)",
    "text": "Week 19 (May 12 - May 18, 2025)\nLiterature Review Finalization:\n\nCompleted the primary data extraction, analysis, and synthesis phases for the ozone-CVD literature review (03-literature-review.qmd) using the pipeline’s outputs. This included finalizing the PRISMA-guided study selection facilitated by knowledge graph queries.\n\nOzone-CVD Empirical Analysis - Data Preparation:\n\nInitiated data preprocessing for the CDC ozone and CVD mortality datasets, as detailed in notebooks/data_preparation.ipynb. This involved cleaning, merging, and feature engineering to prepare the data for modeling.\n\nPreliminary Model Development (Random Forest): Began development and initial training runs of the Random Forest model for the preliminary ozone-CVD analysis, detailed in notebooks/random_forest.ipynb and discussed in 04-analysis.qmd. ___",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-20-may-19---may-22-2025",
    "href": "update.html#week-20-may-19---may-22-2025",
    "title": "Project Update",
    "section": "Week 20 (May 19 - May 22, 2025)",
    "text": "Week 20 (May 19 - May 22, 2025)\nProject Documentation & Reporting:\n\nFocused on documenting the project’s progress, including finalizing this “Project Update” section and drafting other components of the overall project report/proposal.\n\nPreliminary Analysis Interpretation:\n\nContinued in-depth analysis and interpretation of the results from the preliminary Random Forest model for ozone-CVD associations, including SHAP value analysis.\n\nAdvanced Modeling Strategy Refinement:\nFurther refined the analytical plan for applying CNNs and GNNs to the ozone-CVD data, incorporating insights from the literature review and the preliminary Random Forest findings.\nCurrent System Readiness:\n\nThe AI-Powered ETL & RAG system is fully operational, GUI-driven, and leverages a high-quality local LLM. It has been successfully applied to generate a comprehensive literature review and is supporting ongoing empirical data analysis. ___",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-21-may-23---may-29-2025",
    "href": "update.html#week-21-may-23---may-29-2025",
    "title": "Project Update",
    "section": "Week 21 (May 23 - May 29, 2025)",
    "text": "Week 21 (May 23 - May 29, 2025)\nPreliminary Analysis Completion (Aim 2):\n\nCompleted the full analytical pipeline for the preliminary Random Forest model, predicting ozone-CVD mortality rates.\nFinalized the data preprocessing steps, including the log-transformation of the outcome variable and key covariates.\nExecuted the hyperparameter tuning (RandomizedSearchCV) and generated all model evaluation metrics (R², RMSE, MAE, OOB score).\nProduced and interpreted all model explanation outputs, including permutation feature importance and a full suite of global and local SHAP analyses.\nAuthored the complete 04-prelim-analysis.qmd chapter, documenting the methodology and presenting the findings.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-22-may-30---june-7-2025",
    "href": "update.html#week-22-may-30---june-7-2025",
    "title": "Project Update",
    "section": "Week 22 (May 30 - June 7, 2025)",
    "text": "Week 22 (May 30 - June 7, 2025)\nTechnical Documentation (Aim 1):\n\nShifted focus from analysis to comprehensive documentation of the core research instrument.\nDrafted and completed the full 05-technical-report.qmd chapter, detailing the architecture and theory of operation for the entire ETL+RAG system.\nDocumented the extensive re-engineering of the Docling library, including the custom parallelization architecture, the “SmolDocling” model integration, and the advanced post-processing heuristics.\nCreated and published the public docling_mod GitHub repository, packaging the modified codebase, testing scripts, and sample data to ensure transparency and reproducibility.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "update.html#week-23-june-8---present-ongoing",
    "href": "update.html#week-23-june-8---present-ongoing",
    "title": "Project Update",
    "section": "Week 23 (June 8 - Present, Ongoing)",
    "text": "Week 23 (June 8 - Present, Ongoing)\nAdvanced Methodology Conceptualization (Aim 3):\n\nBegan conceptualizing and structuring the final research aim: the deep learning framework for UOG site monitoring.\nOutlined the two-phase approach, starting with CNN-based object detection for site identification and followed by spatiotemporal (ConvLSTM) analysis of methane data.\nDeveloped the novel methodological proposal to use county tax parcel data as a masking layer to improve computational efficiency and model performance.\nDrafted a new dissertation chapter dedicated to this proposed methodology, including a detailed, realistic project timeline and key milestones.",
    "crumbs": [
      "Project Update"
    ]
  },
  {
    "objectID": "appendix-a-etl-rag.html",
    "href": "appendix-a-etl-rag.html",
    "title": "",
    "section": "",
    "text": "AppendicesA  appendix-a-etl-rag.html Code",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>appendix-a-etl-rag.html</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html",
    "href": "appendix-b-lca-rf.html",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "",
    "text": "B.1 Logging Configuration\nIn this section, we set up logging to track the progress and important events throughout the pipeline. Logging ensures that all steps (data loading, preprocessing, model tuning, etc.) are recorded in a log file. This is useful for debugging, reproducibility, and for providing transparency.\nCode\n# ================================\n# 0) LOGGING CONFIG\n# ================================\n\n# Set up logging\n\n# Use the absolute path for the G: drive (mounted as /mnt/g)\nLOG_FILE = \"/data/GWP_randomforest_2.log\"\n\nlogging.basicConfig(\n    filename=LOG_FILE,\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\n\nlogging.info(\"Starting the GWP Prediction Pipeline\")\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#data-loading-and-exploration",
    "href": "appendix-b-lca-rf.html#data-loading-and-exploration",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.2 Data Loading and Exploration",
    "text": "B.2 Data Loading and Exploration\nHere we read the dataset from a Feather file. Feather is a fast binary file format for data frames, making data loading efficient. We then: - Preview the Data: Using df.head() to view the first few rows. - Summarize the Data: Using df.describe() to get summary statistics (mean, standard deviation, etc.) for each feature.\nThis step helps us understand the structure, size, and distribution of the data before any processing.\n\n\nCode\n# ================================\n# 1) Data Loading and Exploration\n# ================================\n\nlogging.info(\"STEP 1: Reading data and exploring\")\n\n# Read in the data (adjust the file path as needed)\ndf = pd.read_feather('data/LCA_DATA.feather')\n\n\n# Data Exploration: Print head and summary statistics\nlogging.info(\"Data Head:\\n%s\", df.head())\nlogging.info(\"Data Summary:\\n%s\", df.describe())\nprint(\"Data Head:\")\nprint(df.head())\nprint(\"\\nData Summary:\")\nprint(df.describe())",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#data-preprocessing",
    "href": "appendix-b-lca-rf.html#data-preprocessing",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.3 Data Preprocessing",
    "text": "B.3 Data Preprocessing\nIn this section, we prepare the data for modeling by:\n\nOne-Hot Encoding: Converting categorical variables (oil, Scenario, LULC, CoverCrop, Rotation) into dummy variables. This allows us to use them in machine learning models.\nDropping Irrelevant Columns: Removing columns that are not useful for prediction (identifiers, target variables already separated).\nHandling Missing Values: Filling missing values with a simple strategy (here, 0) to avoid issues during training.\nDropping Constant Columns: Removing columns that have only one unique value since they don’t contribute any information.\n\nThese steps ensure the dataset is clean and structured appropriately for machine learning.\n\n\nCode\n# ================================\n# 2) Data Preprocessing\n# ================================\nlogging.info(\"STEP 2: Data preprocessing and one-hot encoding\")\n\n# Define target and categorical columns\ntarget_column = \"GWP_Total\"  # variable to predict\ncategorical_cols = [\"Soil\", \"Scenario\", \"LULC\", \"CoverCrop\", \"Rotation\"]  # categorical features\nexcluded_cols = [target_column, \"seed_GWP_main\"]  # columns to exclude\n\n# One-hot encode categorical columns (drop_first=False to retain all info)\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\nprint(\"\\nColumns after one-hot encoding:\")\nlogging.info(\"Columns after one-hot encoding:\\n%s\", df_encoded.columns)\nprint(df_encoded.columns)\n\n# Separate target y and drop excluded columns from predictors\ny = df_encoded[target_column]\ndf_encoded = df_encoded.drop(columns=excluded_cols, errors=\"ignore\")\nX = df_encoded.drop(columns=excluded_cols, errors=\"ignore\")\n\n# Fill missing values simply and drop constant columns\nX = X.fillna(0)\ncols_constant = [col for col in X.columns if X[col].nunique() == 1]\nlogging.info(\"Dropping constant columns: %s\", cols_constant)\nX = X.drop(columns=cols_constant)\nlogging.info(\"Columns after dropping excluded columns:\\n%s\", X.columns)\n\nprint(\"\\nColumns after dropping excluded cols:\")\nprint(X.columns)\nprint(\"Dropping constant columns:\", cols_constant)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#data-preprocessing-output-explanation",
    "href": "appendix-b-lca-rf.html#data-preprocessing-output-explanation",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.4 Data Preprocessing – Output Explanation",
    "text": "B.4 Data Preprocessing – Output Explanation\nAfter running the data preprocessing code, we observe the following outputs:\nColumns after one-hot encoding: Index([‘BROIL_FERT’, ‘BROIL_MINN’, ‘BROIL_MINP’, ‘BROIL_ORGN’, ‘BROIL_ORGP’, ‘BROIL_NH3N’, ‘UAN30_FERT’, ‘UAN30_MINN’, ‘UAN30_NH3N’, ‘P_APPkg_ha’, ‘pest_app_LULC’, ‘pest_app_CoverCrop’, ‘sowing_count’, ‘GWP_Total’, ‘total_glyphosate_kg’, ‘seed_rate_main’, ‘seed_rate_cc’, ‘seed_GWP_main’, ‘Soil_LoSa’, ‘Soil_SaLo’, ‘Soil_SiLo’, ‘Scenario_BE’, ‘Scenario_BL’, ‘Scenario_RE’, ‘Scenario_RL’, ‘Scenario_WE’, ‘Scenario_WL’, ‘LULC_SOYb’, ‘LULC_WWHT’, ‘CoverCrop_RYE’, ‘CoverCrop_WWHT’, ‘CoverCrop_WbAR’, ‘Rotation_AGA2’, ‘Rotation_AGAB’, ‘Rotation_AGB1’, ‘Rotation_AGB2’, ‘Rotation_AGC1’, ‘Rotation_AGD1’, ‘Rotation_AGD2’], dtype=‘object’)\n\nB.4.1 Columns after One-Hot Encoding\nThe printed output shows an index (list) of column names:\nIndex([‘BROIL_FERT’, ‘BROIL_MINN’, ‘BROIL_MINP’, ‘BROIL_ORGN’, ‘BROIL_ORGP’, ‘BROIL_NH3N’, ‘UAN30_FERT’, ‘UAN30_MINN’, ‘UAN30_NH3N’, ‘P_APPkg_ha’, ‘pest_app_LULC’, ‘pest_app_CoverCrop’, ‘sowing_count’, ‘total_glyphosate_kg’, ‘seed_rate_main’, ‘seed_rate_cc’, ‘Soil_LoSa’, ‘Soil_SaLo’, ‘Soil_SiLo’, ‘Scenario_BE’, ‘Scenario_BL’, ‘Scenario_RE’, ‘Scenario_RL’, ‘Scenario_WE’, ‘Scenario_WL’, ‘LULC_SOYb’, ‘LULC_WWHT’, ‘CoverCrop_RYE’, ‘CoverCrop_WWHT’, ‘CoverCrop_WbAR’, ‘Rotation_AGA2’, ‘Rotation_AGAB’, ‘Rotation_AGB1’, ‘Rotation_AGB2’, ‘Rotation_AGC1’, ‘Rotation_AGD1’, ‘Rotation_AGD2’], dtype=‘object’)\n\nWhat This Means:\nAll the original columns are present, including the target variable GWP_Total. Additionally, the categorical variables (like Soil, Scenario, etc.) have been converted into dummy variables. For example, we see columns like Soil_LoSa, Soil_SaLo, etc., which represent the different levels of the original Soil category.\n\n\n\nB.4.2 Columns after Dropping Excluded Columns\nAfter removing the columns that are not needed for prediction (the target variable and any explicitly excluded ones), the output is:\nIndex([‘BROIL_FERT’, ‘BROIL_MINN’, ‘BROIL_MINP’, ‘BROIL_ORGN’, ‘BROIL_ORGP’, ‘BROIL_NH3N’, ‘UAN30_FERT’, ‘UAN30_MINN’, ‘UAN30_NH3N’, ‘P_APPkg_ha’, ‘pest_app_LULC’, ‘pest_app_CoverCrop’, ‘sowing_count’, ‘total_glyphosate_kg’, ‘seed_rate_main’, ‘seed_rate_cc’, ‘Soil_LoSa’, ‘Soil_SaLo’, ‘Soil_SiLo’, ‘Scenario_BE’, ‘Scenario_BL’, ‘Scenario_RE’, ‘Scenario_RL’, ‘Scenario_WE’, ‘Scenario_WL’, ‘LULC_SOYb’, ‘LULC_WWHT’, ‘CoverCrop_RYE’, ‘CoverCrop_WWHT’, ‘CoverCrop_WbAR’, ‘Rotation_AGA2’, ‘Rotation_AGAB’, ‘Rotation_AGB1’, ‘Rotation_AGB2’, ‘Rotation_AGC1’, ‘Rotation_AGD1’, ‘Rotation_AGD2’], dtype=‘object’)\n\nWhat This Means:\nThe target column GWP_Total and the excluded column seed_GWP_main have been removed from the predictor matrix X. Now, X contains only the features that will be used to predict GWP_Total.\n\n\n\nB.4.3 Dropping Constant Columns\nThe output: Dropping constant columns: []\n\nWhat This Means:\nThis indicates that no columns in X had only a single unique value. In other words, every predictor varies to some extent, so no columns were dropped for being constant.\n\n\nthese outputs confirm that: - Categorical variables have been correctly transformed into dummy variables. - Irrelevant columns (target and specified exclusions) have been successfully removed. - The data is clean and ready for the next steps in the pipeline.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#traintest-split",
    "href": "appendix-b-lca-rf.html#traintest-split",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.5 Train/Test Split",
    "text": "B.5 Train/Test Split\nWe split the dataset into: - Training Set (80%): Used to train the model. - Testing Set (20%): Held out for evaluating the model’s performance on unseen data.\nThis split is crucial to validate that our model generalizes well beyond the data it was trained on.\n\n\nCode\n# ================================\n# 3) Train/Test Split\n# ================================\nlogging.info(\"STEP 3: Splitting data into train/test\")\n\n# Split the data (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42)\nlogging.info(\"Train shape = %s, Test shape = %s\", X_train.shape, X_test.shape)\nprint(f\"\\nTraining set size: {X_train.shape}, Test set size: {X_test.shape}\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#traintest-split-output-explanation",
    "href": "appendix-b-lca-rf.html#traintest-split-output-explanation",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.6 Train/Test Split – Output Explanation",
    "text": "B.6 Train/Test Split – Output Explanation\nAfter splitting the dataset into training and testing sets, we obtain the following:\n\nTraining Set Size: (11883, 37)\nThis means there are 11,883 samples (rows) and 37 features (columns) in the training set.\n\nTesting Set Size: (2971, 37)\nThe test set contains 2,971 samples and the same 37 features.\n\nWhat This Means:\n\n80/20 Split:\nApproximately 80% of the data is used for training the model, and 20% is reserved for evaluating its performance on unseen data. This is crucial to ensure that our model generalizes well and is not overfitting.\nConsistent Feature Dimensions:\nBoth sets have 37 features, which confirms that the preprocessing steps (e.g., one-hot encoding, dropping irrelevant columns) have been applied consistently across the entire dataset.\n\nOverall, these numbers indicate a successful split, setting the stage for model training and evaluation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#hyperparameter-tuning-with-randomizedsearchcv",
    "href": "appendix-b-lca-rf.html#hyperparameter-tuning-with-randomizedsearchcv",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.7 Hyperparameter Tuning with RandomizedSearchCV",
    "text": "B.7 Hyperparameter Tuning with RandomizedSearchCV\nHere, we use RandomizedSearchCV to optimize the hyperparameters of the Random Forest model. Key points include:\n\nParameter Grid: We define ranges for parameters such as n_estimators, max_depth, min_samples_split, min_samples_leaf, and max_features.\nRandomized Search: Rather than checking every possible combination (which can be computationally expensive), we sample a subset of parameter settings.\nCross-Validation (CV=3): For each parameter combination, 3-fold cross-validation is used to estimate performance (using negative RMSE here).\n\nThe best hyperparameters are selected based on cross-validation results, ensuring that the final model is tuned for optimal performance.\n\n\nCode\n# ================================\n# 4) Hyperparameter Tuning with RandomizedSearchCV\n# ================================\nlogging.info(\"STEP 4: Hyperparameter Tuning with RandomizedSearchCV\")\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Expanded parameter grid includes min_samples_leaf and max_features\nparam_dist = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 5],\n    'max_features': ['auto', 'sqrt']\n}\n\n# Instantiate RandomForestRegressor with oob_score enabled for extra validation\nrf = RandomForestRegressor(random_state=42, oob_score=True)\n\nsearch = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_dist,\n    n_iter=10, \n    scoring='neg_root_mean_squared_error',\n    cv=3, \n    verbose=1,\n    random_state=42,\n    n_jobs=-1\n)\n\nsearch.fit(X_train, y_train)\nbest_model = search.best_estimator_\nprint(\"Best params:\", search.best_params_)\nlogging.info(\"Best params from RandomizedSearchCV: %s\", search.best_params_)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#hyperparameter-tuning-with-randomizedsearchcv-output-explanation",
    "href": "appendix-b-lca-rf.html#hyperparameter-tuning-with-randomizedsearchcv-output-explanation",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.8 Hyperparameter Tuning with RandomizedSearchCV – Output Explanation",
    "text": "B.8 Hyperparameter Tuning with RandomizedSearchCV – Output Explanation\nIn this step, we optimize the hyperparameters of our Random Forest model using RandomizedSearchCV. Here’s what the output tells us:\n\nFitting 3 folds for each of 10 candidates, totalling 30 fits:\nThis message indicates that the search evaluated 10 different hyperparameter configurations (candidates). For each configuration, 3-fold cross-validation was performed (i.e., the training set was split into 3 parts, with 2 used for training and 1 for validation), resulting in a total of 30 model fits.\nBest params:\nThe output displays the best set of hyperparameters found by the search. In this example, the best parameters are:\n\nn_estimators: 200 (i.e., the model will use 200 trees)\n\nmin_samples_split: 10 (i.e., a node must have at least 10 samples before it can be split)\n\nmin_samples_leaf: 2 (i.e., each leaf must have at least 2 samples)\n\nmax_features: ‘sqrt’ (i.e., the square root of the number of features will be considered when looking for the best split)\n\nmax_depth: 20 (i.e., the maximum depth of each tree is set to 20)\n\n\nInterpretation:\n- The RandomizedSearchCV process sampled 10 configurations from our parameter grid and evaluated each using 3-fold cross-validation.\n- The configuration with the lowest (negative) RMSE during cross-validation was chosen.\n- These best parameters will be used in the final model, ensuring that our Random Forest is well-tuned to our data, potentially leading to improved performance on unseen data.\nThis detailed tuning step helps in making our model more robust and generalizable.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#model-evaluation",
    "href": "appendix-b-lca-rf.html#model-evaluation",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.9 Model Evaluation",
    "text": "B.9 Model Evaluation\nThis section evaluates the performance of our tuned Random Forest model using multiple metrics:\n\nR² (Coefficient of Determination): Measures the proportion of variance explained by the model.\nRMSE (Root Mean Squared Error): Indicates the average magnitude of the prediction error.\nMAE (Mean Absolute Error): Provides the average absolute error, which is less sensitive to outliers than RMSE.\nOut-of-Bag (OOB) Score: An internal cross-validation metric available in Random Forests when oob_score=True.\n\n\nB.9.1 Extra Validation Checks\n\nLearning Curves: We plot learning curves to see how training and validation errors change with increasing training set size. This helps diagnose overfitting or underfitting.\nPermutation Feature Importance: Measures how the model’s performance deteriorates when a feature’s values are randomly shuffled. This provides an alternative view of feature importance.\nResidual Analysis: A plot of residuals (actual minus predicted values) to check for any systematic patterns that the model might be missing.\nBaseline Comparison: We also fit a simple Linear Regression model to compare its performance with our Random Forest model.\n\nThese checks provide a comprehensive view of the model’s performance and robustness.\n\n\nCode\n# ================================\n# 5) Model Evaluation\n# ================================\nlogging.info(\"STEP 5: Evaluating the best model\")\n\n# Predictions on training and test sets\ny_train_pred = best_model.predict(X_train)\ny_test_pred = best_model.predict(X_test)\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)\n\n# Compute performance metrics\nr2_train  = r2_score(y_train, y_train_pred)\nrmse_train = rmse(y_train, y_train_pred)\nmae_train  = mean_absolute_error(y_train, y_train_pred)\n\nr2_test   = r2_score(y_test, y_test_pred)\nrmse_test = rmse(y_test, y_test_pred)\nmae_test  = mean_absolute_error(y_test, y_test_pred)\n\nlogging.info(\"Training set: R^2 = %.3f, RMSE = %.3f, MAE = %.3f\", r2_train, rmse_train, mae_train)\nlogging.info(\"Test set:     R^2 = %.3f, RMSE = %.3f, MAE = %.3f\", r2_test, rmse_test, mae_test)\nprint(\"\\nModel Performance:\")\nprint(f\"Training set: R^2 = {r2_train:.3f}, RMSE = {rmse_train:.3f}, MAE = {mae_train:.3f}\")\nprint(f\"Test set: R^2 = {r2_test:.3f}, RMSE = {rmse_test:.3f}, MAE = {mae_test:.3f}\")\n\n# OOB Error (if available)\nif hasattr(best_model, \"oob_score_\"):\n    oob_score = best_model.oob_score_\n    logging.info(\"OOB Score: %.3f\", oob_score)\n    print(f\"OOB Score: {oob_score:.3f}\")\n\n# ================================\n# Extra Validation: Learning Curves\n# ================================\nlogging.info(\"STEP 5A: Generating learning curves\")\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n    best_model, X_train, y_train, cv=3, scoring='neg_root_mean_squared_error',\n    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)\n\ntrain_scores_mean = -np.mean(train_scores, axis=1)\ntest_scores_mean = -np.mean(test_scores, axis=1)\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training RMSE\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Validation RMSE\")\nplt.title(\"Learning Curves\")\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"RMSE\")\nplt.legend(loc=\"best\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nlogging.info(\"Learning curves generated\")\n\n# ================================\n# Extra Validation: Permutation Feature Importance\n# ================================\nlogging.info(\"STEP 5B: Calculating permutation feature importance\")\nperm_importance = permutation_importance(best_model, X_test, y_test, scoring='neg_root_mean_squared_error', n_repeats=10, random_state=42)\nperm_importance_df = pd.DataFrame({\n    'feature': X_test.columns,\n    'importance_mean': perm_importance.importances_mean,\n    'importance_std': perm_importance.importances_std\n}).sort_values(by='importance_mean', ascending=False)\nprint(\"\\nPermutation Feature Importance:\")\nprint(perm_importance_df)\nlogging.info(\"Permutation Feature Importance:\\n%s\", perm_importance_df)\n\n# ================================\n# Extra Validation: Residual Analysis\n# ================================\nlogging.info(\"STEP 5C: Plotting residuals\")\nresiduals = y_test - y_test_pred\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test_pred, residuals, alpha=0.7)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Residuals (Actual - Predicted)\")\nplt.title(\"Residual Plot\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nlogging.info(\"Residual plot generated\")\n\n# ================================\n# Extra Validation: Baseline Model Comparison\n# ================================\nlogging.info(\"STEP 5D: Comparing with baseline Linear Regression\")\nbaseline_model = LinearRegression()\nbaseline_model.fit(X_train, y_train)\ny_test_pred_baseline = baseline_model.predict(X_test)\nr2_baseline = r2_score(y_test, y_test_pred_baseline)\nrmse_baseline = rmse(y_test, y_test_pred_baseline)\nmae_baseline = mean_absolute_error(y_test, y_test_pred_baseline)\nprint(\"\\nBaseline Linear Regression Performance:\")\nprint(f\"R^2 = {r2_baseline:.3f}, RMSE = {rmse_baseline:.3f}, MAE = {mae_baseline:.3f}\")\nlogging.info(\"Baseline Linear Regression: R^2 = %.3f, RMSE = %.3f, MAE = %.3f\", r2_baseline, rmse_baseline, mae_baseline)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#model-evaluation-output-explanation",
    "href": "appendix-b-lca-rf.html#model-evaluation-output-explanation",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.10 Model Evaluation – Output Explanation",
    "text": "B.10 Model Evaluation – Output Explanation\nAfter training our tuned Random Forest model, we evaluate its performance using multiple metrics and additional validation checks.\n\nB.10.1 Performance Metrics\nPrinted Output: Training set: R^2 = 0.702, RMSE = 3001.564, MAE = 2025.280 Test set: R^2 = 0.668, RMSE = 3155.660, MAE = 2147.089 OOB Score: 0.661\n\nR² (Coefficient of Determination):\n\nTraining R² = 0.702\n\nTest R² = 0.668\nThe model explains roughly 70% of the variance in the training set and about 67% in the test set. This small gap indicates that the model generalizes reasonably well with some overfitting.\n\nRMSE (Root Mean Squared Error):\n\nTraining RMSE ≈ 3001\n\nTest RMSE ≈ 3156\nRMSE gives the average magnitude of the prediction error. A difference of about 155 between training and test suggests the model’s performance on unseen data is close to its performance on training data.\n\nMAE (Mean Absolute Error):\n\nTraining MAE ≈ 2025\n\nTest MAE ≈ 2147\nMAE is less sensitive to outliers than RMSE. These values confirm a consistent level of error on training vs. test sets.\n\nOut-of-Bag (OOB) Score = 0.661\nRandom Forests can estimate performance internally (without a separate validation set) by using out-of-bag samples. An OOB score of 0.661 aligns closely with the test R² of 0.668, giving additional confidence in the model’s ability to generalize.\n\n\n\nB.10.2 Learning Curves\n\n\n\nLearning Curves\n\n\nThe Learning Curves plot shows how RMSE (vertical axis) changes for both the training set (red line) and validation set (green line) as the number of training examples increases (horizontal axis). Key observations:\n\nTraining RMSE remains relatively stable around ~3000, indicating the model fits the training data consistently.\nValidation RMSE decreases from about 3700 to 3200 as more training data is used, suggesting that adding more data helps reduce error.\nThe gap between training and validation curves narrows but does not fully converge, indicating the model could still benefit from more data or further hyperparameter tuning. However, the gap is not excessively large, suggesting a reasonable balance between bias and variance.\n\n\n\nB.10.3 Permutation Feature Importance\nWhat It Means:\nPermutation Importance measures how much the model’s performance deteriorates when each feature’s values are randomly shuffled. Features with higher importance_mean cause a greater increase in RMSE when shuffled, indicating they are more crucial to the model. This provides an alternative perspective on which predictors matter most, complementing the built-in feature importance from the Random Forest itself.\n\n\nB.10.4 Residual Analysis\n\n\n\nResidual Plot\n\n\nThe Residual Plot (actual minus predicted values on the y-axis vs. predicted values on the x-axis) helps identify patterns in the errors:\n\nThe red dashed line at y=0 indicates perfect predictions.\nResiduals appear mostly centered around 0, with some spread at higher predicted values.\nNo strong pattern or trend suggests the model is systematically over- or under-predicting at particular ranges.\nThe moderate spread indicates the model still has some variance in its predictions, but there is no major sign of severe bias.\n\n\n\nB.10.5 Baseline Comparison\nBaseline Linear Regression Performance: R^2 = 0.576, RMSE = 3566.148, MAE = 2570.892\n\nLinear Regression R² = 0.576\n\nLinear Regression RMSE ≈ 3566\n\nLinear Regression MAE ≈ 2571\n\nCompared to our Random Forest (R² = 0.668), the baseline linear model performs worse across all metrics (lower R², higher RMSE, and higher MAE). This shows that the additional complexity of the Random Forest provides a noticeable performance boost and better captures the underlying relationships in the data.\n\n\n\nB.10.6 Summary of Model Evaluation\n\nMetrics show the Random Forest performing well, with a small gap between training and test performance.\nLearning Curves suggest the model benefits from more data and is not drastically overfitting.\nPermutation Importance corroborates which features the model relies on most.\nResidual Plot reveals no major systematic bias in predictions, though there is some variance at higher values.\nBaseline Comparison confirms that the Random Forest outperforms a simpler Linear Regression model.\n\nOverall, these results indicate a reasonably robust model that generalizes better than a simple baseline and provides insights into which features matter most.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#feature-importance-and-partial-dependence",
    "href": "appendix-b-lca-rf.html#feature-importance-and-partial-dependence",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.11 Feature Importance and Partial Dependence",
    "text": "B.11 Feature Importance and Partial Dependence\nUnderstanding which features drive the model predictions is key for interpretability. In this section, we:\n\nBuilt-in Feature Importance: Leverage the Random Forest’s internal mechanism to rank features based on how much they reduce impurity.\nBar Plot Visualization: Display the top 10 features to quickly identify the most influential predictors.\nPartial Dependence Plots (PDP): Visualize the effect of the top features on the predicted outcome while holding other features constant. This helps in understanding the relationship between predictors and the target variable.\n\nThese tools help understand the model’s decision-making process.\n\n\nCode\n# ================================\n# 6) Feature Importance and Partial Dependence\n# ================================\n\"\"\"\nFEATURE IMPORTANCE:\n    Lists the top 10 features based on the model’s built-in importance scores.\n    Produces a bar chart so we can visually interpret which features are most influential.\n\"\"\"\nlogging.info(\"STEP 6: Feature importance plots & partial dependence\")\n\n# Built-in feature importance from Random Forest\nimportances = best_model.feature_importances_\nfeat_imp = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)\nlogging.info(\"Top 10 Features:\\n%s\", feat_imp.head(20))\nprint(\"\\nTop 10 Feature Importances:\")\nprint(feat_imp.head(10))\n\n# Bar plot for feature importance\nplt.figure(figsize=(10, 6))\nsns.barplot(x=feat_imp.head(10), y=feat_imp.head(10).index, palette=\"viridis\")\nplt.title(\"Top 10 Feature Importances (Random Forest)\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.tight_layout()\nplt.show()\n\n# Partial Dependence Plots for top features\ntop_features = feat_imp.head(3).index.tolist()\nlogging.info(\"Generating partial dependence plots for top features: %s\", top_features)\nfig, ax = plt.subplots(figsize=(12, 8))\nPartialDependenceDisplay.from_estimator(best_model, X_train, top_features, n_jobs=-1, ax=ax)\nfig.suptitle(\"Partial Dependence of Top Features\")\nplt.tight_layout()\nplt.show()\n\n\n\nB.11.1 Built-in Feature Importance\nPrinted Output (Top 10 Features):\nSoil_SaLo 0.077287 Soil_SiLo 0.076775 BROIL_MINN 0.065416 pest_app_LULC 0.056814 seed_rate_main 0.055316 P_APPkg_ha 0.053626 Rotation_AGC1 0.052144 BROIL_MINP 0.050464 BROIL_NH3N 0.046516 BROIL_ORGN 0.043727 dtype: float64\n\nWhat It Means:\nThe Random Forest model computes an “importance” score for each feature, reflecting how much that feature reduces impurity (variance) in the trees. Higher values indicate more influential predictors.\n\nSoil_SaLo and Soil_SiLo appear to be the top two features, suggesting that soil types have a significant impact on GWP_Total.\n\nBROIL_MINN (a nitrogen-related feature) also ranks high, indicating fertilizer- or manure-related variables may be critical to the model.\n\npest_app_LULC, seed_rate_main, and P_APPkg_ha follow, further suggesting that application rates and crop management choices drive GWP_Total.\n\n\n\n\nB.11.2 Bar Chart Visualization\n\n\n\nTop 10 Feature Importances (Random Forest)\n\n\nIn the bar chart:\n\nHorizontal Axis (Importance): Shows how crucial each feature is.\nVertical Axis (Feature): Lists the top 10 features in descending order of importance.\n\nThis chart helps both technical and non-technical audiences quickly see which variables most influence the model’s predictions.\n\n\nB.11.3 Partial Dependence Plots (PDP)\nPartial Dependence Plots help us understand how the most important features influence the model’s prediction for GWP_Total. Each plot shows the effect of a single feature while averaging out the effects of all other features.\n\n\n\nPartial Dependence of Top Features\n\n\n\nWhat PDP Shows:\nEach subplot corresponds to one of the top 3 features. On the x-axis is the feature’s possible values; on the y-axis is the partial dependence, which represents the model’s predicted outcome when varying that feature while holding others constant.\n\n\n\nB.11.4 Interpretation of Key Findings\n\nSoil_SaLo (Sandy Loam Soil)\nThis plot shows a strong negative relationship. The model predicts a significantly lower GWP_Total when the soil type is Sandy Loam (value of 1) compared to other types. This suggests that sandy loam soil is a key factor in mitigating GWP in this model.\nSoil_SiLo (Silty Loam Soil)\nIn contrast, the model associates Silty Loam soil (value of 1) with an increase in the predicted GWP_Total.\nBROIL_MINN\nThis feature displays a clear non-linear relationship. The model predicts the lowest GWP_Total when BROIL_MINN is at a “sweet spot” of approximately 25. Values lower or higher than this optimum level are associated with an increase in predicted GWP. This complex pattern highlights the model’s ability to capture relationships that a standard linear regression would miss.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#shap-analysis-for-model-interpretability",
    "href": "appendix-b-lca-rf.html#shap-analysis-for-model-interpretability",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.12 SHAP Analysis for Model Interpretability",
    "text": "B.12 SHAP Analysis for Model Interpretability\nTo understand the drivers of the model’s predictions, SHAP (SHapley Additive exPlanations) analysis was performed. The following bar plot shows the global feature importance, ranking features by their average impact on the model’s output magnitude.\n\nB.12.1 Interpretation of Key Findings\n\nPrimary Drivers (Soil Type): Consistent with other importance measures, soil types are the most influential features. Soil_SaLo (Sandy Loam) has the largest average impact on the model’s predictions, followed closely by Soil_LoSa (Loamy Sand).\nSecondary Drivers (Management & Inputs): After soil type, key management and input variables show significant importance. These include BROIL_MINN (a nitrogen-related feature), seed_rate_main, and pest_app_LULC (pesticide application).\nTertiary Drivers: Other nitrogen and phosphorus-related inputs (BROIL_MINP, P_APPkg_ha) and specific crop rotations (Rotation_AGC1) also contribute meaningfully to the predictions, but with less overall impact than the primary and secondary drivers.\n\nThis ranking confirms that the model’s predictions for GWP_Total are most heavily influenced by the inherent soil characteristics of a site, followed by key agricultural inputs and management decisions.\n\n\nCode\n# ================================\n# 7) SHAP Analysis for Interpretability\n# ================================\n\"\"\"\nSHAP ANALYSIS\n    Provides global interpretability through SHAP summary plots.\n    Gives a local explanation (force plot) for one individual test case, \n    which shows how each feature contributes to a single prediction.\n\"\"\"\nlogging.info(\"STEP 7: SHAP analysis for global & local interpretability\")\n\n# SHAP: Global and local interpretability\nexplainer = shap.TreeExplainer(best_model)\nshap_values = explainer.shap_values(X_train)\n\n# Global SHAP plots (bar and summary)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\", show=False)\nplt.title(\"SHAP Feature Importance (Bar Plot)\")\nplt.tight_layout()\nplt.show()\n\nshap.summary_plot(shap_values, X_train, show=False)\nplt.title(\"SHAP Summary Plot\")\nplt.tight_layout()\nplt.show()\n\n# Local SHAP explanation for the first test instance\nlogging.info(\"Generating SHAP force plot for the first test instance\")\nprint(\"\\nGenerating SHAP force plot for the first test instance...\")\ninstance = X_test.iloc[0]\ninstance_shap = explainer.shap_values(instance)\nshap.force_plot(explainer.expected_value, instance_shap, instance, matplotlib=True)\n\n\n\n\n\nSHAP Feature Importance (Bar Plot)\n\n\n\n\n\nB.12.2 SHAP Summary Plot: Understanding Feature Effects\nThe SHAP summary plot provides a powerful global view of the model’s behavior. It reveals not only the importance of each feature but also the direction and consistency of its effect on the GWP_Total prediction.\n\n\n\nSHAP Summary Plot\n\n\n\nB.12.2.1 Interpretation of Key Findings\nThis plot reveals several clear patterns learned by the model:\nThe Dominant Effect of Soil Type:\n\nSoil_SaLo (Sandy Loam) & Soil_LoSa (Loamy Sand): These are the two most important features. The plot shows a very clean separation: when these soils are present (red dots, feature value = 1), they consistently have a large negative SHAP value, meaning they strongly push the model to predict a lower GWP. Their absence (blue dots, feature value = 0) results in a positive SHAP value, increasing the predicted GWP.\n\nSoil_SiLo (Silty Loam): This feature shows the opposite effect. Its presence (red dots) consistently pushes the prediction higher (positive SHAP values).\n\nThe Non-Linear Impact of Nitrogen (BROIL_MINN):\n\nThis feature shows a complex, non-linear pattern. Both low values (blue dots) and high values (red dots) push the GWP prediction higher. The optimal values that push the prediction lower (negative SHAP values) are in the middle of the range (represented by the purple-ish dots). This confirms the “V” shape we saw in the Partial Dependence Plot, indicating an optimal “sweet spot” for this input.\n\nImpact of Management Practices:\n\nseed_rate_main & pest_app_LULC: For both of these features, there is a clear trend. High values (red dots) are on the right, pushing the prediction higher, while low values (blue dots) are on the left, pushing it lower. This indicates the model has learned a positive correlation: higher seeding and pesticide rates are associated with higher predicted GWP.\n\nThis single visualization confirms the model’s logic. It’s not just a black box; it has learned scientifically plausible relationships, such as the strong mitigating effect of certain soil types and the non-linear impact of nitrogen inputs.\n\n\n\n\nB.12.3 SHAP Force Plot: Explaining a Single Prediction\nThis force plot provides a detailed look at how the model arrived at its prediction for one specific data point in your dataset. It can be described as a “tug-of-war” between features that push the prediction higher and those that pull it lower.\nLets walk through the story of this specific prediction, from right to left:\n\n\n\nSHAP Force Plot\n\n\nThe Starting Point (base value): The model starts with the base value, which is the average prediction for GWP_Total across all data points in your training set. While the exact number isn’t represented, we can see it’s a positive value on the x-axis. This is the model’s initial “guess” before considering any of this specific data points features.\nThe “Pushing” Forces (Red): Next, the features in red push the prediction to the right (higher GWP). For this specific data point:\n\nSoil_SaLo = 0.0: The absence of the highly mitigating Sandy Loam soil is the single biggest factor pushing the GWP prediction up.\nRotation_AGC1 = 1.0: The presence of this specific crop rotation also contributes to a higher predicted GWP.\nP_APPkg_ha = 43.96: The value of this phosphorus input adds another push higher.\n\nThe “Pulling” Forces (Blue): These are the features that drag the prediction to the left (lower GWP). For this data point, their combined effect is much stronger:\n\nBROIL_MINN = 24.8: This nitrogen value is extremely close to the optimal spot of 25 identified earlier, so it has a very strong effect, pulling the GWP prediction down.\nseed_rate_main = 168.1, pest_app_LULC = 1.08, and BROIL_MINP = 9.92: The specific values for seed rate, pesticide application, and another manure input all contribute to lowering the predicted GWP for this instance.\n\nThe Final Prediction (f(x)): The final prediction of -1846.89 is the result of this tug-of-war. The combined downward pull of the blue features was so significant that it completely overwhelmed both the high starting base value and the upward push of the red features.\nIn summary, this plot tells a clear story: This specific field scenario was predicted to have a very low (even negative) GWP, despite lacking the beneficial Sandy Loam soil, primarily because its nitrogen input (BROIL_MINN) was at the perfect level to minimize emissions, with other management practices also contributing favorably.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-b-lca-rf.html#conclusions-from-shap-analysis",
    "href": "appendix-b-lca-rf.html#conclusions-from-shap-analysis",
    "title": "Appendix B — Preliminary LCA Cover crop Analysis",
    "section": "B.13 Conclusions from SHAP Analysis",
    "text": "B.13 Conclusions from SHAP Analysis\nCollectively, the SHAP analyses provide a multi-layered understanding of the Random Forest model’s behavior, moving from high-level feature importance to the specifics of individual predictions.\nThe analysis confirms that soil type (Soil_SaLo, Soil_LoSa) is the single most dominant factor driving GWP_Total predictions, with specific types having a strong mitigating effect. Beyond this, the plots reveal the complex, non-linear impact of key inputs. A prime example is the nitrogen-related feature BROIL_MINN, where the model identified an optimal “sweet spot” for minimizing GWP—an insight unavailable from simpler linear models.\nFurthermore, the force plot provides a clear case study of how these global trends interact at a local level, demonstrating a specific instance where optimal management inputs could overcome less-than-ideal soil conditions to produce a favorable outcome.\nThis deep interpretability, which clarifies both what the model prioritizes and why it makes a specific prediction, is crucial. It validates the model as more than just a predictor, but as a trustworthy tool for generating actionable scientific hypotheses about the drivers of GWP in agricultural systems.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Preliminary LCA Cover crop Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-c-hardware.html",
    "href": "appendix-c-hardware.html",
    "title": "Appendix C — Appendix C: Computational Hardware",
    "section": "",
    "text": "This appendix details the specialized computational hardware infrastructure that was personally invested in and utilized for the development and execution of the AI-Powered ETL & RAG pipeline, as well as for the subsequent data processing, machine learning model training, and computationally intensive inference tasks described in this project. Recognizing the limitations and potential bottlenecks associated with reliance on shared university cluster resources (often characterized by restricted access protocols and limited GPU allocations) or costly third-party API services for large-scale processing, this dedicated hardware setup was established to ensure robust performance, scalability, and economic sustainability for the project’s demanding computational needs.\n\n\n\n\n\n\nFigure C.1: Workstation\n\n\n\nThe primary system configuration includes:\nCentral Processing Units (CPUs): Threadripper Pro 5595wx\nGraphics Processing Units (GPUs): Multiple high-performance NVIDIA GPUs:\n- 2X Nvidia RTX Quadro A6000 Ampere\n- 2X Nvidia RTX 4090D (Sourced from China, modified for 48GB)\n- 1x RTX 3060 GPU for video output\nThis multi-GPU setup was critical for:\n\nAccelerating PDF content and layout extraction using GPU-enabled tools like Docling, and SmolDocling for formula extraction.\nTraining machine learning models, including the preliminary Random Forest models and the planned Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs) which are inherently parallelizable.\nRunning local Large Language Models (LLMs) for tasks such as the field_extraction.py module and the agentic_pipeline.py for Retrieval-Augmented Generation (deploying models like Qwen3-32B, Qwen/QwQ32B).\n\nRandom Access Memory (RAM): 512 GB of high-speed RAM. This large memory capacity was essential for:\n\nHandling large in-memory datasets during ETL processes, especially when processing and merging data from hundreds or thousands of PDF documents.\nSupporting large batch sizes during ML model training and LLM inference.\nManaging extensive knowledge bases, such as the ~10,000 synonyms across numerous dictionaries used by dictionaries_gui.py.\n\nMotherboard: An Asus Pro WS WRX80E-SAGE SE motherboard capable of supporting the multi-GPU configuration and high RAM capacity, ensuring stable and efficient system operation.\nStorage: 8Tb NVME SSD. Fast I/O is crucial for reading/writing large datasets and model checkpoints.\nThis dedicated hardware infrastructure directly enabled several key aspects of the project:\n\nEfficient Large-Scale PDF Processing:\n\n\nThe multi-GPU setup and ample RAM allowed for the rapid processing of large batches of PDF documents for text, table, and layout extraction.\n\n\nIntensive Machine Learning and Deep Learning:\n\n\nTraining complex models like Random Forests on extensive datasets, and particularly the planned development and training of CNNs and GNNs, benefit significantly from local multi-GPU capabilities and large RAM.\n\n\nLocal LLM Deployment and Inference:\n\n\nRunning powerful open-source LLMs locally for field extraction and the RAG pipeline provided greater control, reduced API costs, and allowed for experimentation with models not readily available through standard APIs.\n\n\nReduced Bottlenecks:\n\n\nOvercoming limitations associated with shared cluster queue times, data transfer speeds, and restricted computational allocations.\n\n\nIterative Development and Experimentation:\n\n\nFacilitated rapid prototyping and iteration on all components of the ETL+RAG pipeline and analytical models.\n\nBy ensuring dedicated and powerful computational resources, this hardware investment was instrumental in the successful execution of the project’s objectives and supports the planned future work involving more complex deep learning architectures and larger-scale data analyses.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Appendix C: Computational Hardware</span>"
    ]
  },
  {
    "objectID": "appendix-d-neo4j-queries.html",
    "href": "appendix-d-neo4j-queries.html",
    "title": "Appendix D — Knowledge Graph Queries and Supporting Data",
    "section": "",
    "text": "D.1 Initial Data Corpus Characteristics\nFollowing the initial literature search across Scopus, OpenAlex, and PubMed using a comprehensive set of [mention “over 30” or the exact number if you have it] search query combinations (see Supplementary Appendix A for full search strategy), a total of 1,260 unique records were identified. Full-text PDF articles were successfully retrieved for 439 of these records.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Knowledge Graph Queries and Supporting Data</span>"
    ]
  },
  {
    "objectID": "appendix-d-neo4j-queries.html#initial-data-corpus-characteristics",
    "href": "appendix-d-neo4j-queries.html#initial-data-corpus-characteristics",
    "title": "Appendix D — Knowledge Graph Queries and Supporting Data",
    "section": "",
    "text": "D.1.1 Source Distribution of Retrieved Articles\nThe 439 retrieved articles originated from the following databases:\n\n\nCode\nimport pandas as pd\nOUTP = \"/mnt/c/Users/WSTATION/Desktop/RAG_ETL_RUN/topic_modeled.feather\"\n\ndf_out=pd.read_feather(OUTP)\n\narticles_per_source = df_out['SourceDB'].value_counts()\nprint(articles_per_source)\n\n\n\n\nD.1.2 Overview of Extracted Study Types (from 439 Articles)\nThe initial analysis of the 439 retrieved articles identified a diverse range of study types. The table below summarizes the counts of articles associated with each study type label extracted and unified within our knowledge graph. Note that an article could be associated with multiple study type labels if applicable.\n╒══════════════════════════════════════╤════════════╕\n│StudyType                             │ArticleCount│\n╞══════════════════════════════════════╪════════════╡\n│\"air quality monitoring study\"        │62          │\n├──────────────────────────────────────┼────────────┤\n│\"observational study\"                 │27          │\n├──────────────────────────────────────┼────────────┤\n│\"review\"                              │26          │\n├──────────────────────────────────────┼────────────┤\n│\"time series analysis\"                │25          │\n├──────────────────────────────────────┼────────────┤\n│\"cohort study\"                        │23          │\n├──────────────────────────────────────┼────────────┤\n│\"systematic review\"                   │17          │\n├──────────────────────────────────────┼────────────┤\n│\"cross-sectional study\"               │15          │\n├──────────────────────────────────────┼────────────┤\n│\"case study\"                          │14          │\n├──────────────────────────────────────┼────────────┤\n│\"research article\"                    │11          │\n├──────────────────────────────────────┼────────────┤\n│\"environmental epidemiology study\"    │10          │\n├──────────────────────────────────────┼────────────┤\n│\"meta-analysis\"                       │9           │\n├──────────────────────────────────────┼────────────┤\n│\"case-control study\"                  │8           │\n├──────────────────────────────────────┼────────────┤\n│\"biodiversity assessment\"             │8           │\n├──────────────────────────────────────┼────────────┤\n│\"exposure assessment study\"           │7           │\n├──────────────────────────────────────┼────────────┤\n│\"remote sensing/GIS study\"            │6           │\n├──────────────────────────────────────┼────────────┤\n│\"spatial analysis\"                    │5           │\n├──────────────────────────────────────┼────────────┤\n│\"survey-based research\"               │5           │\n├──────────────────────────────────────┼────────────┤\n│\"health impact assessment\"            │5           │\n├──────────────────────────────────────┼────────────┤\n│\"simulation/modeling study\"           │4           │\n├──────────────────────────────────────┼────────────┤\n│\"expert opinion\"                      │3           │\n├──────────────────────────────────────┼────────────┤\n│\"scoping review\"                      │3           │\n├──────────────────────────────────────┼────────────┤\n│\"intervention study\"                  │2           │\n├──────────────────────────────────────┼────────────┤\n│\"lab-scale experiment\"                │2           │\n├──────────────────────────────────────┼────────────┤\n│\"randomized controlled trial\"         │2           │\n├──────────────────────────────────────┼────────────┤\n│\"policy analysis\"                     │2           │\n├──────────────────────────────────────┼────────────┤\n│\"quasi-experiment\"                    │2           │\n├──────────────────────────────────────┼────────────┤\n│\"primary research article\"            │2           │\n├──────────────────────────────────────┼────────────┤\n│\"field monitoring study\"              │2           │\n├──────────────────────────────────────┼────────────┤\n│\"dissertation/thesis\"                 │1           │\n├──────────────────────────────────────┼────────────┤\n│\"statistical analysis\"                │1           │\n├──────────────────────────────────────┼────────────┤\n│\"pilot-scale study\"                   │1           │\n├──────────────────────────────────────┼────────────┤\n│\"qualitative research study\"          │1           │\n├──────────────────────────────────────┼────────────┤\n│\"controlled environment chamber study\"│1           │\n├──────────────────────────────────────┼────────────┤\n│\"scenario analysis\"                   │1           │\n├──────────────────────────────────────┼────────────┤\n│\"comparative analysis\"                │1           │\n├──────────────────────────────────────┼────────────┤\n│\"methodological paper\"                │1           │\n├──────────────────────────────────────┼────────────┤\n│\"theoretical study\"                   │1           │\n├──────────────────────────────────────┼────────────┤\n│\"carbon footprint assessment\"         │1           │\n├──────────────────────────────────────┼────────────┤\n│\"risk assessment study\"               │1           │\n├──────────────────────────────────────┼────────────┤\n│\"short communication\"                 │1           │\n├──────────────────────────────────────┼────────────┤\n│\"climate modeling study\"              │1           │\n├──────────────────────────────────────┼────────────┤\n│\"report\"                              │1           │\n├──────────────────────────────────────┼────────────┤\n│\"socioeconomic impact assessment\"     │1           │\n└──────────────────────────────────────┴────────────┘\nTable D.1: Distribution of identified study types across the 439 retrieved articles.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Knowledge Graph Queries and Supporting Data</span>"
    ]
  },
  {
    "objectID": "appendix-d-neo4j-queries.html#queries-for-assessing-data-completeness",
    "href": "appendix-d-neo4j-queries.html#queries-for-assessing-data-completeness",
    "title": "Appendix D — Knowledge Graph Queries and Supporting Data",
    "section": "D.2 Queries for Assessing Data Completeness",
    "text": "D.2 Queries for Assessing Data Completeness\nTo understand the completeness of key data elements within our knowledge graph for the 439 articles, the following Cypher queries were executed.\n\nD.2.1 Articles missing StudyType:\nMATCH (a:Article)\nWHERE NOT EXISTS((a)-[:STUDY_TYPE]-&gt;(:StudyType))\nRETURN COUNT(DISTINCT a.doi) AS MissingStudyType\n\n╒════════════════╕\n│MissingStudyType│\n╞════════════════╡\n│145             │\n└────────────────┘\n\n\nD.2.2 Articles missing PollutantTerm:\nMATCH (a:Article)\nWHERE NOT EXISTS((a)-[:RELATED_TO_POLLUTANT]-&gt;(:PollutantTerm))\nRETURN COUNT(DISTINCT a.doi) AS MissingPollutant\n╒════════════════╕\n│MissingPollutant│\n╞════════════════╡\n│22              │\n└────────────────┘\n\n\nD.2.3 Articles missing HeartDisease:\nMATCH (a:Article)\nWHERE NOT EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nRETURN COUNT(DISTINCT a.doi) AS MissingHeartDisease\n╒═══════════════════╕\n│MissingHeartDisease│\n╞═══════════════════╡\n│187                │\n└───────────────────┘\n\n\nD.2.4 Articles missing MLMethod:\nMATCH (a:Article)\nWHERE NOT EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nRETURN COUNT(DISTINCT a.doi) AS MissingMLMethod\n╒═══════════════╕\n│MissingMLMethod│\n╞═══════════════╡\n│262            │\n└───────────────┘\n\n\nD.2.5 Combined Overview of Missing Links:\nMATCH (a:Article)\nRETURN \n  COUNT(a.doi) AS TotalArticles,\n  COUNT(CASE WHEN NOT EXISTS((a)-[:STUDY_TYPE]-&gt;(:StudyType)) THEN 1 END) AS MissingStudyType,\n  COUNT(CASE WHEN NOT EXISTS((a)-[:RELATED_TO_POLLUTANT]-&gt;(:PollutantTerm)) THEN 1 END) AS MissingPollutantTerm,\n  COUNT(CASE WHEN NOT EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease)) THEN 1 END) AS MissingHeartDisease,\n  COUNT(CASE WHEN NOT EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod)) THEN 1 END) AS MissingMLMethod\n╒═════════════╤════════════════╤════════════════════╤═══════════════════╤═══════════════╕\n│TotalArticles│MissingStudyType│MissingPollutantTerm│MissingHeartDisease│MissingMLMethod│\n╞═════════════╪════════════════╪════════════════════╪═══════════════════╪═══════════════╡\n│439          │145             │22                  │187                │262            │\n└─────────────┴────────────────┴────────────────────┴───────────────────┴───────────────┘\n\n\nD.2.6 Articles Missing Combinations of Entity Links\nThese queries identify articles missing specific combinations of links. (Presenting selected key queries for brevity).\nArticles missing all four key nodes (StudyType, PollutantTerm, HeartDisease, MLMethod):\nMATCH (a:Article)\nWHERE NOT EXISTS((a)-[:STUDY_TYPE]-&gt;(:StudyType))\n  AND NOT EXISTS((a)-[:RELATED_TO_POLLUTANT]-&gt;(:PollutantTerm))\n  AND NOT EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\n  AND NOT EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nRETURN COUNT(DISTINCT a.doi) AS MissingAllFourNodes\n╒═══════════════════╕\n│MissingAllFourNodes│\n╞═══════════════════╡\n│10                 │\n└───────────────────┘",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Knowledge Graph Queries and Supporting Data</span>"
    ]
  },
  {
    "objectID": "appendix-d-neo4j-queries.html#sec-appendix-d-neo4j-study-select",
    "href": "appendix-d-neo4j-queries.html#sec-appendix-d-neo4j-study-select",
    "title": "Appendix D — Knowledge Graph Queries and Supporting Data",
    "section": "D.3 Queries for Study Selection and Categorization (PRISMA Flow)",
    "text": "D.3 Queries for Study Selection and Categorization (PRISMA Flow)\nThe following queries were instrumental in the study selection process detailed in the PRISMA flow diagram. These queries implement the mutually exclusive categorization of articles.\n\nD.3.1 Identifying Overlapping Categories in Initial Analysis\nAn initial step to refine categorization logic involved identifying articles that could be ambiguously classified. The following query identified articles that possessed characteristics of both non-empirical and empirical studies (and lacked ML methods), highlighting the need for a prioritized classification scheme:\nMATCH (a:Article)\n// Condition for having at least one Non-Empirical Study Type\nWHERE EXISTS {\n  MATCH (a)-[:STUDY_TYPE]-&gt;(st_ne:StudyType)\n  WHERE toLower(st_ne.name) IN [\n    'review','systematic review','meta-analysis','expert opinion',\n    'scoping review','dissertation/thesis','short communication',\n    'methodological paper','theoretical study','report'\n  ]\n}\n// Condition for having at least one Empirical Study Type\nAND EXISTS {\n  MATCH (a)-[:STUDY_TYPE]-&gt;(st_e:StudyType)\n  WHERE NOT toLower(st_e.name) IN [\n    'review','systematic review','meta-analysis','expert opinion',\n    'scoping review','dissertation/thesis','short communication',\n    'methodological paper','theoretical study','report'\n  ]\n}\n// Condition for being part of EmpiricalWithoutML (Q2)\nAND NOT EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nRETURN COUNT(DISTINCT a.doi) AS OverlappingArticles_Q1_and_Q2\n╒═════════════════════════════╕\n│OverlappingArticles_Q1_and_Q2│\n╞═════════════════════════════╡\n│3                            │\n└─────────────────────────────┘\nThis result indicated that 3 articles were initially classifiable under both non-empirical and empirical (without ML) categories, prompting the development of the comprehensive categorization query below.\n\n\nD.3.2 Comprehensive Mutually Exclusive Categorization Query\nThis query assigns each of the 439 articles to a unique category, forming the basis for the PRISMA flow diagram exclusions. Non-empirical classifications are prioritized.\nMATCH (a:Article)\n\n// Collect all study type names for each article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st_all:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st_all.name)) AS studyTypeNames\n\n// Determine if article has any non-empirical or empirical study types based on collected names\nWITH a, studyTypeNames,\n     [sName IN studyTypeNames WHERE sName IN [\n       'review','systematic review','meta-analysis','expert opinion',\n       'scoping review','dissertation/thesis','short communication',\n       'methodological paper','theoretical study','report'\n     ]] AS nonEmpiricalSTList,\n     [sName IN studyTypeNames WHERE sName &lt;&gt; '' AND sName IS NOT NULL AND NOT sName IN [\n       'review','systematic review','meta-analysis','expert opinion',\n       'scoping review','dissertation/thesis','short communication',\n       'methodological paper','theoretical study','report'\n     ]] AS empiricalSTList\n\n// Gather other relevant existence checks\nWITH a, studyTypeNames, nonEmpiricalSTList, empiricalSTList,\n     EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod)) AS hasML,\n     EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease)) AS hasHD,\n     EXISTS { \n       MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(p:PollutantTerm) \n       WHERE toLower(p.name) CONTAINS 'ozone' \n     } AS hasOzone\n\n// Assign a single category to each article\nWITH a,\n     CASE\n       WHEN size(studyTypeNames) = 0 OR (size(studyTypeNames)=1 AND studyTypeNames[0] IS NULL) THEN 'A_MissingOrNullStudyType'\n       WHEN size(nonEmpiricalSTList) &gt; 0 THEN 'B_NonEmpirical' // Priority: if any ST is non-empirical\n       WHEN size(empiricalSTList) &gt; 0 THEN // Only if no non-empirical STs, then check for empirical\n         CASE\n           WHEN NOT hasML THEN 'C_EmpiricalWithoutML'\n           ELSE // Empirical AND HasML\n             CASE\n               WHEN hasHD AND hasOzone THEN 'D_EmpiricalML_Eligible_OzoneHD'\n               ELSE 'E_EmpiricalML_NotEligible_OzoneHD'\n             END\n         END\n       ELSE 'F_StudyType_Other_Or_Invalid' // Has study types, but not categorized (e.g., empty names not caught by null check)\n     END AS finalCategory\nRETURN finalCategory, COUNT(DISTINCT a.doi) AS count\nORDER BY finalCategory\nResults (used for PRISMA exclusions):\n╒═══════════════════════════════════╤═════╕\n│finalCategory                      │count│\n╞═══════════════════════════════════╪═════╡\n│\"A_MissingOrNullStudyType\"         │145  │\n├───────────────────────────────────┼─────┤\n│\"B_NonEmpirical\"                   │60   │\n├───────────────────────────────────┼─────┤\n│\"C_EmpiricalWithoutML\"             │159  │\n├───────────────────────────────────┼─────┤\n│\"D_EmpiricalML_Eligible_OzoneHD\"   │36   │\n├───────────────────────────────────┼─────┤\n│\"E_EmpiricalML_NotEligible_OzoneHD\"│39   │\n└───────────────────────────────────┴─────┘\nThese counts directly informed the exclusion numbers in the PRISMA diagram:\n\nExcluded due to missing/null study type (Category A): n=145\nExcluded as non-empirical (Category B): n=60\nExcluded as empirical without ML (Category C): n=159\nExcluded as empirical ML but not eligible (Category E): n=39\nExcluded as commentary or response to commentary articles: n=2\nIncluded studies (Category D): n=34",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Knowledge Graph Queries and Supporting Data</span>"
    ]
  },
  {
    "objectID": "appendix-d-neo4j-queries.html#sec-appendix-d-included-studies-char",
    "href": "appendix-d-neo4j-queries.html#sec-appendix-d-included-studies-char",
    "title": "Appendix D — Knowledge Graph Queries and Supporting Data",
    "section": "D.4 Characterization of Included Studies (n=34)",
    "text": "D.4 Characterization of Included Studies (n=34)\nFollowing the comprehensive categorization detailed in Section D.3, 36 articles were initially identified as potentially eligible (Category D_EmpiricalML_Eligible_OzoneHD). Upon closer inspection of these 36 articles, a further two were excluded as they were determined to be commentary or reply-to-commentary articles, rather than primary research. This resulted in a final set of 34 unique studies included in this systematic review.\nThe Cypher query used to extract detailed characteristics for these 34 studies (after applying title-based filters for comments/replies) is presented below. The full tabular output of this query, listing each of the 34 studies and their associated attributes, is available in the project’s data repository at data/export.csv.\n// Collect all study type names per article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames\n\n// Only include articles with at least one empirical StudyType (and no non-empirical types)\nWHERE SIZE(studyTypeNames) &gt; 0\n  AND NONE(stName IN studyTypeNames WHERE stName IN [\n    'review', 'systematic review', 'meta-analysis', 'expert opinion',\n    'scoping review', 'dissertation/thesis', 'short communication',\n    'methodological paper', 'theoretical study', 'report'\n  ])\n\n// Check for ML methods, heart disease, and ozone pollutant explicitly\nAND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nAND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nAND EXISTS {\n  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(p:PollutantTerm)\n  WHERE toLower(p.name) CONTAINS 'ozone'\n}\n\n// Exclude comment/reply-type articles explicitly\nAND NOT toLower(a.title) CONTAINS 'comment'\nAND NOT toLower(a.title) CONTAINS 'reply'\n\n// Retrieve related information\nOPTIONAL MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(pt:PollutantTerm)\nOPTIONAL MATCH (a)-[:USES_ML_METHOD]-&gt;(ml:MLMethod)\nOPTIONAL MATCH (a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(hd:HeartDisease)\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(stypes:StudyType)\nOPTIONAL MATCH (a)-[:USES_ANALYTICAL_TOOL]-&gt;(at:AnalyticalTool)\n\nWITH DISTINCT a.doi AS DOI,\n              a.title AS Title,\n     COLLECT(DISTINCT stypes.name) AS StudyTypes,\n     COLLECT(DISTINCT pt.name) AS Pollutants,\n     COLLECT(DISTINCT ml.name) AS MLMethods,\n     COLLECT(DISTINCT hd.name) AS HeartDiseases,\n     COLLECT(DISTINCT at.name) AS AnalyticalTools\n\nRETURN DOI,\n       Title,\n       StudyTypes,\n       Pollutants,\n       MLMethods,\n       HeartDiseases,\n       AnalyticalTools\nORDER BY DOI\n\n\nD.4.1 Distribution of Study Types\nThe 34 included empirical studies exhibited a range of specific study designs. The frequency of each study type is presented below. (Note: Articles may be associated with multiple study type labels if applicable from the source data, though the KG processing typically unifies these to canonical forms; the table reflects the primary unified types found across the 34 studies).\nMATCH (a:Article)\n\n// Collect all study type names per article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames\n\n// Only include articles with at least one empirical StudyType (and no non-empirical types)\nWHERE SIZE(studyTypeNames) &gt; 0\n  AND NONE(stName IN studyTypeNames WHERE stName IN [\n    'review', 'systematic review', 'meta-analysis', 'expert opinion',\n    'scoping review', 'dissertation/thesis', 'short communication',\n    'methodological paper', 'theoretical study', 'report'\n  ])\n\n// Check for ML methods, heart disease, and ozone pollutant explicitly\nAND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nAND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nAND EXISTS {\n  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(p:PollutantTerm)\n  WHERE toLower(p.name) CONTAINS 'ozone'\n}\n\n// Exclude comment/reply-type articles explicitly\nAND NOT toLower(a.title) CONTAINS 'comment'\nAND NOT toLower(a.title) CONTAINS 'reply'\n\n// Retrieve and unify study types\nMATCH (a)-[:STUDY_TYPE]-&gt;(stypes:StudyType)\nWITH DISTINCT a.doi AS DOI, toLower(stypes.name) AS StudyType\n\n// Count number of articles per StudyType\nRETURN \n    StudyType AS `Study Type`,\n    COUNT(DISTINCT DOI) AS `Number of Articles`\nORDER BY `Number of Articles` DESC\n\n╒══════════════════════════════╤══════════════════╕\n│Study Type                    │Number of Articles│\n╞══════════════════════════════╪══════════════════╡\n│\"air quality monitoring study\"│18                │\n├──────────────────────────────┼──────────────────┤\n│\"cohort study\"                │5                 │\n├──────────────────────────────┼──────────────────┤\n│\"case-control study\"          │2                 │\n├──────────────────────────────┼──────────────────┤\n│\"research article\"            │2                 │\n├──────────────────────────────┼──────────────────┤\n│\"observational study\"         │2                 │\n├──────────────────────────────┼──────────────────┤\n│\"case study\"                  │1                 │\n├──────────────────────────────┼──────────────────┤\n│\"pilot-scale study\"           │1                 │\n├──────────────────────────────┼──────────────────┤\n│\"qualitative research study\"  │1                 │\n├──────────────────────────────┼──────────────────┤\n│\"time series analysis\"        │1                 │\n├──────────────────────────────┼──────────────────┤\n│\"exposure assessment study\"   │1                 │\n└──────────────────────────────┴──────────────────┘\nTable D.X: Distribution of primary study types among the 34 included empirical articles.\nThe analysis of the 34 included studies reveals a primary focus on direct environmental assessment and established epidemiological methods. “Air quality monitoring study” is the most prevalent design, constituting over half of the included literature with 18 articles (52.9%). This highlights the foundational role of environmental measurement in studies linking ozone, health outcomes, and advanced analytical techniques.\nFollowing this, traditional epidemiological designs are represented, with “cohort study” being the second most common (5 articles, 14.7%). Other observational designs include “case-control study” and generic “observational study” (each with 2 articles, 5.9%). The presence of “time series analysis” (1 article, 2.9%) and “exposure assessment study” (1 article, 2.9%) further underscores the application of established methods in this research area.\nThe remainder of the included studies consists of a diverse set of designs, each represented by one or two articles, such as “research article” (2), “case study” (1), “pilot-scale study” (1), and “qualitative research study” (1). The sum of articles across these distinct study types equals the 34 unique studies included, indicating that each study was classified under one primary empirical design category for this analysis. This distribution reflects the variety of approaches eligible for inclusion that investigate the intersection of ozone, heart disease, and machine learning.\n\n\nD.4.2 Pollutants Investigated\nAll 34 included studies focused on ozone as a key pollutant, per the inclusion criteria. Many studies also investigated co-pollutants, reflecting the complex nature of ambient air pollution. The table below lists other pollutants mentioned alongside ozone in these studies, and their frequency across the 34 articles.\n\nMATCH (a:Article)\n\n// Collect study type names per article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames\n\n// Filter for empirical studies only (exclude non-empirical types)\nWHERE SIZE(studyTypeNames) &gt; 0\n  AND NONE(stName IN studyTypeNames WHERE stName IN [\n    'review', 'systematic review', 'meta-analysis', 'expert opinion',\n    'scoping review', 'dissertation/thesis', 'short communication',\n    'methodological paper', 'theoretical study', 'report'\n  ])\n\n// Confirm articles explicitly have ozone, ML methods, heart diseases\nAND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nAND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nAND EXISTS {\n  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(oz:PollutantTerm)\n  WHERE toLower(oz.name) CONTAINS 'ozone'\n}\n\n// Exclude comment/reply-type articles explicitly\nAND NOT toLower(a.title) CONTAINS 'comment'\nAND NOT toLower(a.title) CONTAINS 'reply'\n\n// Retrieve co-pollutants mentioned alongside ozone\nMATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(pt:PollutantTerm)\nWHERE toLower(pt.name) &lt;&gt; 'ozone' // Exclude ozone itself from co-pollutant list\n\n// Count distinct articles per co-pollutant\nRETURN \n    pt.name AS `Co-Pollutant`,\n    COUNT(DISTINCT a.doi) AS `Number of Articles`\nORDER BY `Number of Articles` DESC\n\n╒══════════════════════╤══════════════════╕\n│Co-Pollutant          │Number of Articles│\n╞══════════════════════╪══════════════════╡\n│\"nitrogen dioxide\"    │27                │\n├──────────────────────┼──────────────────┤\n│\"PM2.5\"               │25                │\n├──────────────────────┼──────────────────┤\n│\"PM10\"                │18                │\n├──────────────────────┼──────────────────┤\n│\"carbon monoxide\"     │15                │\n├──────────────────────┼──────────────────┤\n│\"sulfur dioxide\"      │15                │\n├──────────────────────┼──────────────────┤\n│\"particulate matter\"  │11                │\n├──────────────────────┼──────────────────┤\n│\"VOCs\"                │7                 │\n├──────────────────────┼──────────────────┤\n│\"nitrogen oxides\"     │6                 │\n├──────────────────────┼──────────────────┤\n│\"carbon tetrachloride\"│4                 │\n├──────────────────────┼──────────────────┤\n│\"carbon dioxide\"      │3                 │\n├──────────────────────┼──────────────────┤\n│\"ammonia\"             │2                 │\n├──────────────────────┼──────────────────┤\n│\"benzene\"             │2                 │\n├──────────────────────┼──────────────────┤\n│\"nitrates\"            │1                 │\n├──────────────────────┼──────────────────┤\n│\"nitrous oxide\"       │1                 │\n├──────────────────────┼──────────────────┤\n│\"sulfur oxides\"       │1                 │\n└──────────────────────┴──────────────────┘\nTable D.Y: Frequency of co-pollutants investigated alongside ozone in the 34 included empirical articles.\nWhile ozone was the primary pollutant of interest for inclusion in this review, the majority of the 34 studies also considered other ambient air pollutants. This reflects the common practice of assessing exposure to pollutant mixtures.\nThe most frequently investigated co-pollutants were nitrogen dioxide (NO₂), included in 27 (79.4%) of the studies, and fine particulate matter (PM₂.₅), addressed in 25 (73.5%) studies. Coarser particulate matter, PM₁₀, was also commonly studied alongside ozone in 18 (52.9%) articles. Other criteria pollutants, including carbon monoxide (CO) and sulfur dioxide (SO₂), were each featured in 15 (44.1%) of the included articles.\nMore general terms like “particulate matter” (11 articles, 32.4%) and “nitrogen oxides” (6 articles, 17.6%) were also noted. Volatile Organic Compounds (VOCs) were considered in 7 (20.6%) studies. A smaller number of articles investigated other specific compounds such as carbon tetrachloride, carbon dioxide, ammonia, and benzene. This pattern of co-pollutant investigation underscores the understanding that health effects attributed to ozone often occur in the context of exposure to a complex mix of airborne contaminants.\n\n\nD.4.3 Machine Learning Methods Employed\nA variety of machine learning methods were utilized in the included studies. The frequency of each identified ML method is detailed below. A single study could employ multiple ML methods, reflecting the multifaceted analytical approaches in this research domain.\nMATCH (a:Article)\n\n// Collect study type names per article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames\n\n// Filter for empirical studies only (exclude non-empirical types)\nWHERE SIZE(studyTypeNames) &gt; 0\n  AND NONE(stName IN studyTypeNames WHERE stName IN [\n    'review', 'systematic review', 'meta-analysis', 'expert opinion',\n    'scoping review', 'dissertation/thesis', 'short communication',\n    'methodological paper', 'theoretical study', 'report'\n  ])\n\n// Confirm articles explicitly have ozone, ML methods, and heart disease\nAND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nAND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nAND EXISTS {\n  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(oz:PollutantTerm)\n  WHERE toLower(oz.name) CONTAINS 'ozone'\n}\n\n// Exclude comment/reply-type articles explicitly\nAND NOT toLower(a.title) CONTAINS 'comment'\nAND NOT toLower(a.title) CONTAINS 'reply'\n\n// Retrieve ML methods and count their occurrences\nMATCH (a)-[:USES_ML_METHOD]-&gt;(ml:MLMethod)\nRETURN\n    ml.name AS `ML Method`,\n    COUNT(DISTINCT a.doi) AS `Number of Articles`\nORDER BY `Number of Articles` DESC\n\n╒════════════════════════════════════╤══════════════════╕\n│ML Method                           │Number of Articles│\n╞════════════════════════════════════╪══════════════════╡\n│\"Random Forest\"                     │20                │\n├────────────────────────────────────┼──────────────────┤\n│\"Gradient Boosting Machines\"        │14                │\n├────────────────────────────────────┼──────────────────┤\n│\"Support Vector Machines\"           │13                │\n├────────────────────────────────────┼──────────────────┤\n│\"Neural Networks\"                   │12                │\n├────────────────────────────────────┼──────────────────┤\n│\"K-Means Clustering\"                │9                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Recurrent Neural Networks\"         │7                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Deep Neural Networks\"              │7                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Convolutional Neural Networks\"     │5                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Geographically Weighted Regression\"│3                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Long Short-Term Memory Networks\"   │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Hierarchical Clustering\"           │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Markov Chain Monte Carlo\"          │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Autoencoders\"                      │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Prophet\"                           │2                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Principal Component Analysis\"      │1                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Extreme Learning Machines\"         │1                 │\n├────────────────────────────────────┼──────────────────┤\n│\"Named Entity Recognition\"          │1                 │\n└────────────────────────────────────┴──────────────────┘\nTable D.Z: Frequency of machine learning methods employed in the 34 included empirical articles.\nSummary of Machine Learning Methods:\nThe 34 included studies leveraged a diverse toolkit of 17 distinct machine learning methodologies to explore the associations between ozone, heart disease, and often geospatial factors (Table D.Z). Tree-based ensemble methods emerged as the most popular, with Random Forest being the predominant technique, utilized in 20 articles (58.8% of studies). Gradient Boosting Machines were also frequently applied, appearing in 14 studies (41.2%). Another commonly used method was Support Vector Machines, identified in 13 studies (38.2%).\nNeural network architectures showed considerable diversity and adoption. The general category “Neural Networks” was reported in 12 studies (35.3%), while more specific forms such as “Recurrent Neural Networks” and “Deep Neural Networks” were each used in 7 studies (20.6%), and “Convolutional Neural Networks” in 5 studies (14.7%). Unsupervised learning techniques were also present, most notably K-Means Clustering (9 articles, 26.5%).\nReflecting the geospatial aspect of some research questions, Geographically Weighted Regression, a spatially explicit modeling technique, was employed in 3 studies (8.8%). The data indicates a strong trend towards applying multiple ML techniques within a single study; collectively, the 34 articles reported 102 instances of ML method applications, averaging approximately three distinct methods per study. This highlights a common practice of either comparing models or using a combination of ML approaches for tasks such as exposure modeling, prediction, or pattern identification in the context of air pollution and health research.\n\n\nD.4.4 Heart Diseases Investigated\nThe included studies addressed various specific heart or cardiovascular diseases in relation to ozone exposure. The table below shows the distribution of these conditions among the 34 empirical articles that met all inclusion criteria.\nMATCH (a:Article)\n\n// Collect study type names per article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames\n\n// Filter for empirical studies only (exclude non-empirical types)\nWHERE SIZE(studyTypeNames) &gt; 0\n  AND NONE(stName IN studyTypeNames WHERE stName IN [\n    'review', 'systematic review', 'meta-analysis', 'expert opinion',\n    'scoping review', 'dissertation/thesis', 'short communication',\n    'methodological paper', 'theoretical study', 'report'\n  ])\n\n// Ensure explicit presence of ozone, ML methods, and heart disease associations\nAND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nAND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nAND EXISTS {\n  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(p:PollutantTerm)\n  WHERE toLower(p.name) CONTAINS 'ozone'\n}\n\n// Exclude comment/reply-type articles explicitly\nAND NOT toLower(a.title) CONTAINS 'comment'\nAND NOT toLower(a.title) CONTAINS 'reply'\n\n// Retrieve Heart Diseases and count occurrences\nMATCH (a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(hd:HeartDisease)\nRETURN\n    hd.name AS `Heart Disease Category`,\n    COUNT(DISTINCT a.doi) AS `Number of Articles`\nORDER BY `Number of Articles` DESC\n\n╒════════════════════════════╤══════════════════╕\n│Heart Disease Category      │Number of Articles│\n╞════════════════════════════╪══════════════════╡\n│\"coronary artery disease\"   │22                │\n├────────────────────────────┼──────────────────┤\n│\"hypertensive heart disease\"│5                 │\n├────────────────────────────┼──────────────────┤\n│\"cardiomyopathy\"            │5                 │\n├────────────────────────────┼──────────────────┤\n│\"myocardial infarction\"     │4                 │\n├────────────────────────────┼──────────────────┤\n│\"congestive heart failure\"  │4                 │\n├────────────────────────────┼──────────────────┤\n│\"arrhythmia\"                │1                 │\n└────────────────────────────┴──────────────────┘\nTable D.A: Distribution of specific heart diseases investigated in the 34 included articles.\nSummary of Heart Diseases Investigated:\nThe 34 empirical studies included in this review primarily focused on major, well-established cardiovascular conditions when examining the health impacts of ozone exposure (Table D.A). Coronary artery disease (CAD) was the most frequently investigated outcome, appearing in a substantial 22 out of 34 articles (64.7%). This highlights CAD as a central concern in research at the intersection of ozone, cardiovascular health, and machine learning applications.\nBeyond CAD, other significant cardiovascular outcomes were also addressed. Hypertensive heart disease and cardiomyopathy were each examined in 5 studies (14.7% each). Acute ischemic events, specifically myocardial infarction (heart attack), along with chronic conditions like congestive heart failure, were both investigated in 4 studies (11.8% each). A single study (2.9%) focused on arrhythmia.\nThe sum of these instances (41 mentions across 34 articles) indicates that some studies explored more than one specific cardiovascular endpoint. The pronounced focus on coronary artery disease, myocardial infarction, and heart failure aligns with the significant public health burden of these conditions and the recognized role of air pollution as a contributing environmental risk factor. The application of machine learning methods in these contexts likely aims to better understand, predict, or model these serious cardiovascular outcomes.\n\n\nD.4.5 Other Analytical Tools Utilized\nIn addition to machine learning models, some studies reported the use of other distinct analytical tools or techniques. The table below details these tools and their frequency among the 34 included empirical articles.\nMATCH (a:Article)\n\n// Collect study type names per article\nOPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)\nWITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames\n\n// Filter for empirical studies only (exclude non-empirical types)\nWHERE SIZE(studyTypeNames) &gt; 0\n  AND NONE(stName IN studyTypeNames WHERE stName IN [\n    'review', 'systematic review', 'meta-analysis', 'expert opinion',\n    'scoping review', 'dissertation/thesis', 'short communication',\n    'methodological paper', 'theoretical study', 'report'\n  ])\n\n// Explicit presence of ozone, ML methods, and heart disease\nAND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))\nAND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))\nAND EXISTS {\n  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(p:PollutantTerm)\n  WHERE toLower(p.name) CONTAINS 'ozone'\n}\n\n// Exclude commentary or reply articles explicitly\nAND NOT toLower(a.title) CONTAINS 'comment'\nAND NOT toLower(a.title) CONTAINS 'reply'\n\n// Retrieve Analytical Tools and their occurrence\nMATCH (a)-[:USES_ANALYTICAL_TOOL]-&gt;(at:AnalyticalTool)\nRETURN\n  at.name AS `Analytical Tool`,\n  COUNT(DISTINCT a.doi) AS `Number of Articles`\nORDER BY `Number of Articles` DESC\n╒═════════════════════════════════════════════════╤══════════════════╕\n│Analytical Tool                                  │Number of Articles│\n╞═════════════════════════════════════════════════╪══════════════════╡\n│\"Geographic Information Systems (GIS)\"           │6                 │\n├─────────────────────────────────────────────────┼──────────────────┤\n│\"Principal Component Analysis (PCA)\"             │4                 │\n├─────────────────────────────────────────────────┼──────────────────┤\n│\"Environmental Fate Modeling\"                    │2                 │\n├─────────────────────────────────────────────────┼──────────────────┤\n│\"Liquid Chromatography-Mass Spectrometry (LC-MS)\"│1                 │\n├─────────────────────────────────────────────────┼──────────────────┤\n│\"Chemometric Methods\"                            │1                 │\n└─────────────────────────────────────────────────┴──────────────────┘\nTable D.B: Frequency of other analytical tools utilized in the 34 included empirical articles.\nSummary of Other Analytical Tools Utilized:\nBeyond the primary machine learning algorithms, the 34 included studies reported the use of several other analytical tools and techniques, highlighting the interdisciplinary nature of advanced environmental health research (Table D.B).\nGeographic Information Systems (GIS) were the most frequently mentioned ancillary tool, utilized in 6 (17.6%) of the studies. This is consistent with the geospatial focus of many research questions in this domain, where GIS serves a critical role in managing, analyzing, and visualizing spatial data related to pollutant exposure, environmental factors, and health outcome distributions. The use of GIS often underpins or directly complements geospatial machine learning models (such as Geographically Weighted Regression, noted in Section D.4.3) and is frequently seen in “air quality monitoring studies” and “cohort studies” that assess spatially varying exposures.\nPrincipal Component Analysis (PCA) was identified in 4 studies (11.8%). PCA is a common statistical technique for dimensionality reduction and feature extraction. Its presence likely indicates its use as a pre-processing step for machine learning models, helping to manage complex datasets with numerous correlated environmental or demographic variables before predictive modeling.\nOther, more specialized tools included Environmental Fate Modeling (2 studies, 5.9%), which would be used to estimate pollutant concentrations and dispersion, thereby informing exposure assessment for subsequent health impact analysis with ML. Techniques related to detailed chemical analysis, such as Liquid Chromatography-Mass Spectrometry (LC-MS) (1 study, 2.9%) and broader Chemometric Methods (1 study, 2.9%), suggest that a subset of studies incorporated fine-grained compositional or biomarker analysis, potentially linking these detailed measurements to health outcomes via machine learning.\nOverall, while the core focus was on machine learning applications, the co-occurrence of tools like GIS and PCA underscores their supportive and often integral role in the broader analytical workflow of these sophisticated environmental epidemiology studies.\n\n\n\nD.4.6 Cypher Queries Cheat Sheet\nTo get a list of metrics from every DOI:\nMATCH (a:Article)-[r:REPORTS_METRIC]-&gt;(m:Metric)\nRETURN  a.doi      AS DOI,\n        r.MetricID AS metricid,\n        m.name     AS metric,\n        r.value    AS value,\n        r.unit     AS unit\nORDER BY DOI, metricid;\nto get metrics associated with any given study type (for example cohort study)\n/* Cohort-study articles and any metrics they report */\nMATCH (a:Article)--&gt;(:StudyType {name: \"cohort study\"})\nOPTIONAL MATCH (a)-[r:REPORTS_METRIC]-&gt;(m:Metric)\n\nRETURN\n  \"cohort study\"                               AS studyType,\n  a.doi                                        AS DOI,\n  coalesce(r.MetricID , \"NO METRICS\")          AS metricid,\n  coalesce(m.name     , \"NO METRICS\")          AS metric,\n  coalesce(r.value    , \"NO METRICS\")          AS value,\n  coalesce(r.unit     , \"NO METRICS\")          AS unit\nORDER BY DOI, metricid;\n\nor if you are interested in life cycle assessment:\nMATCH (a:Article)--&gt;(:StudyType {name: \"life cycle assessment\"})\nOPTIONAL MATCH (a)-[r:REPORTS_METRIC]-&gt;(m:Metric)\n\nRETURN\n  \"life cycle assessment\"                               AS studyType,\n  a.doi                                        AS DOI,\n  coalesce(r.MetricID , \"NO METRICS\")          AS metricid,\n  coalesce(m.name     , \"NO METRICS\")          AS metric,\n  coalesce(r.value    , \"NO METRICS\")          AS value,\n  coalesce(r.unit     , \"NO METRICS\")          AS unit\nORDER BY DOI, metricid;\nto get a list and count of all nodes:\nMATCH (n)\nUNWIND labels(n) AS nodeLabel\nRETURN nodeLabel, COUNT(*) AS nodeCount\nORDER BY nodeCount DESC;\nto get a list and count of all relationships:\nMATCH ()-[r]-&gt;()\nRETURN TYPE(r) AS relType, COUNT(*) AS relCount\nORDER BY relCount DESC;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Knowledge Graph Queries and Supporting Data</span>"
    ]
  },
  {
    "objectID": "appendix-e-db-queries.html",
    "href": "appendix-e-db-queries.html",
    "title": "Appendix E — Database Queries",
    "section": "",
    "text": "These were the queries which were introduced into the ETL system to search the three databases (OpenAlex, PubMed, Scopus)\n“ozone exposure” “heart disease” “geospatial epidemiology” “spatiotemporal analysis” “ozone” “cardiovascular disease” “geographic information systems” “GIS” “ozone exposure” “heart disease” “geospatial clustering” “ozone exposure” “health outcomes” “hotspot analysis” “ozone” “stroke mortality” “spatial autocorrelation” “ozone levels” “cardiovascular mortality” “modifiable areal unit problem” “ozone exposure” “epidemiology” “spatial lag models” “ozone” “heart disease” “geographically weighted regression” “ozone exposure” “cardiovascular” “random forest” “geospatial” “ozone” “cardiovascular” “gradient boosting” “ozone exposure” “spatial health analysis” “XGBoost” “ozone concentration” “heart disease prediction” “machine learning” “spatiotemporal prediction” “ozone” “cardiovascular” “support vector machine” “SVM” “ozone exposure” “health outcomes” “neural networks” “ozone” “heart disease” “geospatial” “CNN” “satellite imagery” “ozone” “health risk” “LSTM” “temporal ozone exposure” “cardiovascular” “graph neural networks” “ozone exposure” “health networks” “time-series analysis” “ozone levels” “cardiovascular mortality” “distributed lag model” “ozone exposure” “health effects” “ozone exposure lag effects” “heart disease” “temporal clustering” “ozone exposure” “cardiovascular events” “attention mechanisms” “geospatial” “ozone exposure” “health” “transformer models” “spatiotemporal” “ozone exposure” “vision transformer” “ViT” “ozone exposure” “satellite” “geospatial cross-validation” “ozone” “health prediction” “air quality regulations” “ozone” “public health outcomes” “environmental justice” “ozone exposure” “heart disease” “urban vs rural ozone exposure” “cardiovascular” “demographic disparities” “ozone exposure” “health impacts” “ozone exposure” “cardiovascular health” “methodological gaps” “geospatial” “ozone exposure” “underexplored methods” “machine learning” “ozone health effects” “research gaps”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Database Queries</span>"
    ]
  }
]