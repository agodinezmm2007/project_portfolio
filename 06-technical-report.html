<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Technical Report – Portfolio</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./05-aim3.html" rel="prev">
<link href="./pic/cover.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./06-technical-report.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Technical Report</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Portfolio</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-proposal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-data-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Collection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-literature-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Literature Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-prelim-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Preliminary Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-aim3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-technical-report.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Technical Report</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./update.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Update</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-a-etl-rag.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">appendix-a-etl-rag.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-b-lca-rf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Preliminary LCA Cover crop Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-c-hardware.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Appendix C: Computational Hardware</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-d-neo4j-queries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Knowledge Graph Queries and Supporting Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-e-db-queries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Database Queries</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-etl-pipeline-from-raw-literature-to-structured-knowledge" id="toc-the-etl-pipeline-from-raw-literature-to-structured-knowledge" class="nav-link active" data-scroll-target="#the-etl-pipeline-from-raw-literature-to-structured-knowledge"><span class="header-section-number">7.1</span> The ETL Pipeline: From Raw Literature to Structured Knowledge</a>
  <ul>
  <li><a href="#corpus-size-and-variability" id="toc-corpus-size-and-variability" class="nav-link" data-scroll-target="#corpus-size-and-variability"><span class="header-section-number">7.1.1</span> Corpus Size and Variability</a></li>
  <li><a href="#stage-1-multi-source-data-acquisition-and-enrichment" id="toc-stage-1-multi-source-data-acquisition-and-enrichment" class="nav-link" data-scroll-target="#stage-1-multi-source-data-acquisition-and-enrichment"><span class="header-section-number">7.1.2</span> Stage 1: Multi-Source Data Acquisition and Enrichment</a>
  <ul class="collapse">
  <li><a href="#parallelized-metadata-fetching" id="toc-parallelized-metadata-fetching" class="nav-link" data-scroll-target="#parallelized-metadata-fetching"><span class="header-section-number">7.1.2.1</span> Parallelized Metadata Fetching:</a></li>
  <li><a href="#data-fusion-and-deduplication" id="toc-data-fusion-and-deduplication" class="nav-link" data-scroll-target="#data-fusion-and-deduplication"><span class="header-section-number">7.1.2.2</span> Data Fusion and Deduplication:</a></li>
  <li><a href="#metadata-enrichment" id="toc-metadata-enrichment" class="nav-link" data-scroll-target="#metadata-enrichment"><span class="header-section-number">7.1.2.3</span> Metadata Enrichment:</a></li>
  </ul></li>
  <li><a href="#stage-2-asynchronous-full-text-pdf-retrieval" id="toc-stage-2-asynchronous-full-text-pdf-retrieval" class="nav-link" data-scroll-target="#stage-2-asynchronous-full-text-pdf-retrieval"><span class="header-section-number">7.1.3</span> Stage 2: Asynchronous Full-Text PDF Retrieval</a></li>
  <li><a href="#stage-3-bibliographic-management-and-citation-rendering" id="toc-stage-3-bibliographic-management-and-citation-rendering" class="nav-link" data-scroll-target="#stage-3-bibliographic-management-and-citation-rendering"><span class="header-section-number">7.1.4</span> Stage 3: Bibliographic Management and Citation Rendering</a></li>
  <li><a href="#stage-4-content-extraction-via-document-layout-analysis" id="toc-stage-4-content-extraction-via-document-layout-analysis" class="nav-link" data-scroll-target="#stage-4-content-extraction-via-document-layout-analysis"><span class="header-section-number">7.1.5</span> Stage 4: Content Extraction via Document Layout Analysis</a>
  <ul class="collapse">
  <li><a href="#core-technology-and-identified-limitations" id="toc-core-technology-and-identified-limitations" class="nav-link" data-scroll-target="#core-technology-and-identified-limitations"><span class="header-section-number">7.1.5.1</span> Core Technology and Identified Limitations</a></li>
  <li><a href="#custom-enhancements-and-implementation" id="toc-custom-enhancements-and-implementation" class="nav-link" data-scroll-target="#custom-enhancements-and-implementation"><span class="header-section-number">7.1.5.2</span> Custom Enhancements and Implementation</a></li>
  <li><a href="#initial-challenge-inaccurate-bounding-boxes-and-model-inefficiency" id="toc-initial-challenge-inaccurate-bounding-boxes-and-model-inefficiency" class="nav-link" data-scroll-target="#initial-challenge-inaccurate-bounding-boxes-and-model-inefficiency"><span class="header-section-number">7.1.5.3</span> Initial Challenge: Inaccurate Bounding Boxes and Model Inefficiency</a></li>
  <li><a href="#bounding-box-and-image-preprocessing-modifications" id="toc-bounding-box-and-image-preprocessing-modifications" class="nav-link" data-scroll-target="#bounding-box-and-image-preprocessing-modifications"><span class="header-section-number">7.1.5.4</span> Bounding Box and Image Preprocessing Modifications</a></li>
  <li><a href="#implementation-of-selective-content-masking" id="toc-implementation-of-selective-content-masking" class="nav-link" data-scroll-target="#implementation-of-selective-content-masking"><span class="header-section-number">7.1.5.5</span> Implementation of Selective Content Masking</a></li>
  <li><a href="#heuristic-based-merging-of-fragmented-layout-clusters" id="toc-heuristic-based-merging-of-fragmented-layout-clusters" class="nav-link" data-scroll-target="#heuristic-based-merging-of-fragmented-layout-clusters"><span class="header-section-number">7.1.5.6</span> Heuristic-Based Merging of Fragmented Layout Clusters</a></li>
  <li><a href="#misclassification-of-page-layouts" id="toc-misclassification-of-page-layouts" class="nav-link" data-scroll-target="#misclassification-of-page-layouts"><span class="header-section-number">7.1.5.7</span> Misclassification of Page Layouts</a></li>
  <li><a href="#heuristic-based-re-classification-of-misidentified-clusters" id="toc-heuristic-based-re-classification-of-misidentified-clusters" class="nav-link" data-scroll-target="#heuristic-based-re-classification-of-misidentified-clusters"><span class="header-section-number">7.1.5.8</span> Heuristic-Based Re-classification of Misidentified Clusters</a></li>
  <li><a href="#pre-emptive-filtering-of-page-level-artifacts" id="toc-pre-emptive-filtering-of-page-level-artifacts" class="nav-link" data-scroll-target="#pre-emptive-filtering-of-page-level-artifacts"><span class="header-section-number">7.1.5.9</span> Pre-emptive Filtering of Page-Level Artifacts</a></li>
  <li><a href="#refinements-to-the-underlying-layout-predictor" id="toc-refinements-to-the-underlying-layout-predictor" class="nav-link" data-scroll-target="#refinements-to-the-underlying-layout-predictor"><span class="header-section-number">7.1.5.10</span> Refinements to the Underlying Layout Predictor</a></li>
  <li><a href="#final-implementation-in-the-etl-pipeline" id="toc-final-implementation-in-the-etl-pipeline" class="nav-link" data-scroll-target="#final-implementation-in-the-etl-pipeline"><span class="header-section-number">7.1.5.11</span> Final Implementation in the ETL Pipeline</a></li>
  </ul></li>
  <li><a href="#stage-5-structured-field-extraction-using-large-language-models" id="toc-stage-5-structured-field-extraction-using-large-language-models" class="nav-link" data-scroll-target="#stage-5-structured-field-extraction-using-large-language-models"><span class="header-section-number">7.1.6</span> Stage 5: Structured Field Extraction using Large Language Models</a>
  <ul class="collapse">
  <li><a href="#methodology-iterative-context-aware-extraction-with-local-llms" id="toc-methodology-iterative-context-aware-extraction-with-local-llms" class="nav-link" data-scroll-target="#methodology-iterative-context-aware-extraction-with-local-llms"><span class="header-section-number">7.1.6.1</span> Methodology: Iterative, Context-Aware Extraction with Local LLMs</a></li>
  <li><a href="#parallelized-execution-and-data-merging" id="toc-parallelized-execution-and-data-merging" class="nav-link" data-scroll-target="#parallelized-execution-and-data-merging"><span class="header-section-number">7.1.6.2</span> Parallelized Execution and Data Merging</a></li>
  </ul></li>
  <li><a href="#stage-6-thematic-analysis-via-topic-modeling" id="toc-stage-6-thematic-analysis-via-topic-modeling" class="nav-link" data-scroll-target="#stage-6-thematic-analysis-via-topic-modeling"><span class="header-section-number">7.1.7</span> Stage 6: Thematic Analysis via Topic Modeling</a>
  <ul class="collapse">
  <li><a href="#methodology-probabilistic-topic-modeling-with-lda" id="toc-methodology-probabilistic-topic-modeling-with-lda" class="nav-link" data-scroll-target="#methodology-probabilistic-topic-modeling-with-lda"><span class="header-section-number">7.1.7.1</span> Methodology: Probabilistic Topic Modeling with LDA</a></li>
  <li><a href="#data-preprocessing-and-hyperparameter-optimization" id="toc-data-preprocessing-and-hyperparameter-optimization" class="nav-link" data-scroll-target="#data-preprocessing-and-hyperparameter-optimization"><span class="header-section-number">7.1.7.2</span> Data Preprocessing and Hyperparameter Optimization</a></li>
  <li><a href="#model-evaluation-topic-coherence" id="toc-model-evaluation-topic-coherence" class="nav-link" data-scroll-target="#model-evaluation-topic-coherence"><span class="header-section-number">7.1.7.3</span> Model Evaluation: Topic Coherence</a></li>
  <li><a href="#interpretation-and-visualization" id="toc-interpretation-and-visualization" class="nav-link" data-scroll-target="#interpretation-and-visualization"><span class="header-section-number">7.1.7.4</span> Interpretation and Visualization</a></li>
  </ul></li>
  <li><a href="#stage-7-the-semantic-unification-pipeline" id="toc-stage-7-the-semantic-unification-pipeline" class="nav-link" data-scroll-target="#stage-7-the-semantic-unification-pipeline"><span class="header-section-number">7.1.8</span> Stage 7: The Semantic Unification Pipeline</a>
  <ul class="collapse">
  <li><a href="#the-challenge-of-terminological-heterogeneity-in-automated-extraction" id="toc-the-challenge-of-terminological-heterogeneity-in-automated-extraction" class="nav-link" data-scroll-target="#the-challenge-of-terminological-heterogeneity-in-automated-extraction"><span class="header-section-number">7.1.8.1</span> The Challenge of Terminological Heterogeneity in Automated Extraction</a></li>
  <li><a href="#a-vector-space-model-for-semantic-representation" id="toc-a-vector-space-model-for-semantic-representation" class="nav-link" data-scroll-target="#a-vector-space-model-for-semantic-representation"><span class="header-section-number">7.1.8.2</span> A Vector Space Model for Semantic Representation</a></li>
  <li><a href="#pre-computation-of-the-canonical-knowledge-base" id="toc-pre-computation-of-the-canonical-knowledge-base" class="nav-link" data-scroll-target="#pre-computation-of-the-canonical-knowledge-base"><span class="header-section-number">7.1.8.3</span> Pre-computation of the Canonical Knowledge Base</a></li>
  <li><a href="#quantifying-semantic-similarity-the-cosine-similarity-metric" id="toc-quantifying-semantic-similarity-the-cosine-similarity-metric" class="nav-link" data-scroll-target="#quantifying-semantic-similarity-the-cosine-similarity-metric"><span class="header-section-number">7.1.8.4</span> Quantifying Semantic Similarity: The Cosine Similarity Metric</a></li>
  <li><a href="#threshold-based-mapping-for-ontological-consistency" id="toc-threshold-based-mapping-for-ontological-consistency" class="nav-link" data-scroll-target="#threshold-based-mapping-for-ontological-consistency"><span class="header-section-number">7.1.8.5</span> Threshold-Based Mapping for Ontological Consistency</a></li>
  <li><a href="#knowledge-graph-integration" id="toc-knowledge-graph-integration" class="nav-link" data-scroll-target="#knowledge-graph-integration"><span class="header-section-number">7.1.8.6</span> Knowledge Graph Integration</a></li>
  <li><a href="#dynamic-graph-construction" id="toc-dynamic-graph-construction" class="nav-link" data-scroll-target="#dynamic-graph-construction"><span class="header-section-number">7.1.8.7</span> Dynamic Graph Construction</a></li>
  <li><a href="#querying-for-methodological-insights" id="toc-querying-for-methodological-insights" class="nav-link" data-scroll-target="#querying-for-methodological-insights"><span class="header-section-number">7.1.8.8</span> Querying for Methodological Insights</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#etl-pipeline-summary-and-output" id="toc-etl-pipeline-summary-and-output" class="nav-link" data-scroll-target="#etl-pipeline-summary-and-output"><span class="header-section-number">7.2</span> ETL Pipeline Summary and Output</a></li>
  <li><a href="#hysemrag-qa-framework-an-agentic-approach-for-verifiable-generation" id="toc-hysemrag-qa-framework-an-agentic-approach-for-verifiable-generation" class="nav-link" data-scroll-target="#hysemrag-qa-framework-an-agentic-approach-for-verifiable-generation"><span class="header-section-number">8</span> HySemRAG-QA Framework: An Agentic Approach for Verifiable Generation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Technical Report</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>#06-technical-report.qmd</p>
<section id="the-etl-pipeline-from-raw-literature-to-structured-knowledge" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="the-etl-pipeline-from-raw-literature-to-structured-knowledge"><span class="header-section-number">7.1</span> The ETL Pipeline: From Raw Literature to Structured Knowledge</h2>
<p>This chapter will server as the technical report for the ETL+HySemRAG methodology and is a work in progress. This chapter will be updated once the draft is complete to describe all phases and theories of operation of the system.</p>
<div id="fig-graph-abstract" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graph-abstract-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/ETL_RAG.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graph-abstract-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1: Graphical Abstract
</figcaption>
</figure>
</div>
<section id="corpus-size-and-variability" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="corpus-size-and-variability"><span class="header-section-number">7.1.1</span> Corpus Size and Variability</h3>
<p>The final corpus size in each ETL run is not fixed, rather emerges naturally based on the availability of open-access full-text PDFs via the Unpaywall API. Typically, from an initial candidate pool of approximately 3,400 metadata records retrieved from PubMed, OpenAlex, and Scopus, the number of successfully retrieved full-text documents ranges from around 500 to 1,300. This inherent variability reflects realistic constraints encountered during automated scholarly literature retrieval, specifically the reliance on open-access publications. Importantly, this variability underscores the pipeline’s robustness and adaptability, as subsequent processing stages maintain stable performance metrics (accuracy of data extraction, knowledge graph quality, semantic consistency) regardless of the exact corpus size. This adaptability positions the ETL pipeline as particularly suited to real-world research scenarios where comprehensive subscription-based access may not always be available.</p>
<p><strong>Demonstration Video</strong></p>
<p>Here is a video demonstrating the system’s capabilities:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ZCy5ESJ1gVE?si=K8CttwgTj7yGrWjn" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
<section id="stage-1-multi-source-data-acquisition-and-enrichment" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="stage-1-multi-source-data-acquisition-and-enrichment"><span class="header-section-number">7.1.2</span> Stage 1: Multi-Source Data Acquisition and Enrichment</h3>
<p>The initial and most foundational stage of the ETL pipeline is the acquisition of a comprehensive, relevant, and clean corpus of scholarly literature. The objective of this phase is to systematically gather article metadata from multiple disparate sources, each with unique strengths, and then intelligently fuse them into a single, deduplicated, and enriched dataset. This process is orchestrated by a series of high-performance Python modules designed for scalability and robustness, ensuring that the subsequent stages of the pipeline operate on a high-quality foundation.</p>
<section id="parallelized-metadata-fetching" class="level4" data-number="7.1.2.1">
<h4 data-number="7.1.2.1" class="anchored" data-anchor-id="parallelized-metadata-fetching"><span class="header-section-number">7.1.2.1</span> Parallelized Metadata Fetching:</h4>
<p>To achieve a comprehensive corpus, the system employs a multi-source fetching strategy, simultaneously querying three major scholarly databases: PubMed, for its depth in biomedical and life sciences literature; OpenAlex, for its broad multidisciplinary coverage and rich citation network data; and Elsevier’s Scopus API, for its extensive abstract and citation database, particularly for STEM fields. The GUI allows the user to select any combination of these sources for a given run.</p>
<p>Recognizing that fetching metadata for thousands of articles via sequential API calls would present a significant performance bottleneck, the system was architected for high-throughput, concurrent data retrieval. The implementation is tailored to the specific architecture and rate-limiting policies of each API. For PubMed, the scripts/fast_pubmed.py module first performs a single, efficient esearch call to retrieve up to 10,000 PubMed IDs (PMIDs) matching a given query. Subsequently, it uses a ThreadPoolExecutor to create a pool of worker threads that fetch the full article metadata in parallel batches using efetch, while adhering to NCBI’s polite API usage policy of approximately 3 queries per second. For OpenAlex and Scopus, the system leverages modern asynchronous I/O via the asyncio and aiohttp libraries within the scripts/fast_openalex.py and scripts/etl_elsevier.py modules. This approach allows the system to initiate hundreds of concurrent HTTP requests, managing them efficiently without being blocked by network latency. An asyncio.Semaphore is used as a throttling mechanism to ensure the number of requests per second does not exceed the official rate limits for each API, preventing errors and ensuring reliable access.</p>
</section>
<section id="data-fusion-and-deduplication" class="level4" data-number="7.1.2.2">
<h4 data-number="7.1.2.2" class="anchored" data-anchor-id="data-fusion-and-deduplication"><span class="header-section-number">7.1.2.2</span> Data Fusion and Deduplication:</h4>
<p>Once the parallel fetching processes are complete, the metadata from each source, now held in separate pandas DataFrames, must be combined into a single, unified dataset. This is handled by the merge_and_deduplicate function within the scripts/etl_utils_refined.py module. The function first concatenates the list of DataFrames into one large, heterogeneous dataset.</p>
<div id="merge-deduplicate" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_and_deduplicate(from_pubmed_all: List[pd.DataFrame], </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                          from_openalex_all: List[pd.DataFrame], </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                          from_elsevier_all: Optional[List[pd.DataFrame]] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    all_dfs <span class="op">=</span> []</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> from_pubmed_all: all_dfs.extend(from_pubmed_all)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> from_openalex_all: all_dfs.extend(from_openalex_all)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> from_elsevier_all: all_dfs.extend(from_elsevier_all)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> all_dfs:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        logging.warning(<span class="st">"No DataFrames provided to merge_and_deduplicate."</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.DataFrame()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    combined_df <span class="op">=</span> pd.concat(all_dfs, ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> combined_df.empty:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        logging.info(<span class="st">"Combined DataFrame is empty before deduplication."</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> combined_df</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    deduplicated_df <span class="op">=</span> deduplicate_by_doi_title(combined_df)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> deduplicated_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The subsequent critical step is deduplication, as the same article may be retrieved from multiple databases. A simple row-based drop would be insufficient. Therefore, the system implements a robust, hierarchical deduplication algorithm. First, it normalizes all Digital Object Identifiers (DOIs) by converting them to a consistent lowercase format and removing whitespace. It then identifies and retains the first occurrence of each unique DOI.</p>
<div id="deduplicate-doi" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deduplicate_by_doi_title(df: pd.DataFrame) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create normalized columns</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"DOI"</span> <span class="kw">not</span> <span class="kw">in</span> df.columns: df[<span class="st">"DOI"</span>] <span class="op">=</span> <span class="st">""</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"Title"</span> <span class="kw">not</span> <span class="kw">in</span> df.columns: df[<span class="st">"Title"</span>] <span class="op">=</span> <span class="st">""</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"DOI"</span>] <span class="op">=</span> df[<span class="st">"DOI"</span>].fillna(<span class="st">""</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"Title"</span>] <span class="op">=</span> df[<span class="st">"Title"</span>].fillna(<span class="st">""</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"DOI_clean"</span>] <span class="op">=</span> df[<span class="st">"DOI"</span>].astype(<span class="bu">str</span>).<span class="bu">str</span>.lower().<span class="bu">str</span>.strip()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"Title_clean"</span>] <span class="op">=</span> df[<span class="st">"Title"</span>].astype(<span class="bu">str</span>).<span class="bu">str</span>.lower().<span class="bu">str</span>.strip()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    df.sort_values(by<span class="op">=</span>[<span class="st">"DOI_clean"</span>, <span class="st">"Title_clean"</span>], inplace<span class="op">=</span><span class="va">True</span>, kind<span class="op">=</span><span class="st">'stable'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    indices_to_keep <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    seen_dois <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index, doi <span class="kw">in</span> df.loc[df[<span class="st">"DOI_clean"</span>] <span class="op">!=</span> <span class="st">""</span>, <span class="st">"DOI_clean"</span>].items():</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> doi <span class="kw">not</span> <span class="kw">in</span> seen_dois:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            indices_to_keep.add(index)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            seen_dois.add(doi)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    empty_doi_mask <span class="op">=</span> (df[<span class="st">"DOI_clean"</span>] <span class="op">==</span> <span class="st">""</span>) <span class="op">&amp;</span> (<span class="op">~</span>df.index.isin(indices_to_keep))</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    seen_titles_empty_doi <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index, title <span class="kw">in</span> df.loc[empty_doi_mask, <span class="st">"Title_clean"</span>].items():</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> title <span class="kw">not</span> <span class="kw">in</span> seen_titles_empty_doi:</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            indices_to_keep.add(index)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            seen_titles_empty_doi.add(title)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    df_deduplicated <span class="op">=</span> df.loc[<span class="bu">list</span>(indices_to_keep)].copy()</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    df_deduplicated.drop(columns<span class="op">=</span>[<span class="st">"DOI_clean"</span>, <span class="st">"Title_clean"</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    df_deduplicated.reset_index(drop<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df_deduplicated</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For the remaining records that may lack a DOI, the algorithm performs a secondary deduplication based on a similarly normalized article title. This two-pass strategy ensures that each unique scholarly work is represented by a single, canonical record in the final dataset, preventing data redundancy in all downstream analyses.</p>
</section>
<section id="metadata-enrichment" class="level4" data-number="7.1.2.3">
<h4 data-number="7.1.2.3" class="anchored" data-anchor-id="metadata-enrichment"><span class="header-section-number">7.1.2.3</span> Metadata Enrichment:</h4>
<p>The initial, merged dataset, while comprehensive, may contain records with incomplete information (e.g., a record from PubMed might be missing a PDF link that OpenAlex has). The final step in the acquisition phase is therefore a dedicated enrichment process, orchestrated by scripts/metadata_enrichment.py. This module takes the entire list of unique DOIs from the deduplicated dataset and once again uses an asynchronous approach to concurrently query both the OpenAlex and Crossref APIs for every article.</p>
<div id="enrich-metadata" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> enrich_metadata_sync(df: pd.DataFrame, crossref_email: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    email <span class="op">=</span> crossref_email <span class="kw">or</span> MAILTO</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    dois <span class="op">=</span> df[<span class="st">"DOI"</span>].dropna().unique().tolist()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    enrichment <span class="op">=</span> asyncio.run(enrich_metadata(dois, email))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, row <span class="kw">in</span> df.iterrows():</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        doi <span class="op">=</span> row[<span class="st">"DOI"</span>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        metadata <span class="op">=</span> enrichment.get(doi, {})</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> metadata:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pd.isna(row.get(<span class="st">"Abstract"</span>)) <span class="kw">or</span> <span class="kw">not</span> <span class="bu">str</span>(row[<span class="st">"Abstract"</span>]).strip():</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"Abstract"</span>] <span class="op">=</span> metadata.get(<span class="st">"Abstract"</span>, row[<span class="st">"Abstract"</span>])</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pd.isna(row.get(<span class="st">"ItemType"</span>)) <span class="kw">or</span> <span class="kw">not</span> <span class="bu">str</span>(row[<span class="st">"ItemType"</span>]).strip() <span class="kw">or</span> row[<span class="st">"ItemType"</span>] <span class="op">==</span> <span class="st">"N/A"</span>:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"ItemType"</span>] <span class="op">=</span> metadata.get(<span class="st">"ItemType"</span>, row.get(<span class="st">"ItemType"</span>, <span class="st">"N/A"</span>))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            citation_count <span class="op">=</span> metadata.get(<span class="st">"CitationCount"</span>, row.get(<span class="st">"CitationCount"</span>))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"CitationCount"</span>] <span class="op">=</span> <span class="bu">int</span>(citation_count) <span class="cf">if</span> citation_count <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            primary_topic <span class="op">=</span> metadata.get(<span class="st">"primary_topic"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> primary_topic:</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"primary_topic"</span>] <span class="op">=</span> json.dumps(primary_topic)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"primary_topic"</span>] <span class="op">=</span> pd.NA</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"is_published"</span>] <span class="op">=</span> metadata.get(<span class="st">"is_published"</span>, row.get(<span class="st">"is_published"</span>))</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"is_retracted"</span>] <span class="op">=</span> metadata.get(<span class="st">"is_retracted"</span>, row.get(<span class="st">"is_retracted"</span>))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"OpenAlexID"</span>] <span class="op">=</span> metadata.get(<span class="st">"OpenAlexID"</span>, row.get(<span class="st">"OpenAlexID"</span>))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"CitationCount"</span>] <span class="op">=</span> df[<span class="st">"CitationCount"</span>].fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The results are then intelligently merged back into the main DataFrame. This process is non-destructive; it is designed to backfill missing information rather than overwrite existing data. For instance, an abstract retrieved from Crossref will only be added to a record if that record’s Abstract field is currently empty. Similarly, fields like ItemType or CitationCount are updated only if the existing data is null or a placeholder. This ensures that the final output is the most complete possible version of each record, combining the strengths of all queried databases.</p>
<hr>
</section>
</section>
<section id="stage-2-asynchronous-full-text-pdf-retrieval" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="stage-2-asynchronous-full-text-pdf-retrieval"><span class="header-section-number">7.1.3</span> Stage 2: Asynchronous Full-Text PDF Retrieval</h3>
<p>Following the creation of a unified and deduplicated list of scholarly articles, the next critical stage of the pipeline is the acquisition of the full-text content for each entry. This is essential for the deep content analysis, entity extraction, and topic modeling performed in subsequent stages. The system orchestrates this process by leveraging the Digital Object Identifier (DOI) of each article to query the <em>Unpaywall API</em>, a service that indexes legally available open-access versions of scholarly publications. Given that this stage involves potentially thousands of network requests, a high-throughput, asynchronous architecture was implemented to manage this I/O-bound task efficiently.</p>
<p>The core retrieval logic, encapsulated in <em>scripts/async_unpaywall.py</em>, is built upon Python’s <em>asyncio</em> and <em>aiohttp</em> libraries. This approach allows the system to initiate and manage thousands of concurrent HTTP requests, preventing the pipeline from being blocked by network latency from any single request. To ensure compliance with Unpaywall’s API usage policies and to maintain system stability, an <em>asyncio.Semaphore</em> is employed as a robust rate-limiting mechanism. The semaphore is initialized with the <em>API_RATE_LIMIT</em> value specified in the project’s configuration file (e.g., 8 queries per second). Each asynchronous worker task must acquire the semaphore before making an API call, guaranteeing that the overall request rate does not exceed the defined limit. The worker function first queries Unpaywall for a given DOI to get a list of potential PDF locations, prioritizing the <em>best_oa_location</em>, and then iteratively attempts to download the file from each candidate URL until a valid PDF is secured.</p>
<div id="unpaywall-worker" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/async_unpaywall.py</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> _worker(row_idx, doi, title, session):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return (row_idx, pdf_path|None, status_string)."""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    api_url <span class="op">=</span> <span class="ss">f"https://api.unpaywall.org/v2/</span><span class="sc">{</span>doi<span class="sc">}</span><span class="ss">?email=</span><span class="sc">{</span>UNPAYWALL_EMAIL<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># _fetch_json is rate-limited by a semaphore</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="cf">await</span> _fetch_json(session, api_url)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> data:</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> row_idx, <span class="va">None</span>, <span class="st">"Unpaywall_fail"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build a list of candidate URLs, prioritizing the best_oa_location</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    candidates <span class="op">=</span> []</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (b <span class="op">:=</span> data.get(<span class="st">"best_oa_location"</span>)): candidates.append(b)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    candidates <span class="op">+=</span> data.get(<span class="st">"oa_locations"</span>, [])</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> loc <span class="kw">in</span> candidates:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        pdf_url <span class="op">=</span> loc.get(<span class="st">"url_for_pdf"</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> pdf_url:</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attempt to download the PDF from the candidate URL</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        pdf_bytes <span class="op">=</span> <span class="cf">await</span> _download_pdf(session, pdf_url)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pdf_bytes:</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sanitize filename and save the file</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            safe_name <span class="op">=</span> re.sub(<span class="vs">r'[\\/*?:"&lt;&gt;|()]+'</span>, <span class="st">''</span>, title)[:<span class="dv">100</span>] <span class="kw">or</span> doi</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            fname <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>safe_name<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>row_idx<span class="sc">}</span><span class="ss">.pdf"</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            path  <span class="op">=</span> os.path.join(PDF_SAVE_FOLDER, fname)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                f.write(pdf_bytes)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> row_idx, path, <span class="st">"Saved"</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> row_idx, <span class="va">None</span>, <span class="st">"No_PDF"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Upon successful download, the PDF is saved to a local directory defined in the configuration, and the main DataFrame is updated with the local file path for that article record. Records for which a full-text PDF could not be successfully retrieved are filtered out at the conclusion of this stage. The resulting DataFrame, now containing only articles with corresponding local PDF files, is then passed to the next stage of the pipeline for content extraction.</p>
<hr>
</section>
<section id="stage-3-bibliographic-management-and-citation-rendering" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="stage-3-bibliographic-management-and-citation-rendering"><span class="header-section-number">7.1.4</span> Stage 3: Bibliographic Management and Citation Rendering</h3>
<p>Following the acquisition and deduplication of article metadata and the retrieval of corresponding full-text PDFs, the pipeline proceeds to a crucial data management and enrichment stage. The objective here is twofold: first, to create a permanent, centralized, and queryable library of all processed literature using the Zotero reference management system; and second, to pre-generate authoritative citation metadata for each article. This pre-rendered citation data serves as a verifiable “ground truth” for the downstream Retrieval-Augmented Generation (RAG) agent, ensuring the academic rigor and traceability of the final synthesized output. This entire process is orchestrated by the s<em>cripts/fast_zotero_gui.py</em> module.</p>
<p>The integration with Zotero is architected for high-throughput processing. The <em>integrate</em> function within the module receives the DataFrame of articles with valid PDF paths and uses the <em>pyzotero</em> library to communicate with the Zotero API. To maximize efficiency, new items are created in batches of up to 50 articles per API call. Subsequently, the system utilizes a <em>ThreadPoolExecutor</em> to upload the associated PDF files to the newly created Zotero entries in parallel, significantly reducing the time required for this I/O-bound task.</p>
<p>A key technical innovation in this module is the method of citation generation. Rather than relying on slow, repeated API calls to Zotero for formatting each citation, the system leverages the <em>citeproc-py</em> library to perform this rendering locally. This approach uses standard Citation Style Language (CSL) files (e.g., apa.csl, ieee.csl), which are selectable in the GUI, to instantly format both the full bibliographic citation and the corresponding in-text citation for every article. The module also includes a robust fallback mechanism that can query the Zotero API for the citation if the local <em>citeproc</em> rendering encounters an error for a specific entry.</p>
<div id="zotero-citation-rendering" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/fast_zotero_gui.py</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... inside the integrate function loop ...</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, zkey <span class="kw">in</span> attach.items():</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    df.at[idx, <span class="st">"ZoteroKey"</span>] <span class="op">=</span> zkey</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    meta <span class="op">=</span> {c: df.at[idx, c] <span class="cf">for</span> c <span class="kw">in</span> (</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Title"</span>, <span class="st">"DOI"</span>, <span class="st">"Authors"</span>, <span class="st">"Date"</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Journal"</span>, <span class="st">"Volume"</span>, <span class="st">"Issue"</span>, <span class="st">"Pages"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    )}</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attempt local citation rendering first</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        in_text, full <span class="op">=</span> _render_citations(meta, csl_path)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> exc:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If local rendering fails, use the fallback</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        logging.error(<span class="ss">f"citeproc rendering failed for row </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>exc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        in_text, full <span class="op">=</span> fallback(zkey, csl_path) <span class="cf">if</span> <span class="bu">callable</span>(fallback) <span class="cf">else</span> (<span class="st">""</span>, <span class="st">""</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    df.at[idx, <span class="st">"InTextCitation"</span>] <span class="op">=</span> in_text <span class="kw">or</span> <span class="st">""</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    df.at[idx, <span class="st">"FullCitation"</span>] <span class="op">=</span> full <span class="kw">or</span> <span class="st">""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The impact of this stage on the overall system is critical. The generated <em>ZoteroKey</em>, <em>InTextCitation</em>, and <em>FullCitation</em> generated in this stage are not merely for user reference; they are programmatically injected back into the DataFrame. This enriched data is subsequently passed as part of the context to the downstream Retrieval-Augmented Generation (RAG) agent.</p>
<p>This architectural choice is fundamental to ensuring the factuality and academic rigor of the final synthesized output. The RAG agent is explicitly instructed to populate its findings into a structured JSON object that requires these specific metadata fields for each piece of evidence it presents. By providing this pre-generated, validated citation data directly within the context, the system constrains the Large Language Model. This prevents the LLM from attempting to generate its own citations, a task for which they are notoriously unreliable and prone to hallucination.</p>
<p>Instead of showing the entire internal prompt, the following illustrates the structure of the metadata object that the RAG agent must populate for each observation it makes. The presence of the Zotero-derived fields highlights their essential role in the validation process.</p>
<div id="metadata-injection" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Example Metadata Schema <span class="cf">for</span> a single RAG Observation</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"metadata"</span>: {</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PDF_DocIndex"</span>: <span class="st">"148"</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PDF_ChunkIndex"</span>: <span class="st">"0"</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Struct_DocIndex"</span>: <span class="st">"N/A"</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Struct_ChunkIndex"</span>: <span class="st">"N/A"</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"DOI"</span>: <span class="st">"10.1016/j.jobe.2021.103722"</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ZoteroKey"</span>: <span class="st">"ABC123DE"</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"InTextCitation"</span>: <span class="st">"(Author et al., 2021)"</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"FullCitation"</span>: <span class="st">"Author, A., Author, B., &amp; Author, C. (2021). Title of the article. Journal Name, 1(2), 100-110. https://doi.org/10.1016/j.jobe.2021.103722"</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This architectural choice constrains the Large Language Model, preventing it from generating its own, potentially hallucinated, citations. It ensures that every piece of evidence presented in the final answer is verifiably and accurately linked back to a specific source document managed within the Zotero library, creating a fully traceable and academically sound output.</p>
<hr>
</section>
<section id="stage-4-content-extraction-via-document-layout-analysis" class="level3" data-number="7.1.5">
<h3 data-number="7.1.5" class="anchored" data-anchor-id="stage-4-content-extraction-via-document-layout-analysis"><span class="header-section-number">7.1.5</span> Stage 4: Content Extraction via Document Layout Analysis</h3>
<p><strong>Objective: High-Fidelity Extraction from Scholarly PDFs</strong></p>
<p>A foundational requirement for the ETL pipeline is the ability to accurately and efficiently extract content from thousands of scholarly articles in PDF format. This task is notoriously challenging due to the format’s complex, variable layouts and the need to preserve structural context. The objective for this stage was to move beyond simple text extraction and reliably parse not only the full textual body of each article but also complex embedded elements crucial for scientific analysis, such as tabular data and mathematical formulas formatted in LaTeX.</p>
<section id="core-technology-and-identified-limitations" class="level4" data-number="7.1.5.1">
<h4 data-number="7.1.5.1" class="anchored" data-anchor-id="core-technology-and-identified-limitations"><span class="header-section-number">7.1.5.1</span> Core Technology and Identified Limitations</h4>
<p>To meet this objective, the system leverages IBM’s Docling library as its core document analysis engine, a powerful framework that uses machine learning for sophisticated page layout detection. However, initial implementation and testing revealed several significant limitations with the out-of-the-box library that necessitated a substantial re-engineering effort:</p>
<ol type="1">
<li><p><strong>Performance Bottleneck:</strong> The framework lacked native support for efficient parallelization, making the processing of thousands of documents a prohibitively time-consuming, single-threaded operation.</p></li>
<li><p><strong>Model Inefficiency:</strong> TThe original formula recognition model, based on the <em>SamOPTForCausalLM</em> architecture, was found to be excessively resource-intensive, requiring approximately 30 GB of VRAM per instance, constraining the ability to parallelize effectively.</p></li>
<li><p><strong>Layout Inaccuracies:</strong> The layout detection model frequently produced suboptimal bounding boxes. This manifested in two primary ways:</p>
<ul>
<li><strong>Formula Fragmentation:</strong> Single, multi-line mathematical formulas were often incorrectly segmented into several independent clusters.</li>
<li><strong>Layout Misclassification:</strong> Pages containing structured elements like line numbers were often misclassified in their entirety as a single, large TABLE cluster, losing all other semantic information.</li>
</ul></li>
<li><p><strong>Stability Issues:</strong> Persistent glyph-parsing errors within the core library caused instability during large-scale processing runs.</p></li>
</ol>
<p>To address these significant challenges, a substantial re-engineering of the Docling library was undertaken. The resulting modified codebase, along with testing scripts, sample data, and a validation notebook, has been made publicly available in a dedicated GitHub repository to ensure transparency and allow for independent validation of the enhancements.</p>
<p><a href="https://github.com/agodinezmm2007/docling_mod">Github Repository</a></p>
</section>
<section id="custom-enhancements-and-implementation" class="level4" data-number="7.1.5.2">
<h4 data-number="7.1.5.2" class="anchored" data-anchor-id="custom-enhancements-and-implementation"><span class="header-section-number">7.1.5.2</span> Custom Enhancements and Implementation</h4>
<p>To overcome these challenges, the system was transformed into a high-performance, accurate, and efficient processing system through a series of targeted enhancements, primarily orchestrated by <em>scripts/docling_multi_mp_gui.py</em>.</p>
<p>The performance bottleneck was addressed first by designing a custom architecture around Python’s <em>concurrent.futures.ProcessPoolExecutor</em>. A key innovation was the implementation of a custom <em>worker_initializer</em> function that facilitates true parallel processing across multiple GPUs. This initializer uses a locking mechanism to assign each new worker process to a specific GPU from the available hardware pool, effectively converting Docling into a distributed application and dramatically reducing processing time.</p>
<p>The accuracy and inefficiency issues of the formula recognition model were solved by replacing it entirely. The core <em>code_formula_predictor.py</em> was reconfigured to integrate a more efficient and accurate model, <em>“SmolDocling,”</em> which reduced VRAM requirements to ~8GB.</p>
<p>Finally, the layout inaccuracies and stability issues were addressed through direct modifications to the Docling codebase. This included engineering a sophisticated post-processing step in <em>docling/utils/layout_postprocessor.py</em> with rule-based heuristics to merge fragmented formulas and re-classify entire misidentified pages. The persistent glyph errors required low-level debugging by reverse-engineering the compiled C-extension library (<em>pdf_parsers.cpython-312-x86_64-linux-gnu.so</em>) using the <em>Ghidra</em> software reverse engineering suite to diagnose and patch the root cause.</p>
<p>These extensive modifications are reflected in the numerous library files that were changed during development:</p>
<ul>
<li><p><strong>Primary Docling Library Modifications:</strong></p>
<ul>
<li>docling/backend/docling_parse_v4_backend.py</li>
<li>docling/datamodel/base_models.py</li>
<li>docling/datamodel/pipeline_options.py</li>
<li>docling/models/code_formula_model.py</li>
<li>docling/models/layout_model.py</li>
<li>docling/pipeline/vlm_pipeline.py</li>
<li>docling/utils/layout_postprocessor.py</li>
</ul></li>
<li><p><strong>IBM Model Predictor Modifications:</strong></p>
<ul>
<li>docling_ibm_models/code_formula_model/code_formula_predictor.py</li>
<li>docling_ibm_models/layoutmodel/layout_predictor.py</li>
</ul></li>
</ul>
</section>
<section id="initial-challenge-inaccurate-bounding-boxes-and-model-inefficiency" class="level4" data-number="7.1.5.3">
<h4 data-number="7.1.5.3" class="anchored" data-anchor-id="initial-challenge-inaccurate-bounding-boxes-and-model-inefficiency"><span class="header-section-number">7.1.5.3</span> Initial Challenge: Inaccurate Bounding Boxes and Model Inefficiency</h4>
<p>The first major issue identified was the precision of the layout analysis model. The bounding boxes generated for mathematical formulas were often too tight or included extraneous visual artifacts from surrounding text. When these imperfect image snippets were passed to the original formula recognition model <strong>(ds4sd/CodeFormula)</strong>, it frequently failed, resulting in generative feedback loops that produced repetitive, malformed LaTeX output (as exemplified in <a href="#fig-bad-form" class="quarto-xref">Figure&nbsp;<span>7.2</span></a>, showing a poorly cropped snippet).</p>
<div id="fig-bad-form" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bad-form-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/bad_form_17.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bad-form-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Bad Formula
</figcaption>
</figure>
</div>
<p>Furthermore, the original model, based on the <strong>SamOPTForCausalLM</strong> architecture, was found to be excessively resource-intensive, requiring approximately 30 GB of VRAM per instance. This made large-scale parallelization across multiple GPUs prohibitively expensive and inefficient.</p>
</section>
<section id="bounding-box-and-image-preprocessing-modifications" class="level4" data-number="7.1.5.4">
<h4 data-number="7.1.5.4" class="anchored" data-anchor-id="bounding-box-and-image-preprocessing-modifications"><span class="header-section-number">7.1.5.4</span> Bounding Box and Image Preprocessing Modifications</h4>
<p>The initial attempt to resolve the formula recognition issue focused on improving the quality of the image snippets before they were sent to the model. This involved significant modifications to <em>docling/models/code_formula_model.py</em>.</p>
<ul>
<li><strong>Dynamic Bounding Box Expansion:</strong> Logic was introduced in the <em>prepare_element</em> method to dynamically expand the bounding boxes of detected formulas. Instead of using the original, often tightly-cropped box, the new implementation expands the box by a configurable ratio of the snippet’s height and width. This ensures more visual context is captured as demonstrated in <a href="#fig-good-form" class="quarto-xref">Figure&nbsp;<span>7.3</span></a>.</li>
</ul>
<div id="fig-good-form" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-good-form-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/good_form_17.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-good-form-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.3: Good Formula
</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Ratio-Based Image Padding:</strong> The original *_pad_with_most_frequent_edge_color* function, which used static pixel padding, was entirely replaced. The new version accepts floating-point ratios, allowing it to apply padding that is proportional to the image’s dimensions. This creates a more consistent and contextually appropriate input for the model, regardless of the formula’s original size.</p></li>
<li><p><strong>Conditional Image Masking:</strong> To further isolate formulas from surrounding text, conditional logic was added to use Docling’s <em>page.get_masked_image()</em> method exclusively for items labeled as DocItemLabel.FORMULA.</p></li>
</ul>
</section>
<section id="implementation-of-selective-content-masking" class="level4" data-number="7.1.5.5">
<h4 data-number="7.1.5.5" class="anchored" data-anchor-id="implementation-of-selective-content-masking"><span class="header-section-number">7.1.5.5</span> Implementation of Selective Content Masking</h4>
<p>The most critical enhancement was the development of a method to isolate formula elements from distracting adjacent text. When the layout model produced imperfect bounding boxes, snippets would often contain partial lines of text from the main body, confusing the formula recognition model. To solve this, a selective masking capability was engineered directly into the <em>Page</em> class within <em>docling/datamodel/base_models.py</em>.</p>
<p>This was achieved by introducing two new methods, <em>get_masked_image</em> and the internal *_create_masked_image<em>. The </em>_create_masked_image* function operates by first taking a complete, high-resolution image of a document page. It then iterates through all layout clusters previously identified by the layout model. For every cluster that is not labeled as a <em>DocItemLabel.FORMULA</em>, the function programmatically draws a white rectangle over its bounding box.</p>
<div id="create-masked-image" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _create_masked_image(<span class="va">self</span>, scale: <span class="bu">float</span>, pdf_identifier: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> Optional[Image]:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        original_image <span class="op">=</span> <span class="va">self</span>.get_image(scale<span class="op">=</span>scale)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> original_image <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> <span class="va">self</span>.size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>            _log.warning(<span class="st">"Original image or size is None for page </span><span class="sc">%s</span><span class="st">."</span>, <span class="va">self</span>.page_no)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        masked <span class="op">=</span> original_image.copy()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        draw <span class="op">=</span> ImageDraw.Draw(masked)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        scale_x <span class="op">=</span> masked.width <span class="op">/</span> <span class="va">self</span>.size.width</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        scale_y <span class="op">=</span> masked.height <span class="op">/</span> <span class="va">self</span>.size.height</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        top_expansion_factor <span class="op">=</span> <span class="fl">0.045</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        bottom_expansion_factor <span class="op">=</span> <span class="fl">0.045</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        cluster_count <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.predictions.layout.clusters)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        _log.debug(<span class="st">"Creating masked image for page </span><span class="sc">%s</span><span class="st"> with </span><span class="sc">%d</span><span class="st"> clusters."</span>, <span class="va">self</span>.page_no, cluster_count)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mask non-formula clusters explicitly here with correct scaling</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> cl <span class="kw">in</span> <span class="va">self</span>.predictions.layout.clusters:</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> cl.label <span class="op">!=</span> DocItemLabel.FORMULA:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>                bbox <span class="op">=</span> cl.bbox.to_top_left_origin(page_height<span class="op">=</span><span class="va">self</span>.size.height)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>                bbox_height <span class="op">=</span> bbox.b <span class="op">-</span> bbox.t</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>                expanded_bbox <span class="op">=</span> BoundingBox(</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                    l<span class="op">=</span>bbox.l,</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                    t<span class="op">=</span>bbox.t <span class="op">-</span> bbox_height <span class="op">*</span> top_expansion_factor,</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                    r<span class="op">=</span>bbox.r,</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                    b<span class="op">=</span>bbox.b <span class="op">+</span> bbox_height <span class="op">*</span> bottom_expansion_factor,</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                    coord_origin<span class="op">=</span>bbox.coord_origin,</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                )               </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>                scaled_bbox <span class="op">=</span> BoundingBox(</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                    l<span class="op">=</span>expanded_bbox.l <span class="op">*</span> scale_x,</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>                    t<span class="op">=</span>expanded_bbox.t <span class="op">*</span> scale_y,</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>                    r<span class="op">=</span>expanded_bbox.r <span class="op">*</span> scale_x,</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>                    b<span class="op">=</span>expanded_bbox.b <span class="op">*</span> scale_y,</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>                    coord_origin<span class="op">=</span>expanded_bbox.coord_origin</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                draw.rectangle(scaled_bbox.as_tuple(), fill<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> masked</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The result is a new, “masked” image of the page where all non-formula content has been completely obscured, as demonstrated in the comparison between an original page layout (<a href="#fig-unmasked" class="quarto-xref">Figure&nbsp;<span>7.4</span></a>) and its masked counterpart (<a href="#fig-masked" class="quarto-xref">Figure&nbsp;<span>7.5</span></a>).</p>
<div id="fig-unmasked" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unmasked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/postprocessed_layout_page_00020.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unmasked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.4: Unmasked Page
</figcaption>
</figure>
</div>
<div id="fig-masked" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-masked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/masked_page_00020_scale_2.6.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-masked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.5: Masked Page
</figcaption>
</figure>
</div>
<p>When the downstream <em>prepare_element</em> method in <em>code_formula_model.py</em> is called, it now uses this <em>get_masked_image</em> function instead of the standard <em>get_image</em>. By cropping the formula’s bounding box from this pre-masked page, the system guarantees that the final image snippet sent to the SmolDocling model contains only the formula itself, significantly improving the signal-to-noise ratio and preventing recognition errors caused by textual artifacts.</p>
</section>
<section id="heuristic-based-merging-of-fragmented-layout-clusters" class="level4" data-number="7.1.5.6">
<h4 data-number="7.1.5.6" class="anchored" data-anchor-id="heuristic-based-merging-of-fragmented-layout-clusters"><span class="header-section-number">7.1.5.6</span> Heuristic-Based Merging of Fragmented Layout Clusters</h4>
<p>Beyond improving the quality of individual image snippets, a second significant challenge was identified in the output of the base layout analysis model: the fragmentation of single, cohesive document elements into multiple, distinct bounding boxes. This issue was particularly prevalent with complex, multi-line mathematical formulas, which were often incorrectly segmented into several independent <em>FORMULA</em> clusters, as exemplified in <a href="#fig-fragmented" class="quarto-xref">Figure&nbsp;<span>7.6</span></a>. This fragmentation would lead to the downstream formula recognition model receiving only partial equations, resulting in incomplete and unusable LaTeX output.</p>
<div id="fig-fragmented" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fragmented-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/postprocessed_layout_page_00009_bad.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fragmented-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.6: Fragmented Formulas
</figcaption>
</figure>
</div>
<p>To resolve this, a sophisticated post-processing step was engineered within <em>docling/utils/layout_postprocessor.py</em>. The purpose of this step is to apply a series of rule-based heuristics to refine and correct the raw output of the initial layout model before the final document structure is assembled.</p>
<div id="merge-clusters" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _merge_vertically_adjacent_formulas(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    clusters,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    vertical_threshold_factor<span class="op">=</span><span class="fl">1.8</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    horizontal_overlap_threshold<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    alignment_threshold<span class="op">=</span><span class="dv">20</span>,  <span class="co"># pixel-based threshold</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    max_alignment_ratio<span class="op">=</span><span class="fl">0.2</span>,  <span class="co"># ratio-based threshold (e.g., 0.2 means 20% mismatch)</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">    merge vertically adjacent FORMULA clusters using Union-Find.</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">    New logic:</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">    1) Compute a dynamic vertical threshold based on median formula height.</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">    2) Expand bounding boxes horizontally using padding to compute the horizontal overlap.</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co">    3) Compute both absolute (pixel) differences and ratio-based alignment.</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co">    4) Extract formula numbers from cell texts. If both clusters have numbers and they differ, skip merge.</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">    5) If neither cluster has a formula number, require that the vertical gap is very small (&lt;= 10 px)</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co">        and that the horizontal overlap is nearly complete (&gt;= 0.9).</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co">    6) Otherwise, use the normal criteria based on vertical gap (and derived thresholds).</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_alignment_factor(c1, c2):</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the average width and return the maximum normalized edge difference.</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        w1 <span class="op">=</span> c1.bbox.r <span class="op">-</span> c1.bbox.l</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        w2 <span class="op">=</span> c2.bbox.r <span class="op">-</span> c2.bbox.l</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        avg_width <span class="op">=</span> <span class="bu">max</span>((w1 <span class="op">+</span> w2) <span class="op">/</span> <span class="fl">2.0</span>, <span class="fl">1e-6</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        left_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.l <span class="op">-</span> c2.bbox.l)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        right_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.r <span class="op">-</span> c2.bbox.r)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">max</span>(left_diff <span class="op">/</span> avg_width, right_diff <span class="op">/</span> avg_width)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Only process clusters labelled as FORMULA.</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    formula_clusters <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> clusters <span class="cf">if</span> c.label <span class="op">==</span> DocItemLabel.FORMULA]</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    non_formula_clusters <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> clusters <span class="cf">if</span> c.label <span class="op">!=</span> DocItemLabel.FORMULA]</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> formula_clusters:</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> clusters</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort by the top coordinate.</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    formula_clusters.sort(key<span class="op">=</span><span class="kw">lambda</span> c: c.bbox.t)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    heights <span class="op">=</span> [c.bbox.b <span class="op">-</span> c.bbox.t <span class="cf">for</span> c <span class="kw">in</span> formula_clusters]</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    median_height <span class="op">=</span> np.median(heights) <span class="cf">if</span> heights <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    vertical_threshold <span class="op">=</span> median_height <span class="op">*</span> vertical_threshold_factor</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build a mapping from cluster id to cluster instance.</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    id_to_cluster <span class="op">=</span> {c.<span class="bu">id</span>: c <span class="cf">for</span> c <span class="kw">in</span> formula_clusters}</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    uf <span class="op">=</span> UnionFind(<span class="bu">list</span>(id_to_cluster.keys()))</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(formula_clusters)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i <span class="op">+</span> <span class="dv">1</span>, n):</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>            c1 <span class="op">=</span> formula_clusters[i]</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>            c2 <span class="op">=</span> formula_clusters[j]</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute vertical gap (c2 is assumed to be below c1)</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>            vertical_gap <span class="op">=</span> c2.bbox.t <span class="op">-</span> c1.bbox.b</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> vertical_gap <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">or</span> vertical_gap <span class="op">&gt;</span> vertical_threshold:</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Expand bounding boxes horizontally using the given padding.</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>            expanded_bbox_c1 <span class="op">=</span> <span class="bu">type</span>(c1.bbox)(</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>                l<span class="op">=</span>c1.bbox.l <span class="op">-</span> padding,</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>                t<span class="op">=</span>c1.bbox.t,</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>                r<span class="op">=</span>c1.bbox.r <span class="op">+</span> padding,</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>                b<span class="op">=</span>c1.bbox.b</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>            expanded_bbox_c2 <span class="op">=</span> <span class="bu">type</span>(c2.bbox)(</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>                l<span class="op">=</span>c2.bbox.l <span class="op">-</span> padding,</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>                t<span class="op">=</span>c2.bbox.t,</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>                r<span class="op">=</span>c2.bbox.r <span class="op">+</span> padding,</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>                b<span class="op">=</span>c2.bbox.b</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>            horizontal_overlap <span class="op">=</span> <span class="bu">min</span>(expanded_bbox_c1.r, expanded_bbox_c2.r) <span class="op">-</span> <span class="bu">max</span>(expanded_bbox_c1.l, expanded_bbox_c2.l)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>            min_width <span class="op">=</span> <span class="bu">min</span>(expanded_bbox_c1.r <span class="op">-</span> expanded_bbox_c1.l, expanded_bbox_c2.r <span class="op">-</span> expanded_bbox_c2.l)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>            overlap_ratio <span class="op">=</span> horizontal_overlap <span class="op">/</span> min_width <span class="cf">if</span> min_width <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>            left_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.l <span class="op">-</span> c2.bbox.l)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>            right_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.r <span class="op">-</span> c2.bbox.r)</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>            alignment_factor <span class="op">=</span> compute_alignment_factor(c1, c2)</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract formula numbers using _extract_formula_number.</span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>            num1 <span class="op">=</span> <span class="va">self</span>._extract_formula_number(c1)</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>            num2 <span class="op">=</span> <span class="va">self</span>._extract_formula_number(c2)</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 4) If both have numbers and differ, skip</span></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> num1 <span class="kw">and</span> num2 <span class="kw">and</span> num1 <span class="op">!=</span> num2:</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Normal merging criteria based on vertical gap and alignment.</span></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Pre-calculate default geometry-based threshold</span></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> vertical_gap <span class="op">&lt;=</span> (<span class="fl">0.5</span> <span class="op">*</span> vertical_threshold):</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> left_diff <span class="op">&lt;=</span> alignment_threshold <span class="kw">and</span> right_diff <span class="op">&lt;=</span> alignment_threshold:</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>                    required_overlap <span class="op">=</span> horizontal_overlap_threshold</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>                    required_overlap <span class="op">=</span> <span class="fl">0.85</span></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> vertical_gap <span class="op">&lt;=</span> vertical_threshold:</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> alignment_factor <span class="op">&lt;=</span> max_alignment_ratio:</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>                    required_overlap <span class="op">=</span> <span class="fl">0.85</span></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>            <span class="co"># New branch: if no formula number is detected in either cluster,</span></span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>            <span class="co"># then enforce a very small vertical gap and nearly complete horizontal overlap.</span></span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 2) Both None =&gt; require vertical_gap ≤10 and overlap ≥0.9</span></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> num1 <span class="kw">is</span> <span class="va">None</span> <span class="kw">and</span> num2 <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> vertical_gap <span class="op">&gt;</span> <span class="dv">3</span>:  <span class="co"># Change this constant if needed</span></span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Require a higher overlap; allow slight relaxation if the normal criterion is higher.</span></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>                required_overlap <span class="op">=</span> <span class="bu">max</span>(required_overlap, <span class="fl">0.9</span>)</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 3) One number missing =&gt; require vertical_gap ≤5 and overlap ≥0.95</span></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> (num1 <span class="kw">is</span> <span class="va">None</span>) <span class="op">!=</span> (num2 <span class="kw">is</span> <span class="va">None</span>):</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>                <span class="co"># i.e. exactly one side is missing a number</span></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> vertical_gap <span class="op">&gt;</span> <span class="fl">12.8</span>:</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>                required_overlap <span class="op">=</span> <span class="bu">max</span>(required_overlap, <span class="fl">0.95</span>)</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 1) If both exist and match =&gt; use normal geometry (already in required_overlap)</span></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Final merge check</span></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> overlap_ratio <span class="op">&gt;=</span> required_overlap:</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>                uf.union(c1.<span class="bu">id</span>, c2.<span class="bu">id</span>)</span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> uf.get_groups()</span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>    merged_formula_clusters <span class="op">=</span> []</span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> group_ids <span class="kw">in</span> groups.values():</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a>        group_clusters <span class="op">=</span> [id_to_cluster[g] <span class="cf">for</span> g <span class="kw">in</span> group_ids]</span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a>        merged_bbox <span class="op">=</span> <span class="bu">type</span>(group_clusters[<span class="dv">0</span>].bbox)(</span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a>            l<span class="op">=</span><span class="bu">min</span>(c.bbox.l <span class="cf">for</span> c <span class="kw">in</span> group_clusters),</span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a>            t<span class="op">=</span><span class="bu">min</span>(c.bbox.t <span class="cf">for</span> c <span class="kw">in</span> group_clusters),</span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a>            r<span class="op">=</span><span class="bu">max</span>(c.bbox.r <span class="cf">for</span> c <span class="kw">in</span> group_clusters),</span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a>            b<span class="op">=</span><span class="bu">max</span>(c.bbox.b <span class="cf">for</span> c <span class="kw">in</span> group_clusters)</span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a>        combined_cells <span class="op">=</span> []</span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> group_clusters:</span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a>            combined_cells.extend(c.cells)</span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a>        merged_cluster <span class="op">=</span> group_clusters[<span class="dv">0</span>]</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a>        merged_cluster.bbox <span class="op">=</span> merged_bbox</span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a>        merged_cluster.cells <span class="op">=</span> <span class="va">self</span>._sort_cells(<span class="va">self</span>._deduplicate_cells(combined_cells))</span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a>        merged_formula_clusters.append(merged_cluster)</span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> non_formula_clusters <span class="op">+</span> merged_formula_clusters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The core of this enhancement is the new *_merge_vertically_adjacent_formulas* method. This algorithm intelligently identifies and merges clusters that likely belong to the same formula. It operates on a set of heuristics designed to distinguish between separate, adjacent equations and multiple lines of a single equation:</p>
<ol type="1">
<li><p><strong>Dynamic Proximity Threshold:</strong> Rather than using a fixed pixel distance, the method first calculates the median height of all detected formulas on a page. It then defines a dynamic vertical proximity threshold based on a factor of this median height, allowing it to adapt to documents with different font sizes and line spacing.</p></li>
<li><p><strong>Formula Number Extraction:</strong> A helper function, _extract_formula_number, uses regular expressions to find equation numbers (e.g., (1), (2a), (A5)) within the text of each formula cluster.</p></li>
<li><p><strong>Contextual Merging Logic:</strong> The algorithm iterates through pairs of vertically adjacent formula clusters and applies a decision-making process:</p>
<ul>
<li>If both clusters have distinct formula numbers (e.g., (1) and (2)), they are correctly identified as separate equations and are not merged.<br>
</li>
<li>If neither cluster has a formula number, they are only merged if the vertical gap between them is minimal and their horizontal overlap is nearly complete, which is characteristic of multi-line equations without a single encompassing number.<br>
</li>
<li>If one cluster has a number and the adjacent one does not, a stricter set of proximity and overlap rules is applied.</li>
</ul></li>
</ol>
<p>Once candidate clusters for merging are identified, a Union-Find data structure is used to efficiently group them. The final step involves creating a new, single bounding box that encompasses all the merged clusters and combining their constituent text cells. This new, unified cluster is then passed to the next stage of the pipeline, ensuring the entire formula is processed as a single unit as demonstrated in <a href="#fig-unfragmented" class="quarto-xref">Figure&nbsp;<span>7.7</span></a>. Additional heuristics, such as the *_filter_tables_containing_page_footer* method, were also added to this module to correct other common layout analysis errors, further improving the overall quality of the parsed document structure.</p>
<div id="fig-unfragmented" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unfragmented-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/postprocessed_layout_page_00009_crop_carb.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unfragmented-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.7: Unfragmented Formulas
</figcaption>
</figure>
</div>
</section>
<section id="misclassification-of-page-layouts" class="level4" data-number="7.1.5.7">
<h4 data-number="7.1.5.7" class="anchored" data-anchor-id="misclassification-of-page-layouts"><span class="header-section-number">7.1.5.7</span> Misclassification of Page Layouts</h4>
<p>This analysis details further enhancements made to the layout post-processing logic to address specific, recurring classification errors observed in certain document types, such as pre-prints or articles with line numbers.</p>
<p>A common failure mode was observed where the layout detection model would misclassify an entire page or large sections of a page as a single, large TABLE cluster. This issue was particularly prevalent in documents that featured line numbers running down the left-hand margin, as seen in <a href="#fig-table-misscl" class="quarto-xref">Figure&nbsp;<span>7.8</span></a>. The presence of this repeating, structured numerical column appeared to mislead the model into perceiving the entire text block as tabular data. This resulted in the loss of all other semantic distinctions for that page (e.g., paragraphs, headers, lists), rendering the content unusable for downstream extraction.</p>
<div id="fig-table-misscl" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-table-misscl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/utility_postprocessed_layout_page_00003_before_fix_attempt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-table-misscl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.8: Table Missclassification
</figcaption>
</figure>
</div>
</section>
<section id="heuristic-based-re-classification-of-misidentified-clusters" class="level4" data-number="7.1.5.8">
<h4 data-number="7.1.5.8" class="anchored" data-anchor-id="heuristic-based-re-classification-of-misidentified-clusters"><span class="header-section-number">7.1.5.8</span> Heuristic-Based Re-classification of Misidentified Clusters</h4>
<p>To correct this, a new heuristic-based filtering method, *_filter_tables_containing_page_footer*, was engineered and integrated into the main postprocess pipeline. While the method name suggests a focus on page footers, its logic was designed more broadly to identify and re-classify any cluster that was erroneously labeled as a table-like structure when it was, in fact, the main body text of the page.</p>
<div id="filter-large-tables" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/docling/utils/layout_postprocessor.py</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _filter_tables_containing_page_footer(<span class="va">self</span>, clusters: List[Cluster], </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                                          min_area_ratio: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.70</span>, </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                                          min_cells_threshold: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                                          min_density_threshold: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.001</span>) <span class="op">-&gt;</span> List[Cluster]:</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Enhanced logic to avoid reclassifying legitimate large tables.</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    large_labels <span class="op">=</span> {DocItemLabel.TABLE, DocItemLabel.DOCUMENT_INDEX, </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>                    DocItemLabel.KEY_VALUE_REGION, DocItemLabel.FORM}</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    page_area <span class="op">=</span> <span class="va">self</span>.page_size.width <span class="op">*</span> <span class="va">self</span>.page_size.height</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    reclassified_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cluster <span class="kw">in</span> clusters:</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cluster.label <span class="kw">in</span> large_labels:</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            cluster_area_ratio <span class="op">=</span> cluster.bbox.area() <span class="op">/</span> page_area</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> cluster_area_ratio <span class="op">&gt;=</span> min_area_ratio:</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>                all_cells <span class="op">=</span> <span class="va">self</span>._collect_all_cells(cluster)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>                cell_density <span class="op">=</span> <span class="bu">len</span>(all_cells) <span class="op">/</span> cluster.bbox.area()</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Reclassify if the cluster is large but sparsely populated with text cells</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(all_cells) <span class="op">&lt;</span> min_cells_threshold <span class="kw">or</span> cell_density <span class="op">&lt;</span> min_density_threshold:</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>                    cluster.label <span class="op">=</span> DocItemLabel.TEXT</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>                    reclassified_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> clusters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The algorithm operates on a set of carefully tuned heuristics to distinguish between a legitimate, large table and a misclassified text block:</p>
<ol type="1">
<li><p><strong>Target Selection:</strong> The function first identifies any cluster labeled as a <em>TABLE</em> (or other large “wrapper” types like <em>FORM</em>) for inspection.</p></li>
<li><p><strong>Area Threshold:</strong> It only considers clusters that are exceptionally large, occupying a significant portion of the page (e.g., <em>min_area_ratio</em> of 70%). This prevents the algorithm from affecting smaller, legitimate tables.</p></li>
<li><p><strong>Cell Count &amp; Density Check:</strong> This is the core heuristic. It recursively collects all the individual text cells within the large cluster and calculates two metrics: the absolute number of cells and the “cell density” (number of cells per unit of bounding box area).</p></li>
<li><p><strong>Re-classification Logic:</strong> If a cluster is very large but contains a low number of text cells or has a very low cell density, it is highly unlikely to be a real table. In such cases, the algorithm re-classifies the cluster’s label from <em>TABLE</em> to <em>TEXT</em>.</p></li>
</ol>
<p>By adding this function to the postprocess pipeline, the system can now correctly identify and relabel these large, sparse, misclassified text blocks, preserving the semantic integrity of the page structure as demonstrated by the corrected output in <a href="#fig-table-correction" class="quarto-xref">Figure&nbsp;<span>7.9</span></a>.</p>
<div id="fig-table-correction" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-table-correction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/utility_postprocessed_layout_page_00003_after.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-table-correction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.9: Table Correction
</figcaption>
</figure>
</div>
</section>
<section id="pre-emptive-filtering-of-page-level-artifacts" class="level4" data-number="7.1.5.9">
<h4 data-number="7.1.5.9" class="anchored" data-anchor-id="pre-emptive-filtering-of-page-level-artifacts"><span class="header-section-number">7.1.5.9</span> Pre-emptive Filtering of Page-Level Artifacts</h4>
<p>In addition to the post-processing of layout clusters, further enhancements were engineered upstream in the <em>docling/backend/docling_parse_v4_backend.py</em> module to improve the initial quality of the data fed to the layout analysis model. This module serves as a low-level backend that interacts directly with the <em>docling-parse</em> C++ library, which is responsible for the initial extraction of raw text cells and their coordinates from the PDF document. It was observed that certain document formats, particularly pre-prints, often contain page-level artifacts like line numbers in the margins. These structured, non-substantive elements were found to frequently mislead the layout model, leading to catastrophic classification errors where an entire page of text would be misidentified as a single, large <em>TABLE</em> cluster.</p>
<p>To mitigate this failure mode, the primary modification was made to the <em>get_text_cells</em> method within the <em>DoclingParseV4PageBackend</em> class. The original implementation of this method simply transformed the coordinate system of all extracted text cells and returned the complete, unfiltered list. The enhanced version introduces a new helper method, *_is_left_margin_line_number*, which applies a series of geometric heuristics to identify and pre-emptively filter these line-number artifacts before they are passed to the layout model.</p>
<p>The heuristic function determines if a given text cell is a line number by evaluating three specific spatial properties. First, it confirms the cell is located within a narrow vertical band on the far-left of the page, defined by a <em>LEFT_MARGIN_THRESHOLD</em> (e.g., the leftmost 8% of the page width). Second, it verifies that the cell’s bounding box width is less than this <em>MAX_WIDTH_THRESHOLD</em>, characteristic of short numerical strings. Finally, it ensures the cell has a reasonable height via <em>MIN_HEIGHT_THRESHOLD</em> to avoid incorrectly filtering other small page markings like footnote symbols. A cell is only flagged and removed if it satisfies all three conditions. By filtering the cell list with this function, the layout model receives a much cleaner representation of the page’s semantic content, significantly reducing classification errors and preserving the integrity of the document structure for all downstream processing.</p>
<p><strong>Original Implementation (get_text_cells)</strong></p>
<div id="text-cells-og" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_text_cells(<span class="va">self</span>) <span class="op">-&gt;</span> Iterable[TextCell]:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    page_size <span class="op">=</span> <span class="va">self</span>.get_size()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Applies coordinate transformation but returns all cells</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    [tc.to_top_left_origin(page_size.height) <span class="cf">for</span> tc <span class="kw">in</span> <span class="va">self</span>._dpage.textline_cells]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._dpage.textline_cells</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Modified Implementation with Heuristic Filtering</strong></p>
<div id="text-cells-mod" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_text_cells(<span class="va">self</span>) <span class="op">-&gt;</span> Iterable[TextCell]:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    page_size <span class="op">=</span> <span class="va">self</span>.get_size()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tc <span class="kw">in</span> <span class="va">self</span>._dpage.textline_cells:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        tc.to_top_left_origin(page_size.height)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter out cells identified as line numbers by the heuristic</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    filtered_cells <span class="op">=</span> [</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        cell <span class="cf">for</span> cell <span class="kw">in</span> <span class="va">self</span>._dpage.textline_cells</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>._is_left_margin_line_number(cell, page_size)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> filtered_cells</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _is_left_margin_line_number(<span class="va">self</span>, cell: TextCell, page_size: Size) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Identifies if a cell is a line number based on geometric properties.</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    LEFT_MARGIN_THRESHOLD <span class="op">=</span> <span class="fl">0.08</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    MIN_HEIGHT_THRESHOLD <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    MAX_WIDTH_THRESHOLD <span class="op">=</span> page_size.width <span class="op">*</span> LEFT_MARGIN_THRESHOLD</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    bbox <span class="op">=</span> cell.rect.to_bounding_box()</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    is_within_left_margin <span class="op">=</span> bbox.l <span class="op">&lt;</span> MAX_WIDTH_THRESHOLD</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    is_small_horizontal <span class="op">=</span> bbox.width <span class="op">&lt;</span> MAX_WIDTH_THRESHOLD</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    is_not_too_short <span class="op">=</span> bbox.height <span class="op">&gt;=</span> MIN_HEIGHT_THRESHOLD <span class="co"># Corrected logic</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> is_within_left_margin <span class="kw">and</span> is_small_horizontal <span class="kw">and</span> is_not_too_short</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="refinements-to-the-underlying-layout-predictor" class="level4" data-number="7.1.5.10">
<h4 data-number="7.1.5.10" class="anchored" data-anchor-id="refinements-to-the-underlying-layout-predictor"><span class="header-section-number">7.1.5.10</span> Refinements to the Underlying Layout Predictor</h4>
<p>Complementing the logical post-processing of layout clusters, targeted modifications were also made to the underlying <em>docling_ibm_models/layoutmodel/layout_predictor.py</em> module. This module is responsible for the initial, low-level detection of layout elements on a page image by executing the <em>RTDetrForObjectDetection model</em>. While the primary logic of the model itself was not altered, key refinements were made to its data handling and execution to ensure high-performance and stable operation within the custom parallelized framework.</p>
<p>The most critical modification addresses a performance bottleneck related to hardware utilization. In the <em>predict</em> method, the <em>target_sizes</em> tensor, which is required by the <em>post_process_object_detection</em> function to correctly rescale bounding boxes to the original image dimensions was originally created on the CPU by default.</p>
<div id="original-version" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.inference_mode</span>()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(<span class="va">self</span>, orig_img: Union[Image.Image, np.ndarray]) <span class="op">-&gt;</span> Iterable[<span class="bu">dict</span>]:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> <span class="va">self</span>._image_processor.post_process_object_detection(</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        outputs,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        target_sizes<span class="op">=</span>torch.tensor([page_img.size[::<span class="op">-</span><span class="dv">1</span>]]), <span class="co"># Tensor created on CPU by default</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        threshold<span class="op">=</span><span class="va">self</span>._threshold,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    w, h <span class="op">=</span> page_img.size</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> results[<span class="dv">0</span>]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> score, label_id, box <span class="kw">in</span> <span class="bu">zip</span>(result[<span class="st">"scores"</span>], result[<span class="st">"labels"</span>], result[<span class="st">"boxes"</span>]):</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Manual clamping of coordinates</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> <span class="bu">min</span>(w, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">0</span>]))</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="bu">min</span>(h, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">1</span>]))</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> <span class="bu">min</span>(w, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">2</span>]))</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> <span class="bu">min</span>(h, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">3</span>]))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> { <span class="co"># ...</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>When the model was running on a GPU, this mismatch forced an expensive and unnecessary cross-device data transfer during post-processing. The implementation was corrected to explicitly create this tensor on the same device as the model <em>(device=self._device)</em>, eliminating the synchronization penalty and ensuring more efficient CUDA utilization.</p>
<div id="modified-version" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.inference_mode</span>()</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(<span class="va">self</span>, orig_img: Union[Image.Image, np.ndarray]) <span class="op">-&gt;</span> Iterable[<span class="bu">dict</span>]:</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Explicitly create the tensor on the same device as the model (e.g., CUDA)</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    target_sizes <span class="op">=</span> torch.tensor([[original_height, original_width]], device<span class="op">=</span><span class="va">self</span>._device)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> <span class="va">self</span>._image_processor.post_process_object_detection(</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        outputs, target_sizes<span class="op">=</span>target_sizes, threshold<span class="op">=</span><span class="va">self</span>._threshold</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> results[<span class="dv">0</span>]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Directly use the results after converting to NumPy arrays</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> result[<span class="st">"boxes"</span>].cpu().numpy()</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> result[<span class="st">"scores"</span>].cpu().numpy()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> result[<span class="st">"labels"</span>].cpu().numpy()</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> score, label_id, box <span class="kw">in</span> <span class="bu">zip</span>(scores, labels, boxes):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># No more manual clamping</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        l, t, r, b <span class="op">=</span> box</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> { <span class="co"># ...</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Additionally, the code was modernized by removing redundant manual logic for clamping bounding box coordinates. The original implementation manually ensured that the box coordinates did not exceed the page dimensions after being returned by the post-processing function. The refined version removes this step, properly delegating the responsibility for coordinate clamping to the <em>transformers</em> library’s <em>post_process_object_detection</em> function. This change makes the code cleaner, more maintainable, and adheres more closely to the intended use of the library’s API. While subtle, these enhancements to the core predictor were essential for ensuring the stability and high throughput required by the system’s parallel architecture.</p>
</section>
<section id="final-implementation-in-the-etl-pipeline" class="level4" data-number="7.1.5.11">
<h4 data-number="7.1.5.11" class="anchored" data-anchor-id="final-implementation-in-the-etl-pipeline"><span class="header-section-number">7.1.5.11</span> Final Implementation in the ETL Pipeline</h4>
<p>The re-engineered Docling system is integrated as a distinct stage in the ETL pipeline, orchestrated by the do_docling_extraction function.</p>
<div id="docling-extraction-orchestrator" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/docling_multi_mp_gui.py</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> do_docling_extraction(df: pd.DataFrame, progress_callback<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Processes DataFrame rows in parallel using config settings.</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> MAX_WORKERS_DOCLING </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    num_records <span class="op">=</span> <span class="bu">len</span>(df)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the worker count from config, ensuring it's at least 1</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    max_workers_to_use <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, MAX_WORKERS_DOCLING)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    output_cols <span class="op">=</span> [<span class="st">"FullText"</span>, <span class="st">"TablesJson"</span>, <span class="st">"EquationsJson"</span>, <span class="st">"TokenCount"</span>, <span class="st">"Error"</span>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> output_cols:</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> col <span class="kw">not</span> <span class="kw">in</span> df.columns:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> col <span class="op">==</span> <span class="st">"TokenCount"</span>: df[col] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> col <span class="op">==</span> <span class="st">"Error"</span>: df[col] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>: df[col] <span class="op">=</span> <span class="st">""</span> <span class="co"># Default to empty string for text/JSON string columns</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    futures <span class="op">=</span> {}</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    processed_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> ProcessPoolExecutor(max_workers<span class="op">=</span>max_workers_to_use, initializer<span class="op">=</span>worker_initializer) <span class="im">as</span> executor:</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, row <span class="kw">in</span> df.iterrows():</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            pdf_path <span class="op">=</span> row.get(<span class="st">"PDFPath"</span>, <span class="st">""</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> pdf_path <span class="kw">or</span> <span class="kw">not</span> os.path.exists(pdf_path):</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>                df.loc[idx, output_cols] <span class="op">=</span> [<span class="st">"PDF_PATH_ERROR"</span>, <span class="st">"[]"</span>, <span class="st">"[]"</span>, <span class="dv">0</span>, <span class="st">"PDF_PATH_ERROR"</span>]</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>                processed_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            future <span class="op">=</span> executor.submit(extract_pdf_with_docling, pdf_path)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>            futures[future] <span class="op">=</span> idx <span class="co"># Map future to DataFrame index</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process results as they complete</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> future <span class="kw">in</span> as_completed(futures):</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            irow <span class="op">=</span> futures[future] <span class="co"># Get the original DataFrame index</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>                result <span class="op">=</span> future.result() <span class="co"># Get the dict returned by worker</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Update DataFrame using .loc with index 'irow'</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> col <span class="kw">in</span> output_cols:</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>                    df.loc[irow, col] <span class="op">=</span> result.get(col)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                df.loc[irow, output_cols] <span class="op">=</span> [<span class="st">"FUTURE_ERROR"</span>, <span class="st">"[]"</span>, <span class="st">"[]"</span>, <span class="dv">0</span>, <span class="ss">f"FutureError: </span><span class="sc">{</span><span class="bu">type</span>(e)<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>]</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This function receives a pandas DataFrame containing paths to the downloaded PDF articles. It distributes the processing of each PDF to the pool of GPU-powered workers. Each worker executes the extract_pdf_with_docling function, which initializes a dedicated DocumentConverter instance configured for its assigned GPU. The worker processes its assigned PDF, extracts the full text (exported as Markdown to preserve structure), and specifically identifies and serializes all tabular data and mathematical formulas into JSON arrays. Upon completion, the structured outputs, FullText, TablesJson, EquationsJson, along with a TokenCount are returned to the main process and integrated back into the corresponding row of the DataFrame. The resulting enriched DataFrame is then saved as a new artifact (output.feather), ready for the subsequent LLM-based field extraction stage.</p>
<hr>
</section>
</section>
<section id="stage-5-structured-field-extraction-using-large-language-models" class="level3" data-number="7.1.6">
<h3 data-number="7.1.6" class="anchored" data-anchor-id="stage-5-structured-field-extraction-using-large-language-models"><span class="header-section-number">7.1.6</span> Stage 5: Structured Field Extraction using Large Language Models</h3>
<p>Following the extraction of raw text via document layout analysis, the pipeline proceeds to the structured field extraction stage, orchestrated by the <em>scripts/field_extraction.py</em> module. The objective of this component is to parse the unstructured text from each article and populate a predefined set of structured data fields. This process transforms dense, narrative content into a queryable, machine-readable format suitable for populating the knowledge graph and facilitating systematic analysis.</p>
<section id="methodology-iterative-context-aware-extraction-with-local-llms" class="level4" data-number="7.1.6.1">
<h4 data-number="7.1.6.1" class="anchored" data-anchor-id="methodology-iterative-context-aware-extraction-with-local-llms"><span class="header-section-number">7.1.6.1</span> Methodology: Iterative, Context-Aware Extraction with Local LLMs</h4>
<p>The system employs a sophisticated extraction strategy centered around a locally deployed Large Language Model <strong>(Qwen/Qwen3-32B)</strong>. This self-hosted approach, leveraging the project’s multi-GPU hardware, ensures data privacy, eliminates reliance on external API costs and latency, and provides greater control over the inference process. The extraction is not a single pass; rather, it is an iterative process designed to build a rich, cumulative record for each document.</p>
<p>The core of the methodology is the *_process_one_row* function, which operates on each article’s full text. To manage the extensive length of academic papers and stay within the LLM’s context window, the <em>chunk_text_for_extraction</em> utility first splits the full text into overlapping chunks of a configured token size (e.g., 8000 tokens with a 500-token overlap). The system then iterates through these chunks, performing an LLM call for each one.</p>
<p>A key innovation is the use of a <strong>dynamic, context-aware prompt</strong>. For each chunk, the LLM is provided not only with the text of that chunk but also with the current state of the metadata that has been extracted from all previous chunks of the same document. This allows the model to incrementally enrich the data, fill in missing fields, and use previously extracted information as context for interpreting the current text chunk. The prompt dynamically generates a list of fields to be extracted based on the <em>SELECTED_COLS</em> defined in the <em>etl_config.json</em> file, which are interactively selected via the GUI. Crucially, rather than just providing the field name (e.g., “LCA System Boundaries”), the prompt includes a detailed explanation from the <em>FIELD_EXPLANATIONS</em> dictionary, guiding the LLM with a precise definition of the information to look for (e.g., “Describe the scope and boundaries of any Life Cycle Assessment (LCA) mentioned, such as cradle-to-grave, cradle-to-gate…”).</p>
<div id="field-extraction-prompt-logic" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/field_extraction.py, illustrating dynamic prompt generation</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... inside _process_one_row ...</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># The current state of extracted data is passed as context</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>metadata_context_json <span class="op">=</span> json.dumps(final_merged_data, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The prompt is formatted with the context, the new text chunk,</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># and the list of fields with their detailed explanations.</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> extraction_template.<span class="bu">format</span>(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    metadata_json<span class="op">=</span>metadata_context_json,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    chunk_text<span class="op">=</span>chunk_txt,</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    field_list_placeholder<span class="op">=</span>field_list_str</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ... LLM call is made with this enhanced prompt ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="parallelized-execution-and-data-merging" class="level4" data-number="7.1.6.2">
<h4 data-number="7.1.6.2" class="anchored" data-anchor-id="parallelized-execution-and-data-merging"><span class="header-section-number">7.1.6.2</span> Parallelized Execution and Data Merging</h4>
<p>To handle the processing of hundreds of articles efficiently, the entire operation is parallelized. The <em>extract_additional_fields</em> function uses a <em>ThreadPoolExecutor</em> to process multiple articles (rows in the DataFrame) concurrently. Within each of these threads, a nested <em>ThreadPoolExecutor</em> is used to process the individual text chunks for that article in parallel. This multi-level parallelism ensures maximum utilization of the available hardware.</p>
<p>The output from each LLM call is a JSON object containing only the fields found within that specific text chunk. A robust merging function, <em>unify_fields</em>, is then used to intelligently integrate this new data into the master record for the article. This function handles various data types, uniquely appending items to list-based fields (e.g., Pollutant Terms), overwriting simple text fields with more specific information, and carefully aggregating complex structured data like the Metrics field. To prevent performance degradation from repeated LLM calls on identical text, a cache <strong>(llm_response_cache)</strong> stores the output for each unique prompt, returning the cached result if the same text chunk is ever processed again. This entire process, from chunking to parallelized inference and intelligent merging, results in a richly detailed and structured dataset, which is then saved as an enriched Feather file, ready for the final stages of the ETL pipeline.</p>
<hr>
</section>
</section>
<section id="stage-6-thematic-analysis-via-topic-modeling" class="level3" data-number="7.1.7">
<h3 data-number="7.1.7" class="anchored" data-anchor-id="stage-6-thematic-analysis-via-topic-modeling"><span class="header-section-number">7.1.7</span> Stage 6: Thematic Analysis via Topic Modeling</h3>
<p>Following the extraction of structured data, the pipeline performs a thematic analysis of the textual corpus to identify latent topics, which serves as a primary method for systematic research gap analysis. This is achieved through the implementation of a configurable and parallelized Latent Dirichlet Allocation (LDA) topic modeling workflow, orchestrated by the scripts/topic_modeling_gui.py module. The objective is to distill the vast, unstructured text from hundreds of articles into a set of coherent, interpretable topics whose prevalence and relationships can be quantitatively assessed.</p>
<section id="methodology-probabilistic-topic-modeling-with-lda" class="level4" data-number="7.1.7.1">
<h4 data-number="7.1.7.1" class="anchored" data-anchor-id="methodology-probabilistic-topic-modeling-with-lda"><span class="header-section-number">7.1.7.1</span> Methodology: Probabilistic Topic Modeling with LDA</h4>
<p>The core of this stage is <strong>Latent Dirichlet Allocation (LDA)</strong>, a generative probabilistic model for discrete data. The foundational assumption of LDA is that each document in a corpus is a mixture of various topics, and each topic is a distribution of words. The model does not know what the topics are in advance; it learns them by analyzing the patterns of word co-occurrence across the entire set of documents.</p>
<p>Mathematically, LDA models a document as being generated by the following process:</p>
<ol type="1">
<li>For each document <span class="math inline">\((d)\)</span> in the corpus <span class="math inline">\((D)\)</span>, choose a distribution over topics <span class="math inline">\(( \theta_d \sim \text{Dir}(\alpha))\)</span>.</li>
<li>For each word <span class="math inline">\((w_n)\)</span> in document <span class="math inline">\((d)\)</span>:
<ol type="a">
<li>Choose a topic <span class="math inline">\(( z_n \sim \text{Categorical}(\theta_d))\)</span>.</li>
<li>Choose a word <span class="math inline">\(( w_n )\)</span> from <span class="math inline">\((p(w_n \mid z_n, \beta))\)</span>, the probability of word <span class="math inline">\((w_n)\)</span> given topic <span class="math inline">\((z_n)\)</span>.</li>
</ol></li>
</ol>
<p>The goal of the model training is to infer the hidden variables: the topic distributions per document <span class="math inline">\((\theta_d)\)</span> and the word distributions per topic <span class="math inline">\((\beta_k)\)</span>. The system uses scikit-learn’s implementation of LDA, which employs a variational Bayes algorithm to approximate these posterior distributions. The output for each topic <span class="math inline">\((k)\)</span> is a probabilistic representation best described as a list of words that are most likely to belong to that topic.</p>
</section>
<section id="data-preprocessing-and-hyperparameter-optimization" class="level4" data-number="7.1.7.2">
<h4 data-number="7.1.7.2" class="anchored" data-anchor-id="data-preprocessing-and-hyperparameter-optimization"><span class="header-section-number">7.1.7.2</span> Data Preprocessing and Hyperparameter Optimization</h4>
<p>The quality of an LDA model is highly dependent on both the cleanliness of the input text and the choice of model hyperparameters. The pipeline therefore begins with a rigorous, parallelized text preprocessing workflow (<code>preprocess_pipeline</code> function) that tokenizes, removes stopwords, and lemmatizes the text from each document.</p>
<p>To select the optimal hyperparameters for the corpus, the system implements a parallelized grid search, orchestrated by the <code>_search_best_lda_params</code> function. The user can define the search space in the <code>etl_config.json</code> file interactively via the GUI, specifying ranges for key parameters:</p>
<ul>
<li><strong><code>num_topics</code> (k)</strong>: The number of latent topics to discover. The system intelligently searches over a dynamic range to find the optimal granularity.</li>
<li><strong><code>passes / iterations</code></strong>: The number of passes the algorithm makes over the corpus during training.</li>
<li><strong>n-gram Parameters (<code>bigram_threshold</code>, <code>trigram_threshold</code>)</strong>: These control the formation of common multi-word phrases (e.g., “cover crop,” “random forest model”), treating them as single tokens to generate more coherent topics.</li>
<li><strong>Dictionary Filtering (<code>no_below</code>, <code>no_above</code>)</strong>: These parameters prune the vocabulary by removing terms that are either too rare (<code>no_below</code>) or too common (<code>no_above</code>) to be thematically useful.</li>
</ul>
<p>The grid search trains multiple LDA models in parallel using <code>ProcessPoolExecutor</code>, and each resulting model is evaluated using a Topic Coherence score.</p>
</section>
<section id="model-evaluation-topic-coherence" class="level4" data-number="7.1.7.3">
<h4 data-number="7.1.7.3" class="anchored" data-anchor-id="model-evaluation-topic-coherence"><span class="header-section-number">7.1.7.3</span> Model Evaluation: Topic Coherence</h4>
<p>Topic coherence measures the degree of semantic similarity between the high-scoring words within a topic, providing a quantitative way to assess how interpretable and “human-like” a topic is. This system uses the <span class="math inline">\((C_v)\)</span> coherence measure, which is based on a sliding window and the normalized pointwise mutual information (NPMI) of word pairs.</p>
<p>For a set of top <span class="math inline">\((N)\)</span> words <span class="math inline">\((w_1, w_2, \dots, w_N)\)</span> in a given topic, the <span class="math inline">\((C_v)\)</span> score is calculated as the average NPMI of all unique word pairs:</p>
<p><span class="math display">\[
\text{Coherence}(V) = \frac{1}{\binom{N}{2}} \sum_{i=1}^{N-1} \sum_{j=i+1}^{N} \text{NPMI}(w_i, w_j)
\]</span></p>
<p>where the Normalized Pointwise Mutual Information (NPMI) is given by:</p>
<p><span class="math display">\[
\text{NPMI}(w_i, w_j) = \frac{\log \frac{P(w_i, w_j)}{P(w_i)P(w_j)}}{-\log P(w_i, w_j)}
\]</span></p>
<p>Here, <span class="math inline">\(P(w)\)</span> is the probability of seeing word <span class="math inline">\(w\)</span> in a document, and <span class="math inline">\(P(w_i, w_j)\)</span> is the probability of seeing both words in the same document. A score closer to 1 indicates a more coherent topic. The grid search selects the hyperparameter set that produces the model with the highest average <span class="math inline">\(C_v\)</span> score across all its topics. Additionally, the <code>calculate_topic_coherences</code> function is used to compute the coherence score for each individual topic from the best model, allowing for a granular assessment of the quality of each thematic cluster.</p>
</section>
<section id="interpretation-and-visualization" class="level4" data-number="7.1.7.4">
<h4 data-number="7.1.7.4" class="anchored" data-anchor-id="interpretation-and-visualization"><span class="header-section-number">7.1.7.4</span> Interpretation and Visualization</h4>
<p>After the best model is identified, the system assigns a dominant topic to each document and uses an LLM (<code>Qwen/Qwen3-32B</code>) to generate a concise, human-readable label for each topic based on its top keywords. Finally, the <code>generate_lda_visualization.py</code> script leverages the <code>pyLDAvis</code> library to create an interactive visualization. This plot as shown, maps the topics into a 2D space, where the size of each topic’s circle represents its prevalence in the corpus and the distance between circles indicates their semantic dissimilarity. This provides a powerful visual tool for identifying prevalent themes and discovering potentially under-represented research areas.</p>
<iframe src="visualizations/lda_vis_custom_labels.html" width="100%" height="800px" style="border:none;">
</iframe>
<hr>
</section>
</section>
<section id="stage-7-the-semantic-unification-pipeline" class="level3" data-number="7.1.8">
<h3 data-number="7.1.8" class="anchored" data-anchor-id="stage-7-the-semantic-unification-pipeline"><span class="header-section-number">7.1.8</span> Stage 7: The Semantic Unification Pipeline</h3>
<p>This section details the methodology used to resolve terminological ambiguity within the data extracted by the ETL pipeline, ensuring ontological consistency before ingestion into the knowledge graph. This core functionality is encapsulated within the <code>scripts/dictionaries_gui.py</code> module.</p>
<section id="the-challenge-of-terminological-heterogeneity-in-automated-extraction" class="level4" data-number="7.1.8.1">
<h4 data-number="7.1.8.1" class="anchored" data-anchor-id="the-challenge-of-terminological-heterogeneity-in-automated-extraction"><span class="header-section-number">7.1.8.1</span> The Challenge of Terminological Heterogeneity in Automated Extraction</h4>
<p>A primary challenge in the automated processing of scientific literature arises from terminological heterogeneity. The field_extraction.py module, which leverages a Large Language Model (LLM), extracts unstructured textual strings that often exhibit significant variation despite referring to the same underlying concept. For example, within the “Tillage Practices” field, semantically equivalent concepts may be described as “no-till,” “zero tillage,” or “direct drilling.” To construct a coherent and queryable knowledge graph, as orchestrated by kg_pipeline_gui.py, it is imperative that these synonymous variations are resolved into a single, canonical entity. The system addresses this challenge not through fragile string matching or rule-based heuristics, but through a robust process of semantic unification, which mathematically quantifies the contextual meaning of extracted terms. This core functionality is encapsulated within the scripts/dictionaries_gui.py module.</p>
</section>
<section id="a-vector-space-model-for-semantic-representation" class="level4" data-number="7.1.8.2">
<h4 data-number="7.1.8.2" class="anchored" data-anchor-id="a-vector-space-model-for-semantic-representation"><span class="header-section-number">7.1.8.2</span> A Vector Space Model for Semantic Representation</h4>
<p>The foundational theory underpinning the unification process is the representation of language within a high-dimensional vector space, often referred to as word or phrase embedding. This approach posits that the meaning of a term can be captured by a dense numerical vector, where terms with similar meanings are located closer to each other in this geometric space. This system employs a pre-trained Sentence Transformer model (sentence-transformers/all-MiniLM-L6-v2) to perform this transformation. This specific model generates a 384-dimensional vector for any given textual input, effectively mapping each term to a unique coordinate in a 384-dimensional semantic space.</p>
</section>
<section id="pre-computation-of-the-canonical-knowledge-base" class="level4" data-number="7.1.8.3">
<h4 data-number="7.1.8.3" class="anchored" data-anchor-id="pre-computation-of-the-canonical-knowledge-base"><span class="header-section-number">7.1.8.3</span> Pre-computation of the Canonical Knowledge Base</h4>
<p>The unification process is implemented through a sequence of operations designed for both accuracy and computational efficiency. At pipeline initialization, the precompute_all function within dictionaries_gui.py is executed.</p>
<div id="precompute-all" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> precompute_all(config_path<span class="op">=</span><span class="st">"dictionaries_config.json"</span>, device: <span class="bu">str</span> <span class="op">=</span> DEVICE_STR):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Loads synonym dicts from config, then builds embeddings for each.</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> _precomputed, _synonym_dictionaries, _precomputed_embeddings <span class="co"># Added _precomputed_embeddings</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> _precomputed: <span class="cf">return</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load synonym dicts first</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    _synonym_dictionaries <span class="op">=</span> load_synonym_dictionaries(config_path) <span class="co"># Use the loading function</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> _synonym_dictionaries <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>         <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Synonym dictionaries failed to load."</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> get_st_model(device<span class="op">=</span>device)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the dictionary to store computed embeddings</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    _precomputed_embeddings <span class="op">=</span> {}</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f"Starting embedding precomputation for </span><span class="sc">{</span><span class="bu">len</span>(_synonym_dictionaries)<span class="sc">}</span><span class="ss"> dictionaries..."</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamically build embeddings based on loaded dicts</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> dict_key, syn_dict <span class="kw">in</span> _synonym_dictionaries.items():</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f"Processing dictionary: </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> syn_dict: <span class="co"># Check if dictionary is not empty</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>            phrases, embeds, label_map <span class="op">=</span> build_candidate_embeddings(syn_dict, model<span class="op">=</span>model, device<span class="op">=</span>device)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> embeds <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="co"># Check if embedding generation was successful</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>                 _precomputed_embeddings[dict_key] <span class="op">=</span> {</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"phrases"</span>: phrases,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"embeds"</span>: embeds.to(device), <span class="co"># Ensure embeddings are on the correct device</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"label_map"</span>: label_map</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>                 }</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>                 logger.info(<span class="ss">f"Finished </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span><span class="bu">len</span>(phrases)<span class="sc">}</span><span class="ss"> phrases, embedding shape: </span><span class="sc">{</span>embeds<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>                 logger.error(<span class="ss">f"Embedding generation failed for dictionary: </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>             logger.warning(<span class="ss">f"Skipping empty synonym dictionary: </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>    _precomputed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f"All configured synonym embeddings pre-computed and stored on device '</span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">'."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This function iterates through the synonym dictionaries defined in config/dictionaries_config.json. For each dictionary, such as TILLAGE_PRACTICES_SYNONYMS or ML_AI_METHODS_SYNONYMS, it compiles a comprehensive list of all canonical terms and their associated synonyms. Each of these phrases is then passed through the Sentence Transformer model to generate its corresponding 384-dimensional embedding. The resulting collection of vectors and a mapping that links each vector back to its canonical parent term are cached in memory. This pre-computation creates a static, numerically-indexed semantic map of the entire known vocabulary for each category, which allows subsequent matching operations to be performed as highly efficient numerical comparisons, rather than repeated, computationally expensive model inferences.</p>
</section>
<section id="quantifying-semantic-similarity-the-cosine-similarity-metric" class="level4" data-number="7.1.8.4">
<h4 data-number="7.1.8.4" class="anchored" data-anchor-id="quantifying-semantic-similarity-the-cosine-similarity-metric"><span class="header-section-number">7.1.8.4</span> Quantifying Semantic Similarity: The Cosine Similarity Metric</h4>
<p>Once the field_extraction.py module provides a new, unstructured term for a given field (e.g., “conservation tillage” for the “Tillage Practices” category), the term is first converted into a 384-dimensional query vector, <span class="math inline">\(q\)</span> . The _find_best_match function then systematically compares this query vector to every pre-computed candidate vector, <span class="math inline">\(c\)</span>, within the relevant semantic map.</p>
<div id="best-match" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _find_best_match(query_emb, dict_key):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Internal helper to find best match using precomputed embeddings."""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> _precomputed_embeddings <span class="co"># Use the dict storing computed data</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> _precomputed:</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        logger.error(<span class="ss">f"Embeddings not precomputed. Cannot unify for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">'. Call precompute_all() first."</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># could trigger precomputation here, but better to do it explicitly at startup</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># precompute_all()</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if not _precomputed: # Check again if precomputation failed</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span> </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dict_key <span class="kw">not</span> <span class="kw">in</span> _precomputed_embeddings:</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        logger.warning(<span class="ss">f"No precomputed embeddings found for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> _precomputed_embeddings[dict_key]</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    embeds <span class="op">=</span> data.get(<span class="st">"embeds"</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    label_map <span class="op">=</span> data.get(<span class="st">"label_map"</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    phrases <span class="op">=</span> data.get(<span class="st">"phrases"</span>) <span class="co"># For logger</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> embeds <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> label_map <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> phrases <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>         logger.error(<span class="ss">f"Precomputed data is incomplete for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>         <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        <span class="im">from</span> sentence_transformers <span class="im">import</span> util</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> util.cos_sim(query_emb, embeds)[<span class="dv">0</span>]</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> <span class="bu">float</span>(scores.<span class="bu">max</span>())</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        best_idx <span class="op">=</span> <span class="bu">int</span>(scores.argmax())</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> best_idx, best_score</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> sim_err:</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        logger.error(<span class="ss">f"Error during cosine similarity calculation for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>sim_err<span class="sc">}</span><span class="ss">"</span>, exc_info<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The metric used for this comparison is the Cosine Similarity, which measures the cosine of the angle (θ) between the two vectors and serves as a measure of their orientation and semantic alignment, independent of their magnitude. The formula for this calculation is:</p>
<p><span class="math display">\[
\operatorname{sim}(\mathbf q,\mathbf c)
  = \cos\theta
  = \frac{\mathbf q \!\cdot\! \mathbf c}
         {\lVert \mathbf q \rVert \,\lVert \mathbf c \rVert}
  = \frac{\displaystyle\sum_{i=1}^{n} q_i\,c_i}
         {\sqrt{\displaystyle\sum_{i=1}^{n} q_i^{2}}\,
          \sqrt{\displaystyle\sum_{i=1}^{n} c_i^{2}}}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the embedding dimension (384 for <strong>all-MiniLM-L6-v2</strong>);</li>
<li><span class="math inline">\(\mathbf q \in \mathbb R^{n}\)</span> is the 384-dimensional vector representing the new, unstructured query term;</li>
<li><span class="math inline">\(\mathbf c \in \mathbb R^{n}\)</span> is a 384-dimensional candidate vector from the pre-computed knowledge base;</li>
<li><span class="math inline">\(q_i\)</span> and <span class="math inline">\(c_i\)</span> are the <span class="math inline">\(i\)</span>-th components of the query and candidate vectors, respectively.</li>
</ul>
<p>The cosine similarity ranges from <span class="math inline">\(-1\)</span> (opposite) to <span class="math inline">\(1\)</span> (identical), where <span class="math inline">\(1\)</span> signifies that the vectors point in the exact same direction (a perfect semantic match), <span class="math inline">\(0\)</span> indicates they are orthogonal (semantically unrelated), and <span class="math inline">\(-1\)</span> indicates they are diametrically opposed.</p>
</section>
<section id="threshold-based-mapping-for-ontological-consistency" class="level4" data-number="7.1.8.5">
<h4 data-number="7.1.8.5" class="anchored" data-anchor-id="threshold-based-mapping-for-ontological-consistency"><span class="header-section-number">7.1.8.5</span> Threshold-Based Mapping for Ontological Consistency</h4>
<p>After calculating the similarity score between the query vector and all candidate vectors, the system identifies the maximum score, representing the “closest” known term in the semantic space. The final step of the unification process is to transform this probabilistic similarity score into a deterministic mapping. This is achieved by comparing the highest score against a pre-defined confidence threshold (e.g., 0.55). If the score is greater than or equal to this threshold, the match is accepted as valid, and the system uses the pre-computed label_map to retrieve the canonical term associated with the best-matching vector. This canonical term is then used for ingestion into the knowledge graph. If the highest score falls below the threshold, the query term is considered a non-match, ensuring that ambiguous or out-of-domain terms are rejected, thereby safeguarding the ontological integrity and consistency of the final knowledge graph.</p>
</section>
<section id="knowledge-graph-integration" class="level4" data-number="7.1.8.6">
<h4 data-number="7.1.8.6" class="anchored" data-anchor-id="knowledge-graph-integration"><span class="header-section-number">7.1.8.6</span> Knowledge Graph Integration</h4>
<p>The final stage of the ETL pipeline transforms the processed and enriched tabular data into a highly interconnected knowledge graph using a Neo4j graph database. The primary objective is to model the extracted metadata not as isolated rows, but as a network of distinct entities (such as articles, authors, diseases, and methods) and the explicit relationships that connect them. This graph structure enables complex, multi-faceted queries that would be inefficient or impossible with standard relational tables. The entire process, from data preparation to graph ingestion, is orchestrated by scripts/kg_pipeline_gui.py and is driven by a declarative schema defined in the config/kg_pipeline.json file.</p>
</section>
<section id="dynamic-graph-construction" class="level4" data-number="7.1.8.7">
<h4 data-number="7.1.8.7" class="anchored" data-anchor-id="dynamic-graph-construction"><span class="header-section-number">7.1.8.7</span> Dynamic Graph Construction</h4>
<p>The construction of the knowledge graph is a dynamic, two-step process for each article processed by the pipeline:</p>
<ol type="1">
<li><p><strong>Article Node Creation:</strong> The process begins by creating or merging a central :Article node, using the article’s Digital Object Identifier (DOI) as its unique key. All primary metadata, such as the title, citation count, Zotero key, and the results of the topic modeling stage, are set as properties on this main node.</p></li>
<li><p><strong>Entity and Relationship Mapping</strong> The create_or_update_kg function then iterates through the field_mappings defined in the configuration file. For each field in the dataset (e.g., ml_methods_used), the system dynamically creates MERGE statements in the Cypher query language. For instance, for an article that used “Random Forest”, the system ensures a node (:MLMethod {name: ‘Random Forest’}) exists and then creates a [:USES_ML_METHOD] relationship from the :Article node to it. This procedure is applied across all mapped fields, creating a rich network of interconnected entities such as :HeartDisease, :PollutantTerm, and :StudyType, as visualized in the graph schema (<a href="#fig-graph-query-example" class="quarto-xref">Figure&nbsp;<span>7.10</span></a>).</p></li>
</ol>
<div id="fig-graph-query-example" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graph-query-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/nodes_relationships.PNG" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graph-query-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.10: Query example
</figcaption>
</figure>
</div>
</section>
<section id="querying-for-methodological-insights" class="level4" data-number="7.1.8.8">
<h4 data-number="7.1.8.8" class="anchored" data-anchor-id="querying-for-methodological-insights"><span class="header-section-number">7.1.8.8</span> Querying for Methodological Insights</h4>
<p>This graph-based structure allows for powerful analytical queries to uncover trends in the literature. For example, to identify the most frequently used machine learning methods specifically within the subset of empirical studies examining ozone and heart disease, the following Cypher query is executed. This query traverses multiple relationship types to precisely filter and aggregate the data, a task well-suited to a graph database.</p>
<pre class="{cypher}"><code>MATCH (a:Article)
// Collect study type names per article
OPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)
WITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames
// Filter for empirical studies only (exclude non-empirical types)
WHERE SIZE(studyTypeNames) &gt; 0
  AND NONE(stName IN studyTypeNames WHERE stName IN [
    'review', 'systematic review', 'meta-analysis', 'expert opinion',
    'scoping review', 'dissertation/thesis', 'short communication',
    'methodological paper', 'theoretical study', 'report'
  ])
// Confirm articles explicitly have ozone, ML methods, and heart disease
AND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))
AND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))
AND EXISTS {
  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(oz:PollutantTerm)
  WHERE toLower(oz.name) CONTAINS 'ozone'
}
// Exclude comment/reply-type articles explicitly
AND NOT toLower(a.title) CONTAINS 'comment'
AND NOT toLower(a.title) CONTAINS 'reply'
// Retrieve ML methods and count their occurrences
MATCH (a)-[:USES_ML_METHOD]-&gt;(ml:MLMethod)
RETURN
    ml.name AS `ML Method`,
    COUNT(DISTINCT a.doi) AS `Number of Articles`
ORDER BY `Number of Articles` DESC</code></pre>
<pre><code>╒════════════════════════════════════╤══════════════════╕
│ML Method                           │Number of Articles│
╞════════════════════════════════════╪══════════════════╡
│"Random Forest"                     │20                │
├────────────────────────────────────┼──────────────────┤
│"Gradient Boosting Machines"        │14                │
├────────────────────────────────────┼──────────────────┤
│"Support Vector Machines"           │13                │
├────────────────────────────────────┼──────────────────┤
│"Neural Networks"                   │12                │
├────────────────────────────────────┼──────────────────┤
│"K-Means Clustering"                │9                 │
├────────────────────────────────────┼──────────────────┤
│"Recurrent Neural Networks"         │7                 │
├────────────────────────────────────┼──────────────────┤
│"Deep Neural Networks"              │7                 │
├────────────────────────────────────┼──────────────────┤
│"Convolutional Neural Networks"     │5                 │
├────────────────────────────────────┼──────────────────┤
│"Geographically Weighted Regression"│3                 │
├────────────────────────────────────┼──────────────────┤
│"Long Short-Term Memory Networks"   │2                 │
├────────────────────────────────────┼──────────────────┤
│"Hierarchical Clustering"           │2                 │
├────────────────────────────────────┼──────────────────┤
│"Markov Chain Monte Carlo"          │2                 │
├────────────────────────────────────┼──────────────────┤
│"Autoencoders"                      │2                 │
├────────────────────────────────────┼──────────────────┤
│"Prophet"                           │2                 │
├────────────────────────────────────┼──────────────────┤
│"Principal Component Analysis"      │1                 │
├────────────────────────────────────┼──────────────────┤
│"Extreme Learning Machines"         │1                 │
├────────────────────────────────────┼──────────────────┤
│"Named Entity Recognition"          │1                 │
└────────────────────────────────────┴──────────────────┘</code></pre>
<p>The execution of this query against the knowledge graph yields a clear distribution of methodologies employed across the 34 included studies. Tree-based ensemble methods are most prominent, with Random Forest being the predominant technique, utilized in 20 articles (58.8%). Gradient Boosting Machines and Support Vector Machines were also frequently applied, appearing in 14 and 13 studies, respectively. Neural network architectures showed considerable diversity, with the general category of Neural Networks reported in 12 studies, and more specific forms such as Recurrent and Deep Neural Networks each used in 7 studies. The data indicates a strong trend towards applying multiple ML techniques within a single study, with the 34 articles reporting 102 instances of ML method applications, averaging approximately three distinct methods per paper. This detailed, queryable insight into methodological trends is a direct result of structuring the extracted literature as a knowledge graph.</p>
<hr>
</section>
</section>
</section>
<section id="etl-pipeline-summary-and-output" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="etl-pipeline-summary-and-output"><span class="header-section-number">7.2</span> ETL Pipeline Summary and Output</h2>
<p>The sequential execution of these seven stages transforms a collection of raw, unstructured PDF documents into a highly structured, semantically unified, and queryable knowledge graph. The process is designed to be both robust and efficient, leveraging parallel processing for I/O-bound tasks like API calls and GPU-bound tasks like document parsing. Sophisticated, configurable heuristics are employed at multiple stages, from pre-emptive artifact filtering and layout post-processing to semantic unification to ensure the quality and consistency of the final data product.</p>
<p>The ultimate output of this ETL pipeline is a Neo4j graph database. This database serves as the foundational “non-parametric memory” for the downstream Retrieval-Augmented Generation (RAG) system. Each :Article node in the graph is not simply a bibliographic entry, it is a rich object, connected through explicit relationships to its authors, its key concepts, its methodologies, its findings, and its thematic context within the broader literature. It is this interconnected structure, built upon layers of meticulous data processing and enhancement, that enables the RAG agent to perform nuanced, context-aware queries and synthesize high-fidelity, traceable information to address complex research questions.</p>
<hr>
</section>
<section id="hysemrag-qa-framework-an-agentic-approach-for-verifiable-generation" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> HySemRAG-QA Framework: An Agentic Approach for Verifiable Generation</h1>
<p>With the creation of a structured knowledge graph, the pipeline transitions from data ingestion to its primary purpose of enabling a user to ask complex questions and receive accurate, synthesized, and fully verifiable answers. This is accomplished through the Hybrid Semantic Retrieval-Augmented Generation with Quality Assurance (HySemRAG-QA) framework, a system designed to overcome the critical limitations of standard RAG architectures, namely noisy retrieval and a propensity for LLM hallucination. The system’s architecture is founded on a multi-layered approach to ensure the trustworthiness of its final output, from initial query to final validation.</p>
<p>The core of the framework is designed to create a “chain of custody” for every piece of information. This begins with a hybrid retrieval engine that goes beyond simple vector search. It combines results from three distinct sources; semantic search (Qdrant), keyword search, and structured graph traversals (Neo4j), and uses Reciprocal Rank Fusion (RRF) to produce a single, highly relevant context.</p>
<p>This high-quality context is then passed to an agentic self-correction framework. This is not a single-pass generation process. It implements a multi-agent system where a primary “generator” LLM drafts a cited answer, and a secondary “QA agent” LLM audits that output for factual accuracy, logical consistency, and strict adherence to citation protocols. If the QA agent detects a flaw, it provides corrective feedback, and the generator is forced to revise its work in an iterative loop until a reasoned answer is produced.</p>
<p>Finally, to ensure absolute verifiability, the system performs a post-hoc audit. Every citation in the final, agent-approved answer is checked against the ground-truth database to confirm its provenance. As a byproduct of this multi-stage validation process, the system generates a comprehensive interaction log, creating a large-scale, high-quality preference dataset of (prompt, rejected_answer, chosen_answer) triplets suitable for AI model training and fine tuning. The following sections will detail the technical implementation of each of these modules.</p>
<hr>
<p>This page will be updated as drafting is complete</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-aim3.html" class="pagination-link" aria-label="Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link" aria-label="Summary">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>#06-technical-report.qmd</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu"># Technical Report</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="fu">## The ETL Pipeline: From Raw Literature to Structured Knowledge</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>This chapter will server as the technical report for the ETL+HySemRAG methodology and is a work in progress. This chapter will be updated once the draft is complete to describe all phases and theories of operation of the system.</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="al">![Graphical Abstract](figures/ETL_RAG.png)</span>{#fig-graph-abstract}</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="fu">### Corpus Size and Variability</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>The final corpus size in each ETL run is not fixed, rather emerges naturally based on the availability of open-access full-text PDFs via the Unpaywall API. Typically, from an initial candidate pool of approximately 3,400 metadata records retrieved from PubMed, OpenAlex, and Scopus, the number of successfully retrieved full-text documents ranges from around 500 to 1,300. This inherent variability reflects realistic constraints encountered during automated scholarly literature retrieval, specifically the reliance on open-access publications. Importantly, this variability underscores the pipeline’s robustness and adaptability, as subsequent processing stages maintain stable performance metrics (accuracy of data extraction, knowledge graph quality, semantic consistency) regardless of the exact corpus size. This adaptability positions the ETL pipeline as particularly suited to real-world research scenarios where comprehensive subscription-based access may not always be available.</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>**Demonstration Video**</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>Here is a video demonstrating the system's capabilities:</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>{{&lt; video "https://youtu.be/ZCy5ESJ1gVE?si=K8CttwgTj7yGrWjn" &gt;}}</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stage 1: Multi-Source Data Acquisition and Enrichment</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>The initial and most foundational stage of the ETL pipeline is the acquisition of a comprehensive, relevant, and clean corpus of scholarly literature. The objective of this phase is to systematically gather article metadata from multiple disparate sources, each with unique strengths, and then intelligently fuse them into a single, deduplicated, and enriched dataset. This process is orchestrated by a series of high-performance Python modules designed for scalability and robustness, ensuring that the subsequent stages of the pipeline operate on a high-quality foundation.</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Parallelized Metadata Fetching:</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>To achieve a comprehensive corpus, the system employs a multi-source fetching strategy, simultaneously querying three major scholarly databases: PubMed, for its depth in biomedical and life sciences literature; OpenAlex, for its broad multidisciplinary coverage and rich citation network data; and Elsevier's Scopus API, for its extensive abstract and citation database, particularly for STEM fields. The GUI allows the user to select any combination of these sources for a given run.</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>Recognizing that fetching metadata for thousands of articles via sequential API calls would present a significant performance bottleneck, the system was architected for high-throughput, concurrent data retrieval. The implementation is tailored to the specific architecture and rate-limiting policies of each API. For PubMed, the scripts/fast_pubmed.py module first performs a single, efficient esearch call to retrieve up to 10,000 PubMed IDs (PMIDs) matching a given query. Subsequently, it uses a ThreadPoolExecutor to create a pool of worker threads that fetch the full article metadata in parallel batches using efetch, while adhering to NCBI's polite API usage policy of approximately 3 queries per second. For OpenAlex and Scopus, the system leverages modern asynchronous I/O via the asyncio and aiohttp libraries within the scripts/fast_openalex.py and scripts/etl_elsevier.py modules. This approach allows the system to initiate hundreds of concurrent HTTP requests, managing them efficiently without being blocked by network latency. An asyncio.Semaphore is used as a throttling mechanism to ensure the number of requests per second does not exceed the official rate limits for each API, preventing errors and ensuring reliable access.</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Data Fusion and Deduplication:</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>Once the parallel fetching processes are complete, the metadata from each source, now held in separate pandas DataFrames, must be combined into a single, unified dataset. This is handled by the merge_and_deduplicate function within the scripts/etl_utils_refined.py module. The function first concatenates the list of DataFrames into one large, heterogeneous dataset.</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: merge-deduplicate</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_and_deduplicate(from_pubmed_all: List[pd.DataFrame], </span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>                          from_openalex_all: List[pd.DataFrame], </span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>                          from_elsevier_all: Optional[List[pd.DataFrame]] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>    all_dfs <span class="op">=</span> []</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> from_pubmed_all: all_dfs.extend(from_pubmed_all)</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> from_openalex_all: all_dfs.extend(from_openalex_all)</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> from_elsevier_all: all_dfs.extend(from_elsevier_all)</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> all_dfs:</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>        logging.warning(<span class="st">"No DataFrames provided to merge_and_deduplicate."</span>)</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.DataFrame()</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    combined_df <span class="op">=</span> pd.concat(all_dfs, ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> combined_df.empty:</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>        logging.info(<span class="st">"Combined DataFrame is empty before deduplication."</span>)</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> combined_df</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    deduplicated_df <span class="op">=</span> deduplicate_by_doi_title(combined_df)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> deduplicated_df</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>The subsequent critical step is deduplication, as the same article may be retrieved from multiple databases. A simple row-based drop would be insufficient. Therefore, the system implements a robust, hierarchical deduplication algorithm. First, it normalizes all Digital Object Identifiers (DOIs) by converting them to a consistent lowercase format and removing whitespace. It then identifies and retains the first occurrence of each unique DOI. </span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: deduplicate-doi</span></span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deduplicate_by_doi_title(df: pd.DataFrame) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create normalized columns</span></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"DOI"</span> <span class="kw">not</span> <span class="kw">in</span> df.columns: df[<span class="st">"DOI"</span>] <span class="op">=</span> <span class="st">""</span></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"Title"</span> <span class="kw">not</span> <span class="kw">in</span> df.columns: df[<span class="st">"Title"</span>] <span class="op">=</span> <span class="st">""</span></span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"DOI"</span>] <span class="op">=</span> df[<span class="st">"DOI"</span>].fillna(<span class="st">""</span>)</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"Title"</span>] <span class="op">=</span> df[<span class="st">"Title"</span>].fillna(<span class="st">""</span>)</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"DOI_clean"</span>] <span class="op">=</span> df[<span class="st">"DOI"</span>].astype(<span class="bu">str</span>).<span class="bu">str</span>.lower().<span class="bu">str</span>.strip()</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"Title_clean"</span>] <span class="op">=</span> df[<span class="st">"Title"</span>].astype(<span class="bu">str</span>).<span class="bu">str</span>.lower().<span class="bu">str</span>.strip()</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>    df.sort_values(by<span class="op">=</span>[<span class="st">"DOI_clean"</span>, <span class="st">"Title_clean"</span>], inplace<span class="op">=</span><span class="va">True</span>, kind<span class="op">=</span><span class="st">'stable'</span>)</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    indices_to_keep <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>    seen_dois <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index, doi <span class="kw">in</span> df.loc[df[<span class="st">"DOI_clean"</span>] <span class="op">!=</span> <span class="st">""</span>, <span class="st">"DOI_clean"</span>].items():</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> doi <span class="kw">not</span> <span class="kw">in</span> seen_dois:</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>            indices_to_keep.add(index)</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>            seen_dois.add(doi)</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>    empty_doi_mask <span class="op">=</span> (df[<span class="st">"DOI_clean"</span>] <span class="op">==</span> <span class="st">""</span>) <span class="op">&amp;</span> (<span class="op">~</span>df.index.isin(indices_to_keep))</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>    seen_titles_empty_doi <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index, title <span class="kw">in</span> df.loc[empty_doi_mask, <span class="st">"Title_clean"</span>].items():</span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> title <span class="kw">not</span> <span class="kw">in</span> seen_titles_empty_doi:</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>            indices_to_keep.add(index)</span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>            seen_titles_empty_doi.add(title)</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>    df_deduplicated <span class="op">=</span> df.loc[<span class="bu">list</span>(indices_to_keep)].copy()</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>    df_deduplicated.drop(columns<span class="op">=</span>[<span class="st">"DOI_clean"</span>, <span class="st">"Title_clean"</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>    df_deduplicated.reset_index(drop<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df_deduplicated</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>For the remaining records that may lack a DOI, the algorithm performs a secondary deduplication based on a similarly normalized article title. This two-pass strategy ensures that each unique scholarly work is represented by a single, canonical record in the final dataset, preventing data redundancy in all downstream analyses.</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Metadata Enrichment:</span></span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>The initial, merged dataset, while comprehensive, may contain records with incomplete information (e.g., a record from PubMed might be missing a PDF link that OpenAlex has). The final step in the acquisition phase is therefore a dedicated enrichment process, orchestrated by scripts/metadata_enrichment.py. This module takes the entire list of unique DOIs from the deduplicated dataset and once again uses an asynchronous approach to concurrently query both the OpenAlex and Crossref APIs for every article.</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: enrich-metadata</span></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> enrich_metadata_sync(df: pd.DataFrame, crossref_email: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>    email <span class="op">=</span> crossref_email <span class="kw">or</span> MAILTO</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a>    dois <span class="op">=</span> df[<span class="st">"DOI"</span>].dropna().unique().tolist()</span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a>    enrichment <span class="op">=</span> asyncio.run(enrich_metadata(dois, email))</span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, row <span class="kw">in</span> df.iterrows():</span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a>        doi <span class="op">=</span> row[<span class="st">"DOI"</span>]</span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a>        metadata <span class="op">=</span> enrichment.get(doi, {})</span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> metadata:</span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pd.isna(row.get(<span class="st">"Abstract"</span>)) <span class="kw">or</span> <span class="kw">not</span> <span class="bu">str</span>(row[<span class="st">"Abstract"</span>]).strip():</span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"Abstract"</span>] <span class="op">=</span> metadata.get(<span class="st">"Abstract"</span>, row[<span class="st">"Abstract"</span>])</span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pd.isna(row.get(<span class="st">"ItemType"</span>)) <span class="kw">or</span> <span class="kw">not</span> <span class="bu">str</span>(row[<span class="st">"ItemType"</span>]).strip() <span class="kw">or</span> row[<span class="st">"ItemType"</span>] <span class="op">==</span> <span class="st">"N/A"</span>:</span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"ItemType"</span>] <span class="op">=</span> metadata.get(<span class="st">"ItemType"</span>, row.get(<span class="st">"ItemType"</span>, <span class="st">"N/A"</span>))</span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a>            citation_count <span class="op">=</span> metadata.get(<span class="st">"CitationCount"</span>, row.get(<span class="st">"CitationCount"</span>))</span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"CitationCount"</span>] <span class="op">=</span> <span class="bu">int</span>(citation_count) <span class="cf">if</span> citation_count <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a>            primary_topic <span class="op">=</span> metadata.get(<span class="st">"primary_topic"</span>)</span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> primary_topic:</span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"primary_topic"</span>] <span class="op">=</span> json.dumps(primary_topic)</span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a>                df.at[idx, <span class="st">"primary_topic"</span>] <span class="op">=</span> pd.NA</span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"is_published"</span>] <span class="op">=</span> metadata.get(<span class="st">"is_published"</span>, row.get(<span class="st">"is_published"</span>))</span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"is_retracted"</span>] <span class="op">=</span> metadata.get(<span class="st">"is_retracted"</span>, row.get(<span class="st">"is_retracted"</span>))</span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a>            df.at[idx, <span class="st">"OpenAlexID"</span>] <span class="op">=</span> metadata.get(<span class="st">"OpenAlexID"</span>, row.get(<span class="st">"OpenAlexID"</span>))</span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"CitationCount"</span>] <span class="op">=</span> df[<span class="st">"CitationCount"</span>].fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a>The results are then intelligently merged back into the main DataFrame. This process is non-destructive; it is designed to backfill missing information rather than overwrite existing data. For instance, an abstract retrieved from Crossref will only be added to a record if that record's Abstract field is currently empty. Similarly, fields like ItemType or CitationCount are updated only if the existing data is null or a placeholder. This ensures that the final output is the most complete possible version of each record, combining the strengths of all queried databases.</span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stage 2: Asynchronous Full-Text PDF Retrieval</span></span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a>Following the creation of a unified and deduplicated list of scholarly articles, the next critical stage of the pipeline is the acquisition of the full-text content for each entry. This is essential for the deep content analysis, entity extraction, and topic modeling performed in subsequent stages. The system orchestrates this process by leveraging the Digital Object Identifier (DOI) of each article to query the *Unpaywall API*, a service that indexes legally available open-access versions of scholarly publications. Given that this stage involves potentially thousands of network requests, a high-throughput, asynchronous architecture was implemented to manage this I/O-bound task efficiently.</span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a>The core retrieval logic, encapsulated in *scripts/async_unpaywall.py*, is built upon Python's *asyncio* and *aiohttp* libraries. This approach allows the system to initiate and manage thousands of concurrent HTTP requests, preventing the pipeline from being blocked by network latency from any single request. To ensure compliance with Unpaywall's API usage policies and to maintain system stability, an *asyncio.Semaphore* is employed as a robust rate-limiting mechanism. The semaphore is initialized with the *API_RATE_LIMIT* value specified in the project's configuration file (e.g., 8 queries per second). Each asynchronous worker task must acquire the semaphore before making an API call, guaranteeing that the overall request rate does not exceed the defined limit. The worker function first queries Unpaywall for a given DOI to get a list of potential PDF locations, prioritizing the *best_oa_location*, and then iteratively attempts to download the file from each candidate URL until a valid PDF is secured.</span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: unpaywall-worker</span></span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/async_unpaywall.py</span></span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> _worker(row_idx, doi, title, session):</span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return (row_idx, pdf_path|None, status_string)."""</span></span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a>    api_url <span class="op">=</span> <span class="ss">f"https://api.unpaywall.org/v2/</span><span class="sc">{</span>doi<span class="sc">}</span><span class="ss">?email=</span><span class="sc">{</span>UNPAYWALL_EMAIL<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a>    <span class="co"># _fetch_json is rate-limited by a semaphore</span></span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="cf">await</span> _fetch_json(session, api_url)</span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> data:</span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> row_idx, <span class="va">None</span>, <span class="st">"Unpaywall_fail"</span></span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build a list of candidate URLs, prioritizing the best_oa_location</span></span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a>    candidates <span class="op">=</span> []</span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (b <span class="op">:=</span> data.get(<span class="st">"best_oa_location"</span>)): candidates.append(b)</span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a>    candidates <span class="op">+=</span> data.get(<span class="st">"oa_locations"</span>, [])</span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> loc <span class="kw">in</span> candidates:</span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a>        pdf_url <span class="op">=</span> loc.get(<span class="st">"url_for_pdf"</span>)</span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> pdf_url:</span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attempt to download the PDF from the candidate URL</span></span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a>        pdf_bytes <span class="op">=</span> <span class="cf">await</span> _download_pdf(session, pdf_url)</span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pdf_bytes:</span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sanitize filename and save the file</span></span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a>            safe_name <span class="op">=</span> re.sub(<span class="vs">r'[\\/*?:"&lt;&gt;|()]+'</span>, <span class="st">''</span>, title)[:<span class="dv">100</span>] <span class="kw">or</span> doi</span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a>            fname <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>safe_name<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>row_idx<span class="sc">}</span><span class="ss">.pdf"</span></span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a>            path  <span class="op">=</span> os.path.join(PDF_SAVE_FOLDER, fname)</span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a>                f.write(pdf_bytes)</span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> row_idx, path, <span class="st">"Saved"</span></span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> row_idx, <span class="va">None</span>, <span class="st">"No_PDF"</span></span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a>Upon successful download, the PDF is saved to a local directory defined in the configuration, and the main DataFrame is updated with the local file path for that article record. Records for which a full-text PDF could not be successfully retrieved are filtered out at the conclusion of this stage. The resulting DataFrame, now containing only articles with corresponding local PDF files, is then passed to the next stage of the pipeline for content extraction.</span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stage 3: Bibliographic Management and Citation Rendering</span></span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-193"><a href="#cb20-193" aria-hidden="true" tabindex="-1"></a>Following the acquisition and deduplication of article metadata and the retrieval of corresponding full-text PDFs, the pipeline proceeds to a crucial data management and enrichment stage. The objective here is twofold: first, to create a permanent, centralized, and queryable library of all processed literature using the Zotero reference management system; and second, to pre-generate authoritative citation metadata for each article. This pre-rendered citation data serves as a verifiable "ground truth" for the downstream Retrieval-Augmented Generation (RAG) agent, ensuring the academic rigor and traceability of the final synthesized output. This entire process is orchestrated by the s*cripts/fast_zotero_gui.py* module.</span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a>The integration with Zotero is architected for high-throughput processing. The *integrate* function within the module receives the DataFrame of articles with valid PDF paths and uses the *pyzotero* library to communicate with the Zotero API. To maximize efficiency, new items are created in batches of up to 50 articles per API call. Subsequently, the system utilizes a *ThreadPoolExecutor* to upload the associated PDF files to the newly created Zotero entries in parallel, significantly reducing the time required for this I/O-bound task.</span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a>A key technical innovation in this module is the method of citation generation. Rather than relying on slow, repeated API calls to Zotero for formatting each citation, the system leverages the *citeproc-py* library to perform this rendering locally. This approach uses standard Citation Style Language (CSL) files (e.g., apa.csl, ieee.csl), which are selectable in the GUI, to instantly format both the full bibliographic citation and the corresponding in-text citation for every article. The module also includes a robust fallback mechanism that can query the Zotero API for the citation if the local *citeproc* rendering encounters an error for a specific entry.</span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: zotero-citation-rendering</span></span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/fast_zotero_gui.py</span></span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a><span class="co"># ... inside the integrate function loop ...</span></span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, zkey <span class="kw">in</span> attach.items():</span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a>    df.at[idx, <span class="st">"ZoteroKey"</span>] <span class="op">=</span> zkey</span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a>    meta <span class="op">=</span> {c: df.at[idx, c] <span class="cf">for</span> c <span class="kw">in</span> (</span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Title"</span>, <span class="st">"DOI"</span>, <span class="st">"Authors"</span>, <span class="st">"Date"</span>,</span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Journal"</span>, <span class="st">"Volume"</span>, <span class="st">"Issue"</span>, <span class="st">"Pages"</span></span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a>    )}</span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attempt local citation rendering first</span></span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a>        in_text, full <span class="op">=</span> _render_citations(meta, csl_path)</span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> exc:</span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If local rendering fails, use the fallback</span></span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a>        logging.error(<span class="ss">f"citeproc rendering failed for row </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>exc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a>        in_text, full <span class="op">=</span> fallback(zkey, csl_path) <span class="cf">if</span> <span class="bu">callable</span>(fallback) <span class="cf">else</span> (<span class="st">""</span>, <span class="st">""</span>)</span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a>    df.at[idx, <span class="st">"InTextCitation"</span>] <span class="op">=</span> in_text <span class="kw">or</span> <span class="st">""</span></span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a>    df.at[idx, <span class="st">"FullCitation"</span>] <span class="op">=</span> full <span class="kw">or</span> <span class="st">""</span></span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a>The impact of this stage on the overall system is critical. The generated *ZoteroKey*, *InTextCitation*, and *FullCitation* generated in this stage are not merely for user reference; they are programmatically injected back into the DataFrame. This enriched data is subsequently passed as part of the context to the downstream Retrieval-Augmented Generation (RAG) agent.</span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a>This architectural choice is fundamental to ensuring the factuality and academic rigor of the final synthesized output. The RAG agent is explicitly instructed to populate its findings into a structured JSON object that requires these specific metadata fields for each piece of evidence it presents. By providing this pre-generated, validated citation data directly within the context, the system constrains the Large Language Model. This prevents the LLM from attempting to generate its own citations, a task for which they are notoriously unreliable and prone to hallucination.</span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a>Instead of showing the entire internal prompt, the following illustrates the structure of the metadata object that the RAG agent must populate for each observation it makes. The presence of the Zotero-derived fields highlights their essential role in the validation process.</span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: metadata-injection</span></span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a><span class="op">//</span> Example Metadata Schema <span class="cf">for</span> a single RAG Observation</span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a>  <span class="st">"metadata"</span>: {</span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PDF_DocIndex"</span>: <span class="st">"148"</span>,</span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PDF_ChunkIndex"</span>: <span class="st">"0"</span>,</span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Struct_DocIndex"</span>: <span class="st">"N/A"</span>,</span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Struct_ChunkIndex"</span>: <span class="st">"N/A"</span>,</span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a>    <span class="st">"DOI"</span>: <span class="st">"10.1016/j.jobe.2021.103722"</span>,</span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ZoteroKey"</span>: <span class="st">"ABC123DE"</span>,</span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a>    <span class="st">"InTextCitation"</span>: <span class="st">"(Author et al., 2021)"</span>,</span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a>    <span class="st">"FullCitation"</span>: <span class="st">"Author, A., Author, B., &amp; Author, C. (2021). Title of the article. Journal Name, 1(2), 100-110. https://doi.org/10.1016/j.jobe.2021.103722"</span></span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a>This architectural choice constrains the Large Language Model, preventing it from generating its own, potentially hallucinated, citations. It ensures that every piece of evidence presented in the final answer is verifiably and accurately linked back to a specific source document managed within the Zotero library, creating a fully traceable and academically sound output.</span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stage 4: Content Extraction via Document Layout Analysis</span></span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a>**Objective: High-Fidelity Extraction from Scholarly PDFs**</span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a>A foundational requirement for the ETL pipeline is the ability to accurately and efficiently extract content from thousands of scholarly articles in PDF format. This task is notoriously challenging due to the format's complex, variable layouts and the need to preserve structural context. The objective for this stage was to move beyond simple text extraction and reliably parse not only the full textual body of each article but also complex embedded elements crucial for scientific analysis, such as tabular data and mathematical formulas formatted in LaTeX.</span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Core Technology and Identified Limitations</span></span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a>To meet this objective, the system leverages IBM's Docling library as its core document analysis engine, a powerful framework that uses machine learning for sophisticated page layout detection. However, initial implementation and testing revealed several significant limitations with the out-of-the-box library that necessitated a substantial re-engineering effort:</span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Performance Bottleneck:** The framework lacked native support for efficient parallelization, making the processing of thousands of documents a prohibitively time-consuming, single-threaded operation.</span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Model Inefficiency:** TThe original formula recognition model, based on the *SamOPTForCausalLM* architecture, was found to be excessively resource-intensive, requiring approximately 30 GB of VRAM per instance, constraining the ability to parallelize effectively.</span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Layout Inaccuracies:** The layout detection model frequently produced suboptimal bounding boxes. This manifested in two primary ways:</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Formula Fragmentation:** Single, multi-line mathematical formulas were often incorrectly segmented into several independent clusters.</span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Layout Misclassification:** Pages containing structured elements like line numbers were often misclassified in their entirety as a single, large TABLE cluster, losing all other semantic information. </span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Stability Issues:** Persistent glyph-parsing errors within the core library caused instability during large-scale processing runs.</span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-276"><a href="#cb20-276" aria-hidden="true" tabindex="-1"></a>To address these significant challenges, a substantial re-engineering of the Docling library was undertaken. The resulting modified codebase, along with testing scripts, sample data, and a validation notebook, has been made publicly available in a dedicated GitHub repository to ensure transparency and allow for independent validation of the enhancements.</span>
<span id="cb20-277"><a href="#cb20-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Github Repository</span><span class="co">](https://github.com/agodinezmm2007/docling_mod)</span></span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Custom Enhancements and Implementation</span></span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a>To overcome these challenges, the system was transformed into a high-performance, accurate, and efficient processing system through a series of targeted enhancements, primarily orchestrated by *scripts/docling_multi_mp_gui.py*.</span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a>The performance bottleneck was addressed first by designing a custom architecture around Python's *concurrent.futures.ProcessPoolExecutor*. A key innovation was the implementation of a custom *worker_initializer* function that facilitates true parallel processing across multiple GPUs. This initializer uses a locking mechanism to assign each new worker process to a specific GPU from the available hardware pool, effectively converting Docling into a distributed application and dramatically reducing processing time.</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a>The accuracy and inefficiency issues of the formula recognition model were solved by replacing it entirely. The core *code_formula_predictor.py* was reconfigured to integrate a more efficient and accurate model, *"SmolDocling,"* which reduced VRAM requirements to ~8GB.</span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a>Finally, the layout inaccuracies and stability issues were addressed through direct modifications to the Docling codebase. This included engineering a sophisticated post-processing step in *docling/utils/layout_postprocessor.py* with rule-based heuristics to merge fragmented formulas and re-classify entire misidentified pages. The persistent glyph errors required low-level debugging by reverse-engineering the compiled C-extension library (*pdf_parsers.cpython-312-x86_64-linux-gnu.so*) using the *Ghidra* software reverse engineering suite to diagnose and patch the root cause.</span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a>These extensive modifications are reflected in the numerous library files that were changed during development:</span>
<span id="cb20-291"><a href="#cb20-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-292"><a href="#cb20-292" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Primary Docling Library Modifications:**</span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling/backend/docling_parse_v4_backend.py</span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling/datamodel/base_models.py</span>
<span id="cb20-296"><a href="#cb20-296" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling/datamodel/pipeline_options.py</span>
<span id="cb20-297"><a href="#cb20-297" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling/models/code_formula_model.py</span>
<span id="cb20-298"><a href="#cb20-298" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling/models/layout_model.py</span>
<span id="cb20-299"><a href="#cb20-299" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling/pipeline/vlm_pipeline.py</span>
<span id="cb20-300"><a href="#cb20-300" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling/utils/layout_postprocessor.py</span>
<span id="cb20-301"><a href="#cb20-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-302"><a href="#cb20-302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**IBM Model Predictor Modifications:**</span>
<span id="cb20-303"><a href="#cb20-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-304"><a href="#cb20-304" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling_ibm_models/code_formula_model/code_formula_predictor.py</span>
<span id="cb20-305"><a href="#cb20-305" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>docling_ibm_models/layoutmodel/layout_predictor.py</span>
<span id="cb20-306"><a href="#cb20-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-307"><a href="#cb20-307" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Initial Challenge: Inaccurate Bounding Boxes and Model Inefficiency</span></span>
<span id="cb20-308"><a href="#cb20-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-309"><a href="#cb20-309" aria-hidden="true" tabindex="-1"></a>The first major issue identified was the precision of the layout analysis model. The bounding boxes generated for mathematical formulas were often too tight or included extraneous visual artifacts from surrounding text. When these imperfect image snippets were passed to the original formula recognition model **(ds4sd/CodeFormula)**, it frequently failed, resulting in generative feedback loops that produced repetitive, malformed LaTeX output (as exemplified in @fig-bad-form, showing a poorly cropped snippet).</span>
<span id="cb20-310"><a href="#cb20-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-311"><a href="#cb20-311" aria-hidden="true" tabindex="-1"></a><span class="al">![Bad Formula](figures/bad_form_17.png)</span>{#fig-bad-form}</span>
<span id="cb20-312"><a href="#cb20-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-313"><a href="#cb20-313" aria-hidden="true" tabindex="-1"></a>Furthermore, the original model, based on the **SamOPTForCausalLM** architecture, was found to be excessively resource-intensive, requiring approximately 30 GB of VRAM per instance. This made large-scale parallelization across multiple GPUs prohibitively expensive and inefficient.</span>
<span id="cb20-314"><a href="#cb20-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-315"><a href="#cb20-315" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bounding Box and Image Preprocessing Modifications</span></span>
<span id="cb20-316"><a href="#cb20-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-317"><a href="#cb20-317" aria-hidden="true" tabindex="-1"></a>The initial attempt to resolve the formula recognition issue focused on improving the quality of the image snippets before they were sent to the model. This involved significant modifications to *docling/models/code_formula_model.py*.</span>
<span id="cb20-318"><a href="#cb20-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-319"><a href="#cb20-319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dynamic Bounding Box Expansion:** Logic was introduced in the *prepare_element* method to dynamically expand the bounding boxes of detected formulas. Instead of using the original, often tightly-cropped box, the new implementation expands the box by a configurable ratio of the snippet's height and width. This ensures more visual context is captured as demonstrated in @fig-good-form.</span>
<span id="cb20-320"><a href="#cb20-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-321"><a href="#cb20-321" aria-hidden="true" tabindex="-1"></a><span class="al">![Good Formula](figures/good_form_17.png)</span>{#fig-good-form}</span>
<span id="cb20-322"><a href="#cb20-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-323"><a href="#cb20-323" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ratio-Based Image Padding:** The original *_pad_with_most_frequent_edge_color* function, which used static pixel padding, was entirely replaced. The new version accepts floating-point ratios, allowing it to apply padding that is proportional to the image's dimensions. This creates a more consistent and contextually appropriate input for the model, regardless of the formula's original size.</span>
<span id="cb20-324"><a href="#cb20-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-325"><a href="#cb20-325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Conditional Image Masking:** To further isolate formulas from surrounding text, conditional logic was added to use Docling's *page.get_masked_image()* method exclusively for items labeled as DocItemLabel.FORMULA.</span>
<span id="cb20-326"><a href="#cb20-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-327"><a href="#cb20-327" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Implementation of Selective Content Masking</span></span>
<span id="cb20-328"><a href="#cb20-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-329"><a href="#cb20-329" aria-hidden="true" tabindex="-1"></a>The most critical enhancement was the development of a method to isolate formula elements from distracting adjacent text. When the layout model produced imperfect bounding boxes, snippets would often contain partial lines of text from the main body, confusing the formula recognition model. To solve this, a selective masking capability was engineered directly into the *Page* class within *docling/datamodel/base_models.py*.</span>
<span id="cb20-330"><a href="#cb20-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-331"><a href="#cb20-331" aria-hidden="true" tabindex="-1"></a>This was achieved by introducing two new methods, *get_masked_image* and the internal *_create_masked_image*. The *_create_masked_image* function operates by first taking a complete, high-resolution image of a document page. It then iterates through all layout clusters previously identified by the layout model. For every cluster that is not labeled as a *DocItemLabel.FORMULA*, the function programmatically draws a white rectangle over its bounding box.</span>
<span id="cb20-332"><a href="#cb20-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-335"><a href="#cb20-335" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-336"><a href="#cb20-336" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: create-masked-image</span></span>
<span id="cb20-337"><a href="#cb20-337" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-338"><a href="#cb20-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-339"><a href="#cb20-339" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _create_masked_image(<span class="va">self</span>, scale: <span class="bu">float</span>, pdf_identifier: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> Optional[Image]:</span>
<span id="cb20-340"><a href="#cb20-340" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb20-341"><a href="#cb20-341" aria-hidden="true" tabindex="-1"></a>        original_image <span class="op">=</span> <span class="va">self</span>.get_image(scale<span class="op">=</span>scale)</span>
<span id="cb20-342"><a href="#cb20-342" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> original_image <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> <span class="va">self</span>.size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb20-343"><a href="#cb20-343" aria-hidden="true" tabindex="-1"></a>            _log.warning(<span class="st">"Original image or size is None for page </span><span class="sc">%s</span><span class="st">."</span>, <span class="va">self</span>.page_no)</span>
<span id="cb20-344"><a href="#cb20-344" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb20-345"><a href="#cb20-345" aria-hidden="true" tabindex="-1"></a>        masked <span class="op">=</span> original_image.copy()</span>
<span id="cb20-346"><a href="#cb20-346" aria-hidden="true" tabindex="-1"></a>        draw <span class="op">=</span> ImageDraw.Draw(masked)</span>
<span id="cb20-347"><a href="#cb20-347" aria-hidden="true" tabindex="-1"></a>        scale_x <span class="op">=</span> masked.width <span class="op">/</span> <span class="va">self</span>.size.width</span>
<span id="cb20-348"><a href="#cb20-348" aria-hidden="true" tabindex="-1"></a>        scale_y <span class="op">=</span> masked.height <span class="op">/</span> <span class="va">self</span>.size.height</span>
<span id="cb20-349"><a href="#cb20-349" aria-hidden="true" tabindex="-1"></a>        top_expansion_factor <span class="op">=</span> <span class="fl">0.045</span></span>
<span id="cb20-350"><a href="#cb20-350" aria-hidden="true" tabindex="-1"></a>        bottom_expansion_factor <span class="op">=</span> <span class="fl">0.045</span></span>
<span id="cb20-351"><a href="#cb20-351" aria-hidden="true" tabindex="-1"></a>        cluster_count <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.predictions.layout.clusters)</span>
<span id="cb20-352"><a href="#cb20-352" aria-hidden="true" tabindex="-1"></a>        _log.debug(<span class="st">"Creating masked image for page </span><span class="sc">%s</span><span class="st"> with </span><span class="sc">%d</span><span class="st"> clusters."</span>, <span class="va">self</span>.page_no, cluster_count)</span>
<span id="cb20-353"><a href="#cb20-353" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mask non-formula clusters explicitly here with correct scaling</span></span>
<span id="cb20-354"><a href="#cb20-354" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> cl <span class="kw">in</span> <span class="va">self</span>.predictions.layout.clusters:</span>
<span id="cb20-355"><a href="#cb20-355" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> cl.label <span class="op">!=</span> DocItemLabel.FORMULA:</span>
<span id="cb20-356"><a href="#cb20-356" aria-hidden="true" tabindex="-1"></a>                bbox <span class="op">=</span> cl.bbox.to_top_left_origin(page_height<span class="op">=</span><span class="va">self</span>.size.height)</span>
<span id="cb20-357"><a href="#cb20-357" aria-hidden="true" tabindex="-1"></a>                bbox_height <span class="op">=</span> bbox.b <span class="op">-</span> bbox.t</span>
<span id="cb20-358"><a href="#cb20-358" aria-hidden="true" tabindex="-1"></a>                expanded_bbox <span class="op">=</span> BoundingBox(</span>
<span id="cb20-359"><a href="#cb20-359" aria-hidden="true" tabindex="-1"></a>                    l<span class="op">=</span>bbox.l,</span>
<span id="cb20-360"><a href="#cb20-360" aria-hidden="true" tabindex="-1"></a>                    t<span class="op">=</span>bbox.t <span class="op">-</span> bbox_height <span class="op">*</span> top_expansion_factor,</span>
<span id="cb20-361"><a href="#cb20-361" aria-hidden="true" tabindex="-1"></a>                    r<span class="op">=</span>bbox.r,</span>
<span id="cb20-362"><a href="#cb20-362" aria-hidden="true" tabindex="-1"></a>                    b<span class="op">=</span>bbox.b <span class="op">+</span> bbox_height <span class="op">*</span> bottom_expansion_factor,</span>
<span id="cb20-363"><a href="#cb20-363" aria-hidden="true" tabindex="-1"></a>                    coord_origin<span class="op">=</span>bbox.coord_origin,</span>
<span id="cb20-364"><a href="#cb20-364" aria-hidden="true" tabindex="-1"></a>                )               </span>
<span id="cb20-365"><a href="#cb20-365" aria-hidden="true" tabindex="-1"></a>                scaled_bbox <span class="op">=</span> BoundingBox(</span>
<span id="cb20-366"><a href="#cb20-366" aria-hidden="true" tabindex="-1"></a>                    l<span class="op">=</span>expanded_bbox.l <span class="op">*</span> scale_x,</span>
<span id="cb20-367"><a href="#cb20-367" aria-hidden="true" tabindex="-1"></a>                    t<span class="op">=</span>expanded_bbox.t <span class="op">*</span> scale_y,</span>
<span id="cb20-368"><a href="#cb20-368" aria-hidden="true" tabindex="-1"></a>                    r<span class="op">=</span>expanded_bbox.r <span class="op">*</span> scale_x,</span>
<span id="cb20-369"><a href="#cb20-369" aria-hidden="true" tabindex="-1"></a>                    b<span class="op">=</span>expanded_bbox.b <span class="op">*</span> scale_y,</span>
<span id="cb20-370"><a href="#cb20-370" aria-hidden="true" tabindex="-1"></a>                    coord_origin<span class="op">=</span>expanded_bbox.coord_origin</span>
<span id="cb20-371"><a href="#cb20-371" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb20-372"><a href="#cb20-372" aria-hidden="true" tabindex="-1"></a>                draw.rectangle(scaled_bbox.as_tuple(), fill<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb20-373"><a href="#cb20-373" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> masked</span>
<span id="cb20-374"><a href="#cb20-374" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-375"><a href="#cb20-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-376"><a href="#cb20-376" aria-hidden="true" tabindex="-1"></a>The result is a new, "masked" image of the page where all non-formula content has been completely obscured, as demonstrated in the comparison between an original page layout (@fig-unmasked) and its masked counterpart (@fig-masked).</span>
<span id="cb20-377"><a href="#cb20-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-378"><a href="#cb20-378" aria-hidden="true" tabindex="-1"></a><span class="al">![Unmasked Page](figures/postprocessed_layout_page_00020.png)</span>{#fig-unmasked}</span>
<span id="cb20-379"><a href="#cb20-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-380"><a href="#cb20-380" aria-hidden="true" tabindex="-1"></a><span class="al">![Masked Page](figures/masked_page_00020_scale_2.6.png)</span>{#fig-masked}</span>
<span id="cb20-381"><a href="#cb20-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-382"><a href="#cb20-382" aria-hidden="true" tabindex="-1"></a>When the downstream *prepare_element* method in *code_formula_model.py* is called, it now uses this *get_masked_image* function instead of the standard *get_image*. By cropping the formula's bounding box from this pre-masked page, the system guarantees that the final image snippet sent to the SmolDocling model contains only the formula itself, significantly improving the signal-to-noise ratio and preventing recognition errors caused by textual artifacts.</span>
<span id="cb20-383"><a href="#cb20-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-384"><a href="#cb20-384" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Heuristic-Based Merging of Fragmented Layout Clusters</span></span>
<span id="cb20-385"><a href="#cb20-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-386"><a href="#cb20-386" aria-hidden="true" tabindex="-1"></a>Beyond improving the quality of individual image snippets, a second significant challenge was identified in the output of the base layout analysis model: the fragmentation of single, cohesive document elements into multiple, distinct bounding boxes. This issue was particularly prevalent with complex, multi-line mathematical formulas, which were often incorrectly segmented into several independent *FORMULA* clusters, as exemplified in @fig-fragmented. This fragmentation would lead to the downstream formula recognition model receiving only partial equations, resulting in incomplete and unusable LaTeX output.</span>
<span id="cb20-387"><a href="#cb20-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-388"><a href="#cb20-388" aria-hidden="true" tabindex="-1"></a><span class="al">![Fragmented Formulas](figures/postprocessed_layout_page_00009_bad.png)</span>{#fig-fragmented}</span>
<span id="cb20-389"><a href="#cb20-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-390"><a href="#cb20-390" aria-hidden="true" tabindex="-1"></a>To resolve this, a sophisticated post-processing step was engineered within *docling/utils/layout_postprocessor.py*. The purpose of this step is to apply a series of rule-based heuristics to refine and correct the raw output of the initial layout model before the final document structure is assembled.</span>
<span id="cb20-391"><a href="#cb20-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-394"><a href="#cb20-394" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-395"><a href="#cb20-395" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: merge-clusters</span></span>
<span id="cb20-396"><a href="#cb20-396" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-397"><a href="#cb20-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-398"><a href="#cb20-398" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _merge_vertically_adjacent_formulas(</span>
<span id="cb20-399"><a href="#cb20-399" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>,</span>
<span id="cb20-400"><a href="#cb20-400" aria-hidden="true" tabindex="-1"></a>    clusters,</span>
<span id="cb20-401"><a href="#cb20-401" aria-hidden="true" tabindex="-1"></a>    vertical_threshold_factor<span class="op">=</span><span class="fl">1.8</span>,</span>
<span id="cb20-402"><a href="#cb20-402" aria-hidden="true" tabindex="-1"></a>    horizontal_overlap_threshold<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb20-403"><a href="#cb20-403" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb20-404"><a href="#cb20-404" aria-hidden="true" tabindex="-1"></a>    alignment_threshold<span class="op">=</span><span class="dv">20</span>,  <span class="co"># pixel-based threshold</span></span>
<span id="cb20-405"><a href="#cb20-405" aria-hidden="true" tabindex="-1"></a>    max_alignment_ratio<span class="op">=</span><span class="fl">0.2</span>,  <span class="co"># ratio-based threshold (e.g., 0.2 means 20% mismatch)</span></span>
<span id="cb20-406"><a href="#cb20-406" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb20-407"><a href="#cb20-407" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-408"><a href="#cb20-408" aria-hidden="true" tabindex="-1"></a><span class="co">    merge vertically adjacent FORMULA clusters using Union-Find.</span></span>
<span id="cb20-409"><a href="#cb20-409" aria-hidden="true" tabindex="-1"></a><span class="co">    New logic:</span></span>
<span id="cb20-410"><a href="#cb20-410" aria-hidden="true" tabindex="-1"></a><span class="co">    1) Compute a dynamic vertical threshold based on median formula height.</span></span>
<span id="cb20-411"><a href="#cb20-411" aria-hidden="true" tabindex="-1"></a><span class="co">    2) Expand bounding boxes horizontally using padding to compute the horizontal overlap.</span></span>
<span id="cb20-412"><a href="#cb20-412" aria-hidden="true" tabindex="-1"></a><span class="co">    3) Compute both absolute (pixel) differences and ratio-based alignment.</span></span>
<span id="cb20-413"><a href="#cb20-413" aria-hidden="true" tabindex="-1"></a><span class="co">    4) Extract formula numbers from cell texts. If both clusters have numbers and they differ, skip merge.</span></span>
<span id="cb20-414"><a href="#cb20-414" aria-hidden="true" tabindex="-1"></a><span class="co">    5) If neither cluster has a formula number, require that the vertical gap is very small (&lt;= 10 px)</span></span>
<span id="cb20-415"><a href="#cb20-415" aria-hidden="true" tabindex="-1"></a><span class="co">        and that the horizontal overlap is nearly complete (&gt;= 0.9).</span></span>
<span id="cb20-416"><a href="#cb20-416" aria-hidden="true" tabindex="-1"></a><span class="co">    6) Otherwise, use the normal criteria based on vertical gap (and derived thresholds).</span></span>
<span id="cb20-417"><a href="#cb20-417" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-418"><a href="#cb20-418" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_alignment_factor(c1, c2):</span>
<span id="cb20-419"><a href="#cb20-419" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the average width and return the maximum normalized edge difference.</span></span>
<span id="cb20-420"><a href="#cb20-420" aria-hidden="true" tabindex="-1"></a>        w1 <span class="op">=</span> c1.bbox.r <span class="op">-</span> c1.bbox.l</span>
<span id="cb20-421"><a href="#cb20-421" aria-hidden="true" tabindex="-1"></a>        w2 <span class="op">=</span> c2.bbox.r <span class="op">-</span> c2.bbox.l</span>
<span id="cb20-422"><a href="#cb20-422" aria-hidden="true" tabindex="-1"></a>        avg_width <span class="op">=</span> <span class="bu">max</span>((w1 <span class="op">+</span> w2) <span class="op">/</span> <span class="fl">2.0</span>, <span class="fl">1e-6</span>)</span>
<span id="cb20-423"><a href="#cb20-423" aria-hidden="true" tabindex="-1"></a>        left_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.l <span class="op">-</span> c2.bbox.l)</span>
<span id="cb20-424"><a href="#cb20-424" aria-hidden="true" tabindex="-1"></a>        right_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.r <span class="op">-</span> c2.bbox.r)</span>
<span id="cb20-425"><a href="#cb20-425" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">max</span>(left_diff <span class="op">/</span> avg_width, right_diff <span class="op">/</span> avg_width)</span>
<span id="cb20-426"><a href="#cb20-426" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Only process clusters labelled as FORMULA.</span></span>
<span id="cb20-427"><a href="#cb20-427" aria-hidden="true" tabindex="-1"></a>    formula_clusters <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> clusters <span class="cf">if</span> c.label <span class="op">==</span> DocItemLabel.FORMULA]</span>
<span id="cb20-428"><a href="#cb20-428" aria-hidden="true" tabindex="-1"></a>    non_formula_clusters <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> clusters <span class="cf">if</span> c.label <span class="op">!=</span> DocItemLabel.FORMULA]</span>
<span id="cb20-429"><a href="#cb20-429" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> formula_clusters:</span>
<span id="cb20-430"><a href="#cb20-430" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> clusters</span>
<span id="cb20-431"><a href="#cb20-431" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort by the top coordinate.</span></span>
<span id="cb20-432"><a href="#cb20-432" aria-hidden="true" tabindex="-1"></a>    formula_clusters.sort(key<span class="op">=</span><span class="kw">lambda</span> c: c.bbox.t)</span>
<span id="cb20-433"><a href="#cb20-433" aria-hidden="true" tabindex="-1"></a>    heights <span class="op">=</span> [c.bbox.b <span class="op">-</span> c.bbox.t <span class="cf">for</span> c <span class="kw">in</span> formula_clusters]</span>
<span id="cb20-434"><a href="#cb20-434" aria-hidden="true" tabindex="-1"></a>    median_height <span class="op">=</span> np.median(heights) <span class="cf">if</span> heights <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb20-435"><a href="#cb20-435" aria-hidden="true" tabindex="-1"></a>    vertical_threshold <span class="op">=</span> median_height <span class="op">*</span> vertical_threshold_factor</span>
<span id="cb20-436"><a href="#cb20-436" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build a mapping from cluster id to cluster instance.</span></span>
<span id="cb20-437"><a href="#cb20-437" aria-hidden="true" tabindex="-1"></a>    id_to_cluster <span class="op">=</span> {c.<span class="bu">id</span>: c <span class="cf">for</span> c <span class="kw">in</span> formula_clusters}</span>
<span id="cb20-438"><a href="#cb20-438" aria-hidden="true" tabindex="-1"></a>    uf <span class="op">=</span> UnionFind(<span class="bu">list</span>(id_to_cluster.keys()))</span>
<span id="cb20-439"><a href="#cb20-439" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(formula_clusters)</span>
<span id="cb20-440"><a href="#cb20-440" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb20-441"><a href="#cb20-441" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i <span class="op">+</span> <span class="dv">1</span>, n):</span>
<span id="cb20-442"><a href="#cb20-442" aria-hidden="true" tabindex="-1"></a>            c1 <span class="op">=</span> formula_clusters[i]</span>
<span id="cb20-443"><a href="#cb20-443" aria-hidden="true" tabindex="-1"></a>            c2 <span class="op">=</span> formula_clusters[j]</span>
<span id="cb20-444"><a href="#cb20-444" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute vertical gap (c2 is assumed to be below c1)</span></span>
<span id="cb20-445"><a href="#cb20-445" aria-hidden="true" tabindex="-1"></a>            vertical_gap <span class="op">=</span> c2.bbox.t <span class="op">-</span> c1.bbox.b</span>
<span id="cb20-446"><a href="#cb20-446" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> vertical_gap <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">or</span> vertical_gap <span class="op">&gt;</span> vertical_threshold:</span>
<span id="cb20-447"><a href="#cb20-447" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb20-448"><a href="#cb20-448" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Expand bounding boxes horizontally using the given padding.</span></span>
<span id="cb20-449"><a href="#cb20-449" aria-hidden="true" tabindex="-1"></a>            expanded_bbox_c1 <span class="op">=</span> <span class="bu">type</span>(c1.bbox)(</span>
<span id="cb20-450"><a href="#cb20-450" aria-hidden="true" tabindex="-1"></a>                l<span class="op">=</span>c1.bbox.l <span class="op">-</span> padding,</span>
<span id="cb20-451"><a href="#cb20-451" aria-hidden="true" tabindex="-1"></a>                t<span class="op">=</span>c1.bbox.t,</span>
<span id="cb20-452"><a href="#cb20-452" aria-hidden="true" tabindex="-1"></a>                r<span class="op">=</span>c1.bbox.r <span class="op">+</span> padding,</span>
<span id="cb20-453"><a href="#cb20-453" aria-hidden="true" tabindex="-1"></a>                b<span class="op">=</span>c1.bbox.b</span>
<span id="cb20-454"><a href="#cb20-454" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb20-455"><a href="#cb20-455" aria-hidden="true" tabindex="-1"></a>            expanded_bbox_c2 <span class="op">=</span> <span class="bu">type</span>(c2.bbox)(</span>
<span id="cb20-456"><a href="#cb20-456" aria-hidden="true" tabindex="-1"></a>                l<span class="op">=</span>c2.bbox.l <span class="op">-</span> padding,</span>
<span id="cb20-457"><a href="#cb20-457" aria-hidden="true" tabindex="-1"></a>                t<span class="op">=</span>c2.bbox.t,</span>
<span id="cb20-458"><a href="#cb20-458" aria-hidden="true" tabindex="-1"></a>                r<span class="op">=</span>c2.bbox.r <span class="op">+</span> padding,</span>
<span id="cb20-459"><a href="#cb20-459" aria-hidden="true" tabindex="-1"></a>                b<span class="op">=</span>c2.bbox.b</span>
<span id="cb20-460"><a href="#cb20-460" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb20-461"><a href="#cb20-461" aria-hidden="true" tabindex="-1"></a>            horizontal_overlap <span class="op">=</span> <span class="bu">min</span>(expanded_bbox_c1.r, expanded_bbox_c2.r) <span class="op">-</span> <span class="bu">max</span>(expanded_bbox_c1.l, expanded_bbox_c2.l)</span>
<span id="cb20-462"><a href="#cb20-462" aria-hidden="true" tabindex="-1"></a>            min_width <span class="op">=</span> <span class="bu">min</span>(expanded_bbox_c1.r <span class="op">-</span> expanded_bbox_c1.l, expanded_bbox_c2.r <span class="op">-</span> expanded_bbox_c2.l)</span>
<span id="cb20-463"><a href="#cb20-463" aria-hidden="true" tabindex="-1"></a>            overlap_ratio <span class="op">=</span> horizontal_overlap <span class="op">/</span> min_width <span class="cf">if</span> min_width <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb20-464"><a href="#cb20-464" aria-hidden="true" tabindex="-1"></a>            left_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.l <span class="op">-</span> c2.bbox.l)</span>
<span id="cb20-465"><a href="#cb20-465" aria-hidden="true" tabindex="-1"></a>            right_diff <span class="op">=</span> <span class="bu">abs</span>(c1.bbox.r <span class="op">-</span> c2.bbox.r)</span>
<span id="cb20-466"><a href="#cb20-466" aria-hidden="true" tabindex="-1"></a>            alignment_factor <span class="op">=</span> compute_alignment_factor(c1, c2)</span>
<span id="cb20-467"><a href="#cb20-467" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract formula numbers using _extract_formula_number.</span></span>
<span id="cb20-468"><a href="#cb20-468" aria-hidden="true" tabindex="-1"></a>            num1 <span class="op">=</span> <span class="va">self</span>._extract_formula_number(c1)</span>
<span id="cb20-469"><a href="#cb20-469" aria-hidden="true" tabindex="-1"></a>            num2 <span class="op">=</span> <span class="va">self</span>._extract_formula_number(c2)</span>
<span id="cb20-470"><a href="#cb20-470" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 4) If both have numbers and differ, skip</span></span>
<span id="cb20-471"><a href="#cb20-471" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> num1 <span class="kw">and</span> num2 <span class="kw">and</span> num1 <span class="op">!=</span> num2:</span>
<span id="cb20-472"><a href="#cb20-472" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb20-473"><a href="#cb20-473" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Normal merging criteria based on vertical gap and alignment.</span></span>
<span id="cb20-474"><a href="#cb20-474" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Pre-calculate default geometry-based threshold</span></span>
<span id="cb20-475"><a href="#cb20-475" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> vertical_gap <span class="op">&lt;=</span> (<span class="fl">0.5</span> <span class="op">*</span> vertical_threshold):</span>
<span id="cb20-476"><a href="#cb20-476" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> left_diff <span class="op">&lt;=</span> alignment_threshold <span class="kw">and</span> right_diff <span class="op">&lt;=</span> alignment_threshold:</span>
<span id="cb20-477"><a href="#cb20-477" aria-hidden="true" tabindex="-1"></a>                    required_overlap <span class="op">=</span> horizontal_overlap_threshold</span>
<span id="cb20-478"><a href="#cb20-478" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb20-479"><a href="#cb20-479" aria-hidden="true" tabindex="-1"></a>                    required_overlap <span class="op">=</span> <span class="fl">0.85</span></span>
<span id="cb20-480"><a href="#cb20-480" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> vertical_gap <span class="op">&lt;=</span> vertical_threshold:</span>
<span id="cb20-481"><a href="#cb20-481" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> alignment_factor <span class="op">&lt;=</span> max_alignment_ratio:</span>
<span id="cb20-482"><a href="#cb20-482" aria-hidden="true" tabindex="-1"></a>                    required_overlap <span class="op">=</span> <span class="fl">0.85</span></span>
<span id="cb20-483"><a href="#cb20-483" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb20-484"><a href="#cb20-484" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb20-485"><a href="#cb20-485" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-486"><a href="#cb20-486" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb20-487"><a href="#cb20-487" aria-hidden="true" tabindex="-1"></a>            <span class="co"># New branch: if no formula number is detected in either cluster,</span></span>
<span id="cb20-488"><a href="#cb20-488" aria-hidden="true" tabindex="-1"></a>            <span class="co"># then enforce a very small vertical gap and nearly complete horizontal overlap.</span></span>
<span id="cb20-489"><a href="#cb20-489" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 2) Both None =&gt; require vertical_gap ≤10 and overlap ≥0.9</span></span>
<span id="cb20-490"><a href="#cb20-490" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> num1 <span class="kw">is</span> <span class="va">None</span> <span class="kw">and</span> num2 <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb20-491"><a href="#cb20-491" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> vertical_gap <span class="op">&gt;</span> <span class="dv">3</span>:  <span class="co"># Change this constant if needed</span></span>
<span id="cb20-492"><a href="#cb20-492" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb20-493"><a href="#cb20-493" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Require a higher overlap; allow slight relaxation if the normal criterion is higher.</span></span>
<span id="cb20-494"><a href="#cb20-494" aria-hidden="true" tabindex="-1"></a>                required_overlap <span class="op">=</span> <span class="bu">max</span>(required_overlap, <span class="fl">0.9</span>)</span>
<span id="cb20-495"><a href="#cb20-495" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 3) One number missing =&gt; require vertical_gap ≤5 and overlap ≥0.95</span></span>
<span id="cb20-496"><a href="#cb20-496" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> (num1 <span class="kw">is</span> <span class="va">None</span>) <span class="op">!=</span> (num2 <span class="kw">is</span> <span class="va">None</span>):</span>
<span id="cb20-497"><a href="#cb20-497" aria-hidden="true" tabindex="-1"></a>                <span class="co"># i.e. exactly one side is missing a number</span></span>
<span id="cb20-498"><a href="#cb20-498" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> vertical_gap <span class="op">&gt;</span> <span class="fl">12.8</span>:</span>
<span id="cb20-499"><a href="#cb20-499" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb20-500"><a href="#cb20-500" aria-hidden="true" tabindex="-1"></a>                required_overlap <span class="op">=</span> <span class="bu">max</span>(required_overlap, <span class="fl">0.95</span>)</span>
<span id="cb20-501"><a href="#cb20-501" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Branch 1) If both exist and match =&gt; use normal geometry (already in required_overlap)</span></span>
<span id="cb20-502"><a href="#cb20-502" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Final merge check</span></span>
<span id="cb20-503"><a href="#cb20-503" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> overlap_ratio <span class="op">&gt;=</span> required_overlap:</span>
<span id="cb20-504"><a href="#cb20-504" aria-hidden="true" tabindex="-1"></a>                uf.union(c1.<span class="bu">id</span>, c2.<span class="bu">id</span>)</span>
<span id="cb20-505"><a href="#cb20-505" aria-hidden="true" tabindex="-1"></a>    groups <span class="op">=</span> uf.get_groups()</span>
<span id="cb20-506"><a href="#cb20-506" aria-hidden="true" tabindex="-1"></a>    merged_formula_clusters <span class="op">=</span> []</span>
<span id="cb20-507"><a href="#cb20-507" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> group_ids <span class="kw">in</span> groups.values():</span>
<span id="cb20-508"><a href="#cb20-508" aria-hidden="true" tabindex="-1"></a>        group_clusters <span class="op">=</span> [id_to_cluster[g] <span class="cf">for</span> g <span class="kw">in</span> group_ids]</span>
<span id="cb20-509"><a href="#cb20-509" aria-hidden="true" tabindex="-1"></a>        merged_bbox <span class="op">=</span> <span class="bu">type</span>(group_clusters[<span class="dv">0</span>].bbox)(</span>
<span id="cb20-510"><a href="#cb20-510" aria-hidden="true" tabindex="-1"></a>            l<span class="op">=</span><span class="bu">min</span>(c.bbox.l <span class="cf">for</span> c <span class="kw">in</span> group_clusters),</span>
<span id="cb20-511"><a href="#cb20-511" aria-hidden="true" tabindex="-1"></a>            t<span class="op">=</span><span class="bu">min</span>(c.bbox.t <span class="cf">for</span> c <span class="kw">in</span> group_clusters),</span>
<span id="cb20-512"><a href="#cb20-512" aria-hidden="true" tabindex="-1"></a>            r<span class="op">=</span><span class="bu">max</span>(c.bbox.r <span class="cf">for</span> c <span class="kw">in</span> group_clusters),</span>
<span id="cb20-513"><a href="#cb20-513" aria-hidden="true" tabindex="-1"></a>            b<span class="op">=</span><span class="bu">max</span>(c.bbox.b <span class="cf">for</span> c <span class="kw">in</span> group_clusters)</span>
<span id="cb20-514"><a href="#cb20-514" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-515"><a href="#cb20-515" aria-hidden="true" tabindex="-1"></a>        combined_cells <span class="op">=</span> []</span>
<span id="cb20-516"><a href="#cb20-516" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> group_clusters:</span>
<span id="cb20-517"><a href="#cb20-517" aria-hidden="true" tabindex="-1"></a>            combined_cells.extend(c.cells)</span>
<span id="cb20-518"><a href="#cb20-518" aria-hidden="true" tabindex="-1"></a>        merged_cluster <span class="op">=</span> group_clusters[<span class="dv">0</span>]</span>
<span id="cb20-519"><a href="#cb20-519" aria-hidden="true" tabindex="-1"></a>        merged_cluster.bbox <span class="op">=</span> merged_bbox</span>
<span id="cb20-520"><a href="#cb20-520" aria-hidden="true" tabindex="-1"></a>        merged_cluster.cells <span class="op">=</span> <span class="va">self</span>._sort_cells(<span class="va">self</span>._deduplicate_cells(combined_cells))</span>
<span id="cb20-521"><a href="#cb20-521" aria-hidden="true" tabindex="-1"></a>        merged_formula_clusters.append(merged_cluster)</span>
<span id="cb20-522"><a href="#cb20-522" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> non_formula_clusters <span class="op">+</span> merged_formula_clusters</span>
<span id="cb20-523"><a href="#cb20-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-524"><a href="#cb20-524" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-525"><a href="#cb20-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-526"><a href="#cb20-526" aria-hidden="true" tabindex="-1"></a>The core of this enhancement is the new *_merge_vertically_adjacent_formulas* method. This algorithm intelligently identifies and merges clusters that likely belong to the same formula. It operates on a set of heuristics designed to distinguish between separate, adjacent equations and multiple lines of a single equation:</span>
<span id="cb20-527"><a href="#cb20-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-528"><a href="#cb20-528" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Dynamic Proximity Threshold:** Rather than using a fixed pixel distance, the method first calculates the median height of all detected formulas on a page. It then defines a dynamic vertical proximity threshold based on a factor of this median height, allowing it to adapt to documents with different font sizes and line spacing.</span>
<span id="cb20-529"><a href="#cb20-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-530"><a href="#cb20-530" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Formula Number Extraction:** A helper function, _extract_formula_number, uses regular expressions to find equation numbers (e.g., (1), (2a), (A5)) within the text of each formula cluster.</span>
<span id="cb20-531"><a href="#cb20-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-532"><a href="#cb20-532" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Contextual Merging Logic:** The algorithm iterates through pairs of vertically adjacent formula clusters and applies a decision-making process:</span>
<span id="cb20-533"><a href="#cb20-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-534"><a href="#cb20-534" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If both clusters have distinct formula numbers (e.g., (1) and (2)), they are correctly identified as separate equations and are not merged.   </span>
<span id="cb20-535"><a href="#cb20-535" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If neither cluster has a formula number, they are only merged if the vertical gap between them is minimal and their horizontal overlap is nearly complete, which is characteristic of multi-line equations without a single encompassing number.  </span>
<span id="cb20-536"><a href="#cb20-536" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>If one cluster has a number and the adjacent one does not, a stricter set of proximity and overlap rules is applied.  </span>
<span id="cb20-537"><a href="#cb20-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-538"><a href="#cb20-538" aria-hidden="true" tabindex="-1"></a>Once candidate clusters for merging are identified, a Union-Find data structure is used to efficiently group them. The final step involves creating a new, single bounding box that encompasses all the merged clusters and combining their constituent text cells. This new, unified cluster is then passed to the next stage of the pipeline, ensuring the entire formula is processed as a single unit as demonstrated in @fig-unfragmented. Additional heuristics, such as the *_filter_tables_containing_page_footer* method, were also added to this module to correct other common layout analysis errors, further improving the overall quality of the parsed document structure.</span>
<span id="cb20-539"><a href="#cb20-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-540"><a href="#cb20-540" aria-hidden="true" tabindex="-1"></a><span class="al">![Unfragmented Formulas](figures/postprocessed_layout_page_00009_crop_carb.png)</span>{#fig-unfragmented}</span>
<span id="cb20-541"><a href="#cb20-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-542"><a href="#cb20-542" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Misclassification of Page Layouts</span></span>
<span id="cb20-543"><a href="#cb20-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-544"><a href="#cb20-544" aria-hidden="true" tabindex="-1"></a>This analysis details further enhancements made to the layout post-processing logic to address specific, recurring classification errors observed in certain document types, such as pre-prints or articles with line numbers.</span>
<span id="cb20-545"><a href="#cb20-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-546"><a href="#cb20-546" aria-hidden="true" tabindex="-1"></a>A common failure mode was observed where the layout detection model would misclassify an entire page or large sections of a page as a single, large TABLE cluster. This issue was particularly prevalent in documents that featured line numbers running down the left-hand margin, as seen in @fig-table-misscl. The presence of this repeating, structured numerical column appeared to mislead the model into perceiving the entire text block as tabular data. This resulted in the loss of all other semantic distinctions for that page (e.g., paragraphs, headers, lists), rendering the content unusable for downstream extraction.</span>
<span id="cb20-547"><a href="#cb20-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-548"><a href="#cb20-548" aria-hidden="true" tabindex="-1"></a><span class="al">![Table Missclassification](figures/utility_postprocessed_layout_page_00003_before_fix_attempt.png)</span>{#fig-table-misscl}</span>
<span id="cb20-549"><a href="#cb20-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-550"><a href="#cb20-550" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Heuristic-Based Re-classification of Misidentified Clusters</span></span>
<span id="cb20-551"><a href="#cb20-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-552"><a href="#cb20-552" aria-hidden="true" tabindex="-1"></a>To correct this, a new heuristic-based filtering method, *_filter_tables_containing_page_footer*, was engineered and integrated into the main postprocess pipeline. While the method name suggests a focus on page footers, its logic was designed more broadly to identify and re-classify any cluster that was erroneously labeled as a table-like structure when it was, in fact, the main body text of the page.</span>
<span id="cb20-553"><a href="#cb20-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-556"><a href="#cb20-556" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-557"><a href="#cb20-557" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: filter-large-tables</span></span>
<span id="cb20-558"><a href="#cb20-558" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-559"><a href="#cb20-559" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/docling/utils/layout_postprocessor.py</span></span>
<span id="cb20-560"><a href="#cb20-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-561"><a href="#cb20-561" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _filter_tables_containing_page_footer(<span class="va">self</span>, clusters: List[Cluster], </span>
<span id="cb20-562"><a href="#cb20-562" aria-hidden="true" tabindex="-1"></a>                                          min_area_ratio: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.70</span>, </span>
<span id="cb20-563"><a href="#cb20-563" aria-hidden="true" tabindex="-1"></a>                                          min_cells_threshold: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>,</span>
<span id="cb20-564"><a href="#cb20-564" aria-hidden="true" tabindex="-1"></a>                                          min_density_threshold: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.001</span>) <span class="op">-&gt;</span> List[Cluster]:</span>
<span id="cb20-565"><a href="#cb20-565" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-566"><a href="#cb20-566" aria-hidden="true" tabindex="-1"></a><span class="co">    Enhanced logic to avoid reclassifying legitimate large tables.</span></span>
<span id="cb20-567"><a href="#cb20-567" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-568"><a href="#cb20-568" aria-hidden="true" tabindex="-1"></a>    large_labels <span class="op">=</span> {DocItemLabel.TABLE, DocItemLabel.DOCUMENT_INDEX, </span>
<span id="cb20-569"><a href="#cb20-569" aria-hidden="true" tabindex="-1"></a>                    DocItemLabel.KEY_VALUE_REGION, DocItemLabel.FORM}</span>
<span id="cb20-570"><a href="#cb20-570" aria-hidden="true" tabindex="-1"></a>    page_area <span class="op">=</span> <span class="va">self</span>.page_size.width <span class="op">*</span> <span class="va">self</span>.page_size.height</span>
<span id="cb20-571"><a href="#cb20-571" aria-hidden="true" tabindex="-1"></a>    reclassified_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-572"><a href="#cb20-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-573"><a href="#cb20-573" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cluster <span class="kw">in</span> clusters:</span>
<span id="cb20-574"><a href="#cb20-574" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cluster.label <span class="kw">in</span> large_labels:</span>
<span id="cb20-575"><a href="#cb20-575" aria-hidden="true" tabindex="-1"></a>            cluster_area_ratio <span class="op">=</span> cluster.bbox.area() <span class="op">/</span> page_area</span>
<span id="cb20-576"><a href="#cb20-576" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> cluster_area_ratio <span class="op">&gt;=</span> min_area_ratio:</span>
<span id="cb20-577"><a href="#cb20-577" aria-hidden="true" tabindex="-1"></a>                all_cells <span class="op">=</span> <span class="va">self</span>._collect_all_cells(cluster)</span>
<span id="cb20-578"><a href="#cb20-578" aria-hidden="true" tabindex="-1"></a>                cell_density <span class="op">=</span> <span class="bu">len</span>(all_cells) <span class="op">/</span> cluster.bbox.area()</span>
<span id="cb20-579"><a href="#cb20-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-580"><a href="#cb20-580" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Reclassify if the cluster is large but sparsely populated with text cells</span></span>
<span id="cb20-581"><a href="#cb20-581" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(all_cells) <span class="op">&lt;</span> min_cells_threshold <span class="kw">or</span> cell_density <span class="op">&lt;</span> min_density_threshold:</span>
<span id="cb20-582"><a href="#cb20-582" aria-hidden="true" tabindex="-1"></a>                    cluster.label <span class="op">=</span> DocItemLabel.TEXT</span>
<span id="cb20-583"><a href="#cb20-583" aria-hidden="true" tabindex="-1"></a>                    reclassified_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb20-584"><a href="#cb20-584" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> clusters</span>
<span id="cb20-585"><a href="#cb20-585" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-586"><a href="#cb20-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-587"><a href="#cb20-587" aria-hidden="true" tabindex="-1"></a>The algorithm operates on a set of carefully tuned heuristics to distinguish between a legitimate, large table and a misclassified text block:</span>
<span id="cb20-588"><a href="#cb20-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-589"><a href="#cb20-589" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Target Selection:** The function first identifies any cluster labeled as a *TABLE* (or other large "wrapper" types like *FORM*) for inspection.</span>
<span id="cb20-590"><a href="#cb20-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-591"><a href="#cb20-591" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Area Threshold:** It only considers clusters that are exceptionally large, occupying a significant portion of the page (e.g., *min_area_ratio* of 70%). This prevents the algorithm from affecting smaller, legitimate tables.</span>
<span id="cb20-592"><a href="#cb20-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-593"><a href="#cb20-593" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Cell Count &amp; Density Check:** This is the core heuristic. It recursively collects all the individual text cells within the large cluster and calculates two metrics: the absolute number of cells and the "cell density" (number of cells per unit of bounding box area).</span>
<span id="cb20-594"><a href="#cb20-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-595"><a href="#cb20-595" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Re-classification Logic:** If a cluster is very large but contains a low number of text cells or has a very low cell density, it is highly unlikely to be a real table. In such cases, the algorithm re-classifies the cluster's label from *TABLE* to *TEXT*.</span>
<span id="cb20-596"><a href="#cb20-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-597"><a href="#cb20-597" aria-hidden="true" tabindex="-1"></a>By adding this function to the postprocess pipeline, the system can now correctly identify and relabel these large, sparse, misclassified text blocks, preserving the semantic integrity of the page structure as demonstrated by the corrected output in @fig-table-correction.</span>
<span id="cb20-598"><a href="#cb20-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-599"><a href="#cb20-599" aria-hidden="true" tabindex="-1"></a><span class="al">![Table Correction](figures/utility_postprocessed_layout_page_00003_after.png)</span>{#fig-table-correction}</span>
<span id="cb20-600"><a href="#cb20-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-601"><a href="#cb20-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-602"><a href="#cb20-602" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pre-emptive Filtering of Page-Level Artifacts</span></span>
<span id="cb20-603"><a href="#cb20-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-604"><a href="#cb20-604" aria-hidden="true" tabindex="-1"></a>In addition to the post-processing of layout clusters, further enhancements were engineered upstream in the *docling/backend/docling_parse_v4_backend.py* module to improve the initial quality of the data fed to the layout analysis model. This module serves as a low-level backend that interacts directly with the *docling-parse* C++ library, which is responsible for the initial extraction of raw text cells and their coordinates from the PDF document. It was observed that certain document formats, particularly pre-prints, often contain page-level artifacts like line numbers in the margins. These structured, non-substantive elements were found to frequently mislead the layout model, leading to catastrophic classification errors where an entire page of text would be misidentified as a single, large *TABLE* cluster.</span>
<span id="cb20-605"><a href="#cb20-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-606"><a href="#cb20-606" aria-hidden="true" tabindex="-1"></a>To mitigate this failure mode, the primary modification was made to the *get_text_cells* method within the *DoclingParseV4PageBackend* class. The original implementation of this method simply transformed the coordinate system of all extracted text cells and returned the complete, unfiltered list. The enhanced version introduces a new helper method, *_is_left_margin_line_number*, which applies a series of geometric heuristics to identify and pre-emptively filter these line-number artifacts before they are passed to the layout model.</span>
<span id="cb20-607"><a href="#cb20-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-608"><a href="#cb20-608" aria-hidden="true" tabindex="-1"></a>The heuristic function determines if a given text cell is a line number by evaluating three specific spatial properties. First, it confirms the cell is located within a narrow vertical band on the far-left of the page, defined by a *LEFT_MARGIN_THRESHOLD* (e.g., the leftmost 8% of the page width). Second, it verifies that the cell's bounding box width is less than this *MAX_WIDTH_THRESHOLD*, characteristic of short numerical strings. Finally, it ensures the cell has a reasonable height via *MIN_HEIGHT_THRESHOLD* to avoid incorrectly filtering other small page markings like footnote symbols. A cell is only flagged and removed if it satisfies all three conditions. By filtering the cell list with this function, the layout model receives a much cleaner representation of the page's semantic content, significantly reducing classification errors and preserving the integrity of the document structure for all downstream processing.</span>
<span id="cb20-609"><a href="#cb20-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-610"><a href="#cb20-610" aria-hidden="true" tabindex="-1"></a>**Original Implementation (get_text_cells)**</span>
<span id="cb20-611"><a href="#cb20-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-614"><a href="#cb20-614" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-615"><a href="#cb20-615" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: text-cells-og</span></span>
<span id="cb20-616"><a href="#cb20-616" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-617"><a href="#cb20-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-618"><a href="#cb20-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-619"><a href="#cb20-619" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_text_cells(<span class="va">self</span>) <span class="op">-&gt;</span> Iterable[TextCell]:</span>
<span id="cb20-620"><a href="#cb20-620" aria-hidden="true" tabindex="-1"></a>    page_size <span class="op">=</span> <span class="va">self</span>.get_size()</span>
<span id="cb20-621"><a href="#cb20-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-622"><a href="#cb20-622" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Applies coordinate transformation but returns all cells</span></span>
<span id="cb20-623"><a href="#cb20-623" aria-hidden="true" tabindex="-1"></a>    [tc.to_top_left_origin(page_size.height) <span class="cf">for</span> tc <span class="kw">in</span> <span class="va">self</span>._dpage.textline_cells]</span>
<span id="cb20-624"><a href="#cb20-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-625"><a href="#cb20-625" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>._dpage.textline_cells</span>
<span id="cb20-626"><a href="#cb20-626" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-627"><a href="#cb20-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-628"><a href="#cb20-628" aria-hidden="true" tabindex="-1"></a>**Modified Implementation with Heuristic Filtering**</span>
<span id="cb20-629"><a href="#cb20-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-632"><a href="#cb20-632" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-633"><a href="#cb20-633" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: text-cells-mod</span></span>
<span id="cb20-634"><a href="#cb20-634" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-635"><a href="#cb20-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-636"><a href="#cb20-636" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_text_cells(<span class="va">self</span>) <span class="op">-&gt;</span> Iterable[TextCell]:</span>
<span id="cb20-637"><a href="#cb20-637" aria-hidden="true" tabindex="-1"></a>    page_size <span class="op">=</span> <span class="va">self</span>.get_size()</span>
<span id="cb20-638"><a href="#cb20-638" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tc <span class="kw">in</span> <span class="va">self</span>._dpage.textline_cells:</span>
<span id="cb20-639"><a href="#cb20-639" aria-hidden="true" tabindex="-1"></a>        tc.to_top_left_origin(page_size.height)</span>
<span id="cb20-640"><a href="#cb20-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-641"><a href="#cb20-641" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter out cells identified as line numbers by the heuristic</span></span>
<span id="cb20-642"><a href="#cb20-642" aria-hidden="true" tabindex="-1"></a>    filtered_cells <span class="op">=</span> [</span>
<span id="cb20-643"><a href="#cb20-643" aria-hidden="true" tabindex="-1"></a>        cell <span class="cf">for</span> cell <span class="kw">in</span> <span class="va">self</span>._dpage.textline_cells</span>
<span id="cb20-644"><a href="#cb20-644" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>._is_left_margin_line_number(cell, page_size)</span>
<span id="cb20-645"><a href="#cb20-645" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb20-646"><a href="#cb20-646" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> filtered_cells</span>
<span id="cb20-647"><a href="#cb20-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-648"><a href="#cb20-648" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _is_left_margin_line_number(<span class="va">self</span>, cell: TextCell, page_size: Size) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb20-649"><a href="#cb20-649" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-650"><a href="#cb20-650" aria-hidden="true" tabindex="-1"></a><span class="co">    Identifies if a cell is a line number based on geometric properties.</span></span>
<span id="cb20-651"><a href="#cb20-651" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-652"><a href="#cb20-652" aria-hidden="true" tabindex="-1"></a>    LEFT_MARGIN_THRESHOLD <span class="op">=</span> <span class="fl">0.08</span></span>
<span id="cb20-653"><a href="#cb20-653" aria-hidden="true" tabindex="-1"></a>    MIN_HEIGHT_THRESHOLD <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb20-654"><a href="#cb20-654" aria-hidden="true" tabindex="-1"></a>    MAX_WIDTH_THRESHOLD <span class="op">=</span> page_size.width <span class="op">*</span> LEFT_MARGIN_THRESHOLD</span>
<span id="cb20-655"><a href="#cb20-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-656"><a href="#cb20-656" aria-hidden="true" tabindex="-1"></a>    bbox <span class="op">=</span> cell.rect.to_bounding_box()</span>
<span id="cb20-657"><a href="#cb20-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-658"><a href="#cb20-658" aria-hidden="true" tabindex="-1"></a>    is_within_left_margin <span class="op">=</span> bbox.l <span class="op">&lt;</span> MAX_WIDTH_THRESHOLD</span>
<span id="cb20-659"><a href="#cb20-659" aria-hidden="true" tabindex="-1"></a>    is_small_horizontal <span class="op">=</span> bbox.width <span class="op">&lt;</span> MAX_WIDTH_THRESHOLD</span>
<span id="cb20-660"><a href="#cb20-660" aria-hidden="true" tabindex="-1"></a>    is_not_too_short <span class="op">=</span> bbox.height <span class="op">&gt;=</span> MIN_HEIGHT_THRESHOLD <span class="co"># Corrected logic</span></span>
<span id="cb20-661"><a href="#cb20-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-662"><a href="#cb20-662" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> is_within_left_margin <span class="kw">and</span> is_small_horizontal <span class="kw">and</span> is_not_too_short</span>
<span id="cb20-663"><a href="#cb20-663" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-664"><a href="#cb20-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-665"><a href="#cb20-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-666"><a href="#cb20-666" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Refinements to the Underlying Layout Predictor</span></span>
<span id="cb20-667"><a href="#cb20-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-668"><a href="#cb20-668" aria-hidden="true" tabindex="-1"></a>Complementing the logical post-processing of layout clusters, targeted modifications were also made to the underlying *docling_ibm_models/layoutmodel/layout_predictor.py* module. This module is responsible for the initial, low-level detection of layout elements on a page image by executing the *RTDetrForObjectDetection model*. While the primary logic of the model itself was not altered, key refinements were made to its data handling and execution to ensure high-performance and stable operation within the custom parallelized framework.</span>
<span id="cb20-669"><a href="#cb20-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-670"><a href="#cb20-670" aria-hidden="true" tabindex="-1"></a>The most critical modification addresses a performance bottleneck related to hardware utilization. In the *predict* method, the *target_sizes* tensor, which is required by the *post_process_object_detection* function to correctly rescale bounding boxes to the original image dimensions was originally created on the CPU by default.</span>
<span id="cb20-671"><a href="#cb20-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-674"><a href="#cb20-674" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-675"><a href="#cb20-675" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: original-version</span></span>
<span id="cb20-676"><a href="#cb20-676" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-677"><a href="#cb20-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-678"><a href="#cb20-678" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.inference_mode</span>()</span>
<span id="cb20-679"><a href="#cb20-679" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(<span class="va">self</span>, orig_img: Union[Image.Image, np.ndarray]) <span class="op">-&gt;</span> Iterable[<span class="bu">dict</span>]:</span>
<span id="cb20-680"><a href="#cb20-680" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> <span class="va">self</span>._image_processor.post_process_object_detection(</span>
<span id="cb20-681"><a href="#cb20-681" aria-hidden="true" tabindex="-1"></a>        outputs,</span>
<span id="cb20-682"><a href="#cb20-682" aria-hidden="true" tabindex="-1"></a>        target_sizes<span class="op">=</span>torch.tensor([page_img.size[::<span class="op">-</span><span class="dv">1</span>]]), <span class="co"># Tensor created on CPU by default</span></span>
<span id="cb20-683"><a href="#cb20-683" aria-hidden="true" tabindex="-1"></a>        threshold<span class="op">=</span><span class="va">self</span>._threshold,</span>
<span id="cb20-684"><a href="#cb20-684" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-685"><a href="#cb20-685" aria-hidden="true" tabindex="-1"></a>    w, h <span class="op">=</span> page_img.size</span>
<span id="cb20-686"><a href="#cb20-686" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> results[<span class="dv">0</span>]</span>
<span id="cb20-687"><a href="#cb20-687" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> score, label_id, box <span class="kw">in</span> <span class="bu">zip</span>(result[<span class="st">"scores"</span>], result[<span class="st">"labels"</span>], result[<span class="st">"boxes"</span>]):</span>
<span id="cb20-688"><a href="#cb20-688" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb20-689"><a href="#cb20-689" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Manual clamping of coordinates</span></span>
<span id="cb20-690"><a href="#cb20-690" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> <span class="bu">min</span>(w, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">0</span>]))</span>
<span id="cb20-691"><a href="#cb20-691" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="bu">min</span>(h, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">1</span>]))</span>
<span id="cb20-692"><a href="#cb20-692" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> <span class="bu">min</span>(w, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">2</span>]))</span>
<span id="cb20-693"><a href="#cb20-693" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> <span class="bu">min</span>(h, <span class="bu">max</span>(<span class="dv">0</span>, bbox_float[<span class="dv">3</span>]))</span>
<span id="cb20-694"><a href="#cb20-694" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> { <span class="co"># ...</span></span>
<span id="cb20-695"><a href="#cb20-695" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb20-696"><a href="#cb20-696" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-697"><a href="#cb20-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-698"><a href="#cb20-698" aria-hidden="true" tabindex="-1"></a>When the model was running on a GPU, this mismatch forced an expensive and unnecessary cross-device data transfer during post-processing. The implementation was corrected to explicitly create this tensor on the same device as the model *(device=self._device)*, eliminating the synchronization penalty and ensuring more efficient CUDA utilization.</span>
<span id="cb20-699"><a href="#cb20-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-702"><a href="#cb20-702" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-703"><a href="#cb20-703" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: modified-version</span></span>
<span id="cb20-704"><a href="#cb20-704" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-705"><a href="#cb20-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-706"><a href="#cb20-706" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb20-707"><a href="#cb20-707" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.inference_mode</span>()</span>
<span id="cb20-708"><a href="#cb20-708" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(<span class="va">self</span>, orig_img: Union[Image.Image, np.ndarray]) <span class="op">-&gt;</span> Iterable[<span class="bu">dict</span>]:</span>
<span id="cb20-709"><a href="#cb20-709" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb20-710"><a href="#cb20-710" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Explicitly create the tensor on the same device as the model (e.g., CUDA)</span></span>
<span id="cb20-711"><a href="#cb20-711" aria-hidden="true" tabindex="-1"></a>    target_sizes <span class="op">=</span> torch.tensor([[original_height, original_width]], device<span class="op">=</span><span class="va">self</span>._device)</span>
<span id="cb20-712"><a href="#cb20-712" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> <span class="va">self</span>._image_processor.post_process_object_detection(</span>
<span id="cb20-713"><a href="#cb20-713" aria-hidden="true" tabindex="-1"></a>        outputs, target_sizes<span class="op">=</span>target_sizes, threshold<span class="op">=</span><span class="va">self</span>._threshold</span>
<span id="cb20-714"><a href="#cb20-714" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-715"><a href="#cb20-715" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> results[<span class="dv">0</span>]</span>
<span id="cb20-716"><a href="#cb20-716" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Directly use the results after converting to NumPy arrays</span></span>
<span id="cb20-717"><a href="#cb20-717" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> result[<span class="st">"boxes"</span>].cpu().numpy()</span>
<span id="cb20-718"><a href="#cb20-718" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> result[<span class="st">"scores"</span>].cpu().numpy()</span>
<span id="cb20-719"><a href="#cb20-719" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> result[<span class="st">"labels"</span>].cpu().numpy()</span>
<span id="cb20-720"><a href="#cb20-720" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> score, label_id, box <span class="kw">in</span> <span class="bu">zip</span>(scores, labels, boxes):</span>
<span id="cb20-721"><a href="#cb20-721" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb20-722"><a href="#cb20-722" aria-hidden="true" tabindex="-1"></a>        <span class="co"># No more manual clamping</span></span>
<span id="cb20-723"><a href="#cb20-723" aria-hidden="true" tabindex="-1"></a>        l, t, r, b <span class="op">=</span> box</span>
<span id="cb20-724"><a href="#cb20-724" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> { <span class="co"># ...</span></span>
<span id="cb20-725"><a href="#cb20-725" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb20-726"><a href="#cb20-726" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-727"><a href="#cb20-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-728"><a href="#cb20-728" aria-hidden="true" tabindex="-1"></a>Additionally, the code was modernized by removing redundant manual logic for clamping bounding box coordinates. The original implementation manually ensured that the box coordinates did not exceed the page dimensions after being returned by the post-processing function. The refined version removes this step, properly delegating the responsibility for coordinate clamping to the *transformers* library's *post_process_object_detection* function. This change makes the code cleaner, more maintainable, and adheres more closely to the intended use of the library's API. While subtle, these enhancements to the core predictor were essential for ensuring the stability and high throughput required by the system's parallel architecture.</span>
<span id="cb20-729"><a href="#cb20-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-730"><a href="#cb20-730" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Final Implementation in the ETL Pipeline</span></span>
<span id="cb20-731"><a href="#cb20-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-732"><a href="#cb20-732" aria-hidden="true" tabindex="-1"></a>The re-engineered Docling system is integrated as a distinct stage in the ETL pipeline, orchestrated by the do_docling_extraction function.</span>
<span id="cb20-733"><a href="#cb20-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-736"><a href="#cb20-736" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-737"><a href="#cb20-737" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: docling-extraction-orchestrator</span></span>
<span id="cb20-738"><a href="#cb20-738" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-739"><a href="#cb20-739" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/docling_multi_mp_gui.py</span></span>
<span id="cb20-740"><a href="#cb20-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-741"><a href="#cb20-741" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> do_docling_extraction(df: pd.DataFrame, progress_callback<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb20-742"><a href="#cb20-742" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-743"><a href="#cb20-743" aria-hidden="true" tabindex="-1"></a><span class="co">    Processes DataFrame rows in parallel using config settings.</span></span>
<span id="cb20-744"><a href="#cb20-744" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-745"><a href="#cb20-745" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> MAX_WORKERS_DOCLING </span>
<span id="cb20-746"><a href="#cb20-746" aria-hidden="true" tabindex="-1"></a>    num_records <span class="op">=</span> <span class="bu">len</span>(df)</span>
<span id="cb20-747"><a href="#cb20-747" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the worker count from config, ensuring it's at least 1</span></span>
<span id="cb20-748"><a href="#cb20-748" aria-hidden="true" tabindex="-1"></a>    max_workers_to_use <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, MAX_WORKERS_DOCLING)</span>
<span id="cb20-749"><a href="#cb20-749" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-750"><a href="#cb20-750" aria-hidden="true" tabindex="-1"></a>    output_cols <span class="op">=</span> [<span class="st">"FullText"</span>, <span class="st">"TablesJson"</span>, <span class="st">"EquationsJson"</span>, <span class="st">"TokenCount"</span>, <span class="st">"Error"</span>]</span>
<span id="cb20-751"><a href="#cb20-751" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> output_cols:</span>
<span id="cb20-752"><a href="#cb20-752" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> col <span class="kw">not</span> <span class="kw">in</span> df.columns:</span>
<span id="cb20-753"><a href="#cb20-753" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> col <span class="op">==</span> <span class="st">"TokenCount"</span>: df[col] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-754"><a href="#cb20-754" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> col <span class="op">==</span> <span class="st">"Error"</span>: df[col] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb20-755"><a href="#cb20-755" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>: df[col] <span class="op">=</span> <span class="st">""</span> <span class="co"># Default to empty string for text/JSON string columns</span></span>
<span id="cb20-756"><a href="#cb20-756" aria-hidden="true" tabindex="-1"></a>    futures <span class="op">=</span> {}</span>
<span id="cb20-757"><a href="#cb20-757" aria-hidden="true" tabindex="-1"></a>    processed_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-758"><a href="#cb20-758" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> ProcessPoolExecutor(max_workers<span class="op">=</span>max_workers_to_use, initializer<span class="op">=</span>worker_initializer) <span class="im">as</span> executor:</span>
<span id="cb20-759"><a href="#cb20-759" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, row <span class="kw">in</span> df.iterrows():</span>
<span id="cb20-760"><a href="#cb20-760" aria-hidden="true" tabindex="-1"></a>            pdf_path <span class="op">=</span> row.get(<span class="st">"PDFPath"</span>, <span class="st">""</span>)</span>
<span id="cb20-761"><a href="#cb20-761" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> pdf_path <span class="kw">or</span> <span class="kw">not</span> os.path.exists(pdf_path):</span>
<span id="cb20-762"><a href="#cb20-762" aria-hidden="true" tabindex="-1"></a>                df.loc[idx, output_cols] <span class="op">=</span> [<span class="st">"PDF_PATH_ERROR"</span>, <span class="st">"[]"</span>, <span class="st">"[]"</span>, <span class="dv">0</span>, <span class="st">"PDF_PATH_ERROR"</span>]</span>
<span id="cb20-763"><a href="#cb20-763" aria-hidden="true" tabindex="-1"></a>                processed_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb20-764"><a href="#cb20-764" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb20-765"><a href="#cb20-765" aria-hidden="true" tabindex="-1"></a>            future <span class="op">=</span> executor.submit(extract_pdf_with_docling, pdf_path)</span>
<span id="cb20-766"><a href="#cb20-766" aria-hidden="true" tabindex="-1"></a>            futures[future] <span class="op">=</span> idx <span class="co"># Map future to DataFrame index</span></span>
<span id="cb20-767"><a href="#cb20-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-768"><a href="#cb20-768" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process results as they complete</span></span>
<span id="cb20-769"><a href="#cb20-769" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> future <span class="kw">in</span> as_completed(futures):</span>
<span id="cb20-770"><a href="#cb20-770" aria-hidden="true" tabindex="-1"></a>            irow <span class="op">=</span> futures[future] <span class="co"># Get the original DataFrame index</span></span>
<span id="cb20-771"><a href="#cb20-771" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb20-772"><a href="#cb20-772" aria-hidden="true" tabindex="-1"></a>                result <span class="op">=</span> future.result() <span class="co"># Get the dict returned by worker</span></span>
<span id="cb20-773"><a href="#cb20-773" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Update DataFrame using .loc with index 'irow'</span></span>
<span id="cb20-774"><a href="#cb20-774" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> col <span class="kw">in</span> output_cols:</span>
<span id="cb20-775"><a href="#cb20-775" aria-hidden="true" tabindex="-1"></a>                    df.loc[irow, col] <span class="op">=</span> result.get(col)</span>
<span id="cb20-776"><a href="#cb20-776" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb20-777"><a href="#cb20-777" aria-hidden="true" tabindex="-1"></a>                df.loc[irow, output_cols] <span class="op">=</span> [<span class="st">"FUTURE_ERROR"</span>, <span class="st">"[]"</span>, <span class="st">"[]"</span>, <span class="dv">0</span>, <span class="ss">f"FutureError: </span><span class="sc">{</span><span class="bu">type</span>(e)<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>]</span>
<span id="cb20-778"><a href="#cb20-778" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb20-779"><a href="#cb20-779" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-780"><a href="#cb20-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-781"><a href="#cb20-781" aria-hidden="true" tabindex="-1"></a>This function receives a pandas DataFrame containing paths to the downloaded PDF articles. It distributes the processing of each PDF to the pool of GPU-powered workers. Each worker executes the extract_pdf_with_docling function, which initializes a dedicated DocumentConverter instance configured for its assigned GPU. The worker processes its assigned PDF, extracts the full text (exported as Markdown to preserve structure), and specifically identifies and serializes all tabular data and mathematical formulas into JSON arrays. Upon completion, the structured outputs, FullText, TablesJson, EquationsJson, along with a TokenCount are returned to the main process and integrated back into the corresponding row of the DataFrame. The resulting enriched DataFrame is then saved as a new artifact (output.feather), ready for the subsequent LLM-based field extraction stage.</span>
<span id="cb20-782"><a href="#cb20-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-783"><a href="#cb20-783" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-784"><a href="#cb20-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-785"><a href="#cb20-785" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stage 5: Structured Field Extraction using Large Language Models</span></span>
<span id="cb20-786"><a href="#cb20-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-787"><a href="#cb20-787" aria-hidden="true" tabindex="-1"></a>Following the extraction of raw text via document layout analysis, the pipeline proceeds to the structured field extraction stage, orchestrated by the *scripts/field_extraction.py* module. The objective of this component is to parse the unstructured text from each article and populate a predefined set of structured data fields. This process transforms dense, narrative content into a queryable, machine-readable format suitable for populating the knowledge graph and facilitating systematic analysis.</span>
<span id="cb20-788"><a href="#cb20-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-789"><a href="#cb20-789" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Methodology: Iterative, Context-Aware Extraction with Local LLMs</span></span>
<span id="cb20-790"><a href="#cb20-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-791"><a href="#cb20-791" aria-hidden="true" tabindex="-1"></a>The system employs a sophisticated extraction strategy centered around a locally deployed Large Language Model **(Qwen/Qwen3-32B)**. This self-hosted approach, leveraging the project's multi-GPU hardware, ensures data privacy, eliminates reliance on external API costs and latency, and provides greater control over the inference process. The extraction is not a single pass; rather, it is an iterative process designed to build a rich, cumulative record for each document.</span>
<span id="cb20-792"><a href="#cb20-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-793"><a href="#cb20-793" aria-hidden="true" tabindex="-1"></a>The core of the methodology is the *_process_one_row* function, which operates on each article's full text. To manage the extensive length of academic papers and stay within the LLM's context window, the *chunk_text_for_extraction* utility first splits the full text into overlapping chunks of a configured token size (e.g., 8000 tokens with a 500-token overlap). The system then iterates through these chunks, performing an LLM call for each one.</span>
<span id="cb20-794"><a href="#cb20-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-795"><a href="#cb20-795" aria-hidden="true" tabindex="-1"></a>A key innovation is the use of a **dynamic, context-aware prompt**. For each chunk, the LLM is provided not only with the text of that chunk but also with the current state of the metadata that has been extracted from all previous chunks of the same document. This allows the model to incrementally enrich the data, fill in missing fields, and use previously extracted information as context for interpreting the current text chunk. The prompt dynamically generates a list of fields to be extracted based on the *SELECTED_COLS* defined in the *etl_config.json* file, which are interactively selected via the GUI. Crucially, rather than just providing the field name (e.g., "LCA System Boundaries"), the prompt includes a detailed explanation from the *FIELD_EXPLANATIONS* dictionary, guiding the LLM with a precise definition of the information to look for (e.g., "Describe the scope and boundaries of any Life Cycle Assessment (LCA) mentioned, such as cradle-to-grave, cradle-to-gate...").</span>
<span id="cb20-796"><a href="#cb20-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-799"><a href="#cb20-799" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-800"><a href="#cb20-800" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: field-extraction-prompt-logic</span></span>
<span id="cb20-801"><a href="#cb20-801" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-802"><a href="#cb20-802" aria-hidden="true" tabindex="-1"></a><span class="co"># In scripts/field_extraction.py, illustrating dynamic prompt generation</span></span>
<span id="cb20-803"><a href="#cb20-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-804"><a href="#cb20-804" aria-hidden="true" tabindex="-1"></a><span class="co"># ... inside _process_one_row ...</span></span>
<span id="cb20-805"><a href="#cb20-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-806"><a href="#cb20-806" aria-hidden="true" tabindex="-1"></a><span class="co"># The current state of extracted data is passed as context</span></span>
<span id="cb20-807"><a href="#cb20-807" aria-hidden="true" tabindex="-1"></a>metadata_context_json <span class="op">=</span> json.dumps(final_merged_data, indent<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-808"><a href="#cb20-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-809"><a href="#cb20-809" aria-hidden="true" tabindex="-1"></a><span class="co"># The prompt is formatted with the context, the new text chunk,</span></span>
<span id="cb20-810"><a href="#cb20-810" aria-hidden="true" tabindex="-1"></a><span class="co"># and the list of fields with their detailed explanations.</span></span>
<span id="cb20-811"><a href="#cb20-811" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> extraction_template.<span class="bu">format</span>(</span>
<span id="cb20-812"><a href="#cb20-812" aria-hidden="true" tabindex="-1"></a>    metadata_json<span class="op">=</span>metadata_context_json,</span>
<span id="cb20-813"><a href="#cb20-813" aria-hidden="true" tabindex="-1"></a>    chunk_text<span class="op">=</span>chunk_txt,</span>
<span id="cb20-814"><a href="#cb20-814" aria-hidden="true" tabindex="-1"></a>    field_list_placeholder<span class="op">=</span>field_list_str</span>
<span id="cb20-815"><a href="#cb20-815" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-816"><a href="#cb20-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-817"><a href="#cb20-817" aria-hidden="true" tabindex="-1"></a><span class="co"># ... LLM call is made with this enhanced prompt ...</span></span>
<span id="cb20-818"><a href="#cb20-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-819"><a href="#cb20-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-820"><a href="#cb20-820" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Parallelized Execution and Data Merging</span></span>
<span id="cb20-821"><a href="#cb20-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-822"><a href="#cb20-822" aria-hidden="true" tabindex="-1"></a>To handle the processing of hundreds of articles efficiently, the entire operation is parallelized. The *extract_additional_fields* function uses a *ThreadPoolExecutor* to process multiple articles (rows in the DataFrame) concurrently. Within each of these threads, a nested *ThreadPoolExecutor* is used to process the individual text chunks for that article in parallel. This multi-level parallelism ensures maximum utilization of the available hardware.</span>
<span id="cb20-823"><a href="#cb20-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-824"><a href="#cb20-824" aria-hidden="true" tabindex="-1"></a>The output from each LLM call is a JSON object containing only the fields found within that specific text chunk. A robust merging function, *unify_fields*, is then used to intelligently integrate this new data into the master record for the article. This function handles various data types, uniquely appending items to list-based fields (e.g., Pollutant Terms), overwriting simple text fields with more specific information, and carefully aggregating complex structured data like the Metrics field. To prevent performance degradation from repeated LLM calls on identical text, a cache **(llm_response_cache)** stores the output for each unique prompt, returning the cached result if the same text chunk is ever processed again. This entire process, from chunking to parallelized inference and intelligent merging, results in a richly detailed and structured dataset, which is then saved as an enriched Feather file, ready for the final stages of the ETL pipeline.</span>
<span id="cb20-825"><a href="#cb20-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-826"><a href="#cb20-826" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-827"><a href="#cb20-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-828"><a href="#cb20-828" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stage 6: Thematic Analysis via Topic Modeling</span></span>
<span id="cb20-829"><a href="#cb20-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-830"><a href="#cb20-830" aria-hidden="true" tabindex="-1"></a>Following the extraction of structured data, the pipeline performs a thematic analysis of the textual corpus to identify latent topics, which serves as a primary method for systematic research gap analysis. This is achieved through the implementation of a configurable and parallelized Latent Dirichlet Allocation (LDA) topic modeling workflow, orchestrated by the scripts/topic_modeling_gui.py module. The objective is to distill the vast, unstructured text from hundreds of articles into a set of coherent, interpretable topics whose prevalence and relationships can be quantitatively assessed.</span>
<span id="cb20-831"><a href="#cb20-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-832"><a href="#cb20-832" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Methodology: Probabilistic Topic Modeling with LDA</span></span>
<span id="cb20-833"><a href="#cb20-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-834"><a href="#cb20-834" aria-hidden="true" tabindex="-1"></a>The core of this stage is **Latent Dirichlet Allocation (LDA)**, a generative probabilistic model for discrete data. The foundational assumption of LDA is that each document in a corpus is a mixture of various topics, and each topic is a distribution of words. The model does not know what the topics are in advance; it learns them by analyzing the patterns of word co-occurrence across the entire set of documents.</span>
<span id="cb20-835"><a href="#cb20-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-836"><a href="#cb20-836" aria-hidden="true" tabindex="-1"></a>Mathematically, LDA models a document as being generated by the following process:</span>
<span id="cb20-837"><a href="#cb20-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-838"><a href="#cb20-838" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>For each document $(d)$ in the corpus $(D)$, choose a distribution over topics $( \theta_d \sim \text{Dir}(\alpha))$.</span>
<span id="cb20-839"><a href="#cb20-839" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For each word $(w_n)$ in document $(d)$:</span>
<span id="cb20-840"><a href="#cb20-840" aria-hidden="true" tabindex="-1"></a>    a. Choose a topic $( z_n \sim \text{Categorical}(\theta_d))$.</span>
<span id="cb20-841"><a href="#cb20-841" aria-hidden="true" tabindex="-1"></a>    b. Choose a word $( w_n )$ from $(p(w_n \mid z_n, \beta))$, the probability of word $(w_n)$ given topic $(z_n)$.</span>
<span id="cb20-842"><a href="#cb20-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-843"><a href="#cb20-843" aria-hidden="true" tabindex="-1"></a>The goal of the model training is to infer the hidden variables: the topic distributions per document $(\theta_d)$ and the word distributions per topic $(\beta_k)$. The system uses scikit-learn's implementation of LDA, which employs a variational Bayes algorithm to approximate these posterior distributions. The output for each topic $(k)$ is a probabilistic representation best described as a list of words that are most likely to belong to that topic.</span>
<span id="cb20-844"><a href="#cb20-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-845"><a href="#cb20-845" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Data Preprocessing and Hyperparameter Optimization</span></span>
<span id="cb20-846"><a href="#cb20-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-847"><a href="#cb20-847" aria-hidden="true" tabindex="-1"></a>The quality of an LDA model is highly dependent on both the cleanliness of the input text and the choice of model hyperparameters. The pipeline therefore begins with a rigorous, parallelized text preprocessing workflow (<span class="in">`preprocess_pipeline`</span> function) that tokenizes, removes stopwords, and lemmatizes the text from each document.</span>
<span id="cb20-848"><a href="#cb20-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-849"><a href="#cb20-849" aria-hidden="true" tabindex="-1"></a>To select the optimal hyperparameters for the corpus, the system implements a parallelized grid search, orchestrated by the <span class="in">`_search_best_lda_params`</span> function. The user can define the search space in the <span class="in">`etl_config.json`</span> file interactively via the GUI, specifying ranges for key parameters:</span>
<span id="cb20-850"><a href="#cb20-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-851"><a href="#cb20-851" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`num_topics` (k)**: The number of latent topics to discover. The system intelligently searches over a dynamic range to find the optimal granularity.</span>
<span id="cb20-852"><a href="#cb20-852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`passes / iterations`**: The number of passes the algorithm makes over the corpus during training.</span>
<span id="cb20-853"><a href="#cb20-853" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**n-gram Parameters (`bigram_threshold`, `trigram_threshold`)**: These control the formation of common multi-word phrases (e.g., "cover crop," "random forest model"), treating them as single tokens to generate more coherent topics.</span>
<span id="cb20-854"><a href="#cb20-854" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dictionary Filtering (`no_below`, `no_above`)**: These parameters prune the vocabulary by removing terms that are either too rare (<span class="in">`no_below`</span>) or too common (<span class="in">`no_above`</span>) to be thematically useful.</span>
<span id="cb20-855"><a href="#cb20-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-856"><a href="#cb20-856" aria-hidden="true" tabindex="-1"></a>The grid search trains multiple LDA models in parallel using <span class="in">`ProcessPoolExecutor`</span>, and each resulting model is evaluated using a Topic Coherence score.</span>
<span id="cb20-857"><a href="#cb20-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-858"><a href="#cb20-858" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Model Evaluation: Topic Coherence</span></span>
<span id="cb20-859"><a href="#cb20-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-860"><a href="#cb20-860" aria-hidden="true" tabindex="-1"></a>Topic coherence measures the degree of semantic similarity between the high-scoring words within a topic, providing a quantitative way to assess how interpretable and "human-like" a topic is. This system uses the $(C_v)$ coherence measure, which is based on a sliding window and the normalized pointwise mutual information (NPMI) of word pairs.</span>
<span id="cb20-861"><a href="#cb20-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-862"><a href="#cb20-862" aria-hidden="true" tabindex="-1"></a>For a set of top $(N)$ words $(w_1, w_2, \dots, w_N)$ in a given topic, the $(C_v)$ score is calculated as the average NPMI of all unique word pairs:</span>
<span id="cb20-863"><a href="#cb20-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-864"><a href="#cb20-864" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-865"><a href="#cb20-865" aria-hidden="true" tabindex="-1"></a>\text{Coherence}(V) = \frac{1}{\binom{N}{2}} \sum_{i=1}^{N-1} \sum_{j=i+1}^{N} \text{NPMI}(w_i, w_j)</span>
<span id="cb20-866"><a href="#cb20-866" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-867"><a href="#cb20-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-868"><a href="#cb20-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-869"><a href="#cb20-869" aria-hidden="true" tabindex="-1"></a>where the Normalized Pointwise Mutual Information (NPMI) is given by:</span>
<span id="cb20-870"><a href="#cb20-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-871"><a href="#cb20-871" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-872"><a href="#cb20-872" aria-hidden="true" tabindex="-1"></a>\text{NPMI}(w_i, w_j) = \frac{\log \frac{P(w_i, w_j)}{P(w_i)P(w_j)}}{-\log P(w_i, w_j)}</span>
<span id="cb20-873"><a href="#cb20-873" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-874"><a href="#cb20-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-875"><a href="#cb20-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-876"><a href="#cb20-876" aria-hidden="true" tabindex="-1"></a>Here, $P(w)$ is the probability of seeing word $w$ in a document, and $P(w_i, w_j)$ is the probability of seeing both words in the same document. A score closer to 1 indicates a more coherent topic. The grid search selects the hyperparameter set that produces the model with the highest average $C_v$ score across all its topics. Additionally, the <span class="in">`calculate_topic_coherences`</span> function is used to compute the coherence score for each individual topic from the best model, allowing for a granular assessment of the quality of each thematic cluster.</span>
<span id="cb20-877"><a href="#cb20-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-878"><a href="#cb20-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-879"><a href="#cb20-879" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation and Visualization</span></span>
<span id="cb20-880"><a href="#cb20-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-881"><a href="#cb20-881" aria-hidden="true" tabindex="-1"></a>After the best model is identified, the system assigns a dominant topic to each document and uses an LLM (<span class="in">`Qwen/Qwen3-32B`</span>) to generate a concise, human-readable label for each topic based on its top keywords. Finally, the <span class="in">`generate_lda_visualization.py`</span> script leverages the <span class="in">`pyLDAvis`</span> library to create an interactive visualization. This plot as shown, maps the topics into a 2D space, where the size of each topic's circle represents its prevalence in the corpus and the distance between circles indicates their semantic dissimilarity. This provides a powerful visual tool for identifying prevalent themes and discovering potentially under-represented research areas.</span>
<span id="cb20-882"><a href="#cb20-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-883"><a href="#cb20-883" aria-hidden="true" tabindex="-1"></a>&lt;iframe src="visualizations/lda_vis_custom_labels.html" width="100%" height="800px" style="border:none;"&gt;</span>
<span id="cb20-884"><a href="#cb20-884" aria-hidden="true" tabindex="-1"></a>&lt;/iframe&gt;</span>
<span id="cb20-885"><a href="#cb20-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-886"><a href="#cb20-886" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-887"><a href="#cb20-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-888"><a href="#cb20-888" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stage 7: The Semantic Unification Pipeline</span></span>
<span id="cb20-889"><a href="#cb20-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-890"><a href="#cb20-890" aria-hidden="true" tabindex="-1"></a>This section details the methodology used to resolve terminological ambiguity within the data extracted by the ETL pipeline, ensuring ontological consistency before ingestion into the knowledge graph. This core functionality is encapsulated within the <span class="in">`scripts/dictionaries_gui.py`</span> module.</span>
<span id="cb20-891"><a href="#cb20-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-892"><a href="#cb20-892" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Challenge of Terminological Heterogeneity in Automated Extraction</span></span>
<span id="cb20-893"><a href="#cb20-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-894"><a href="#cb20-894" aria-hidden="true" tabindex="-1"></a>A primary challenge in the automated processing of scientific literature arises from terminological heterogeneity. The field_extraction.py module, which leverages a Large Language Model (LLM), extracts unstructured textual strings that often exhibit significant variation despite referring to the same underlying concept. For example, within the "Tillage Practices" field, semantically equivalent concepts may be described as "no-till," "zero tillage," or "direct drilling." To construct a coherent and queryable knowledge graph, as orchestrated by kg_pipeline_gui.py, it is imperative that these synonymous variations are resolved into a single, canonical entity. The system addresses this challenge not through fragile string matching or rule-based heuristics, but through a robust process of semantic unification, which mathematically quantifies the contextual meaning of extracted terms. This core functionality is encapsulated within the scripts/dictionaries_gui.py module.</span>
<span id="cb20-895"><a href="#cb20-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-896"><a href="#cb20-896" aria-hidden="true" tabindex="-1"></a><span class="fu">#### A Vector Space Model for Semantic Representation</span></span>
<span id="cb20-897"><a href="#cb20-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-898"><a href="#cb20-898" aria-hidden="true" tabindex="-1"></a>The foundational theory underpinning the unification process is the representation of language within a high-dimensional vector space, often referred to as word or phrase embedding. This approach posits that the meaning of a term can be captured by a dense numerical vector, where terms with similar meanings are located closer to each other in this geometric space. This system employs a pre-trained Sentence Transformer model (sentence-transformers/all-MiniLM-L6-v2) to perform this transformation. This specific model generates a 384-dimensional vector for any given textual input, effectively mapping each term to a unique coordinate in a 384-dimensional semantic space.</span>
<span id="cb20-899"><a href="#cb20-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-900"><a href="#cb20-900" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pre-computation of the Canonical Knowledge Base</span></span>
<span id="cb20-901"><a href="#cb20-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-902"><a href="#cb20-902" aria-hidden="true" tabindex="-1"></a>The unification process is implemented through a sequence of operations designed for both accuracy and computational efficiency. At pipeline initialization, the precompute_all function within dictionaries_gui.py is executed. </span>
<span id="cb20-903"><a href="#cb20-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-906"><a href="#cb20-906" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-907"><a href="#cb20-907" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: precompute-all</span></span>
<span id="cb20-908"><a href="#cb20-908" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-909"><a href="#cb20-909" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> precompute_all(config_path<span class="op">=</span><span class="st">"dictionaries_config.json"</span>, device: <span class="bu">str</span> <span class="op">=</span> DEVICE_STR):</span>
<span id="cb20-910"><a href="#cb20-910" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-911"><a href="#cb20-911" aria-hidden="true" tabindex="-1"></a><span class="co">    Loads synonym dicts from config, then builds embeddings for each.</span></span>
<span id="cb20-912"><a href="#cb20-912" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-913"><a href="#cb20-913" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> _precomputed, _synonym_dictionaries, _precomputed_embeddings <span class="co"># Added _precomputed_embeddings</span></span>
<span id="cb20-914"><a href="#cb20-914" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> _precomputed: <span class="cf">return</span></span>
<span id="cb20-915"><a href="#cb20-915" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load synonym dicts first</span></span>
<span id="cb20-916"><a href="#cb20-916" aria-hidden="true" tabindex="-1"></a>    _synonym_dictionaries <span class="op">=</span> load_synonym_dictionaries(config_path) <span class="co"># Use the loading function</span></span>
<span id="cb20-917"><a href="#cb20-917" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> _synonym_dictionaries <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb20-918"><a href="#cb20-918" aria-hidden="true" tabindex="-1"></a>         <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Synonym dictionaries failed to load."</span>)</span>
<span id="cb20-919"><a href="#cb20-919" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> get_st_model(device<span class="op">=</span>device)</span>
<span id="cb20-920"><a href="#cb20-920" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the dictionary to store computed embeddings</span></span>
<span id="cb20-921"><a href="#cb20-921" aria-hidden="true" tabindex="-1"></a>    _precomputed_embeddings <span class="op">=</span> {}</span>
<span id="cb20-922"><a href="#cb20-922" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f"Starting embedding precomputation for </span><span class="sc">{</span><span class="bu">len</span>(_synonym_dictionaries)<span class="sc">}</span><span class="ss"> dictionaries..."</span>)</span>
<span id="cb20-923"><a href="#cb20-923" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamically build embeddings based on loaded dicts</span></span>
<span id="cb20-924"><a href="#cb20-924" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> dict_key, syn_dict <span class="kw">in</span> _synonym_dictionaries.items():</span>
<span id="cb20-925"><a href="#cb20-925" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f"Processing dictionary: </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-926"><a href="#cb20-926" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> syn_dict: <span class="co"># Check if dictionary is not empty</span></span>
<span id="cb20-927"><a href="#cb20-927" aria-hidden="true" tabindex="-1"></a>            phrases, embeds, label_map <span class="op">=</span> build_candidate_embeddings(syn_dict, model<span class="op">=</span>model, device<span class="op">=</span>device)</span>
<span id="cb20-928"><a href="#cb20-928" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> embeds <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="co"># Check if embedding generation was successful</span></span>
<span id="cb20-929"><a href="#cb20-929" aria-hidden="true" tabindex="-1"></a>                 _precomputed_embeddings[dict_key] <span class="op">=</span> {</span>
<span id="cb20-930"><a href="#cb20-930" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"phrases"</span>: phrases,</span>
<span id="cb20-931"><a href="#cb20-931" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"embeds"</span>: embeds.to(device), <span class="co"># Ensure embeddings are on the correct device</span></span>
<span id="cb20-932"><a href="#cb20-932" aria-hidden="true" tabindex="-1"></a>                      <span class="st">"label_map"</span>: label_map</span>
<span id="cb20-933"><a href="#cb20-933" aria-hidden="true" tabindex="-1"></a>                 }</span>
<span id="cb20-934"><a href="#cb20-934" aria-hidden="true" tabindex="-1"></a>                 logger.info(<span class="ss">f"Finished </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span><span class="bu">len</span>(phrases)<span class="sc">}</span><span class="ss"> phrases, embedding shape: </span><span class="sc">{</span>embeds<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-935"><a href="#cb20-935" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-936"><a href="#cb20-936" aria-hidden="true" tabindex="-1"></a>                 logger.error(<span class="ss">f"Embedding generation failed for dictionary: </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-937"><a href="#cb20-937" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-938"><a href="#cb20-938" aria-hidden="true" tabindex="-1"></a>             logger.warning(<span class="ss">f"Skipping empty synonym dictionary: </span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-939"><a href="#cb20-939" aria-hidden="true" tabindex="-1"></a>    _precomputed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb20-940"><a href="#cb20-940" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f"All configured synonym embeddings pre-computed and stored on device '</span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb20-941"><a href="#cb20-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-942"><a href="#cb20-942" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-943"><a href="#cb20-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-944"><a href="#cb20-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-945"><a href="#cb20-945" aria-hidden="true" tabindex="-1"></a>This function iterates through the synonym dictionaries defined in config/dictionaries_config.json. For each dictionary, such as TILLAGE_PRACTICES_SYNONYMS or ML_AI_METHODS_SYNONYMS, it compiles a comprehensive list of all canonical terms and their associated synonyms. Each of these phrases is then passed through the Sentence Transformer model to generate its corresponding 384-dimensional embedding. The resulting collection of vectors and a mapping that links each vector back to its canonical parent term are cached in memory. This pre-computation creates a static, numerically-indexed semantic map of the entire known vocabulary for each category, which allows subsequent matching operations to be performed as highly efficient numerical comparisons, rather than repeated, computationally expensive model inferences.</span>
<span id="cb20-946"><a href="#cb20-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-947"><a href="#cb20-947" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Quantifying Semantic Similarity: The Cosine Similarity Metric</span></span>
<span id="cb20-948"><a href="#cb20-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-949"><a href="#cb20-949" aria-hidden="true" tabindex="-1"></a>Once the field_extraction.py module provides a new, unstructured term for a given field (e.g., "conservation tillage" for the "Tillage Practices" category), the term is first converted into a 384-dimensional query vector, $q$ . The _find_best_match function then systematically compares this query vector to every pre-computed candidate vector, $c$, within the relevant semantic map. </span>
<span id="cb20-950"><a href="#cb20-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-953"><a href="#cb20-953" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-954"><a href="#cb20-954" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: best-match</span></span>
<span id="cb20-955"><a href="#cb20-955" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb20-956"><a href="#cb20-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-957"><a href="#cb20-957" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _find_best_match(query_emb, dict_key):</span>
<span id="cb20-958"><a href="#cb20-958" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Internal helper to find best match using precomputed embeddings."""</span></span>
<span id="cb20-959"><a href="#cb20-959" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> _precomputed_embeddings <span class="co"># Use the dict storing computed data</span></span>
<span id="cb20-960"><a href="#cb20-960" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> _precomputed:</span>
<span id="cb20-961"><a href="#cb20-961" aria-hidden="true" tabindex="-1"></a>        logger.error(<span class="ss">f"Embeddings not precomputed. Cannot unify for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">'. Call precompute_all() first."</span>)</span>
<span id="cb20-962"><a href="#cb20-962" aria-hidden="true" tabindex="-1"></a>        <span class="co"># could trigger precomputation here, but better to do it explicitly at startup</span></span>
<span id="cb20-963"><a href="#cb20-963" aria-hidden="true" tabindex="-1"></a>        <span class="co"># precompute_all()</span></span>
<span id="cb20-964"><a href="#cb20-964" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if not _precomputed: # Check again if precomputation failed</span></span>
<span id="cb20-965"><a href="#cb20-965" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span> </span>
<span id="cb20-966"><a href="#cb20-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-967"><a href="#cb20-967" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dict_key <span class="kw">not</span> <span class="kw">in</span> _precomputed_embeddings:</span>
<span id="cb20-968"><a href="#cb20-968" aria-hidden="true" tabindex="-1"></a>        logger.warning(<span class="ss">f"No precomputed embeddings found for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb20-969"><a href="#cb20-969" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span></span>
<span id="cb20-970"><a href="#cb20-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-971"><a href="#cb20-971" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> _precomputed_embeddings[dict_key]</span>
<span id="cb20-972"><a href="#cb20-972" aria-hidden="true" tabindex="-1"></a>    embeds <span class="op">=</span> data.get(<span class="st">"embeds"</span>)</span>
<span id="cb20-973"><a href="#cb20-973" aria-hidden="true" tabindex="-1"></a>    label_map <span class="op">=</span> data.get(<span class="st">"label_map"</span>)</span>
<span id="cb20-974"><a href="#cb20-974" aria-hidden="true" tabindex="-1"></a>    phrases <span class="op">=</span> data.get(<span class="st">"phrases"</span>) <span class="co"># For logger</span></span>
<span id="cb20-975"><a href="#cb20-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-976"><a href="#cb20-976" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> embeds <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> label_map <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> phrases <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb20-977"><a href="#cb20-977" aria-hidden="true" tabindex="-1"></a>         logger.error(<span class="ss">f"Precomputed data is incomplete for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb20-978"><a href="#cb20-978" aria-hidden="true" tabindex="-1"></a>         <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span></span>
<span id="cb20-979"><a href="#cb20-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-980"><a href="#cb20-980" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb20-981"><a href="#cb20-981" aria-hidden="true" tabindex="-1"></a>        <span class="im">from</span> sentence_transformers <span class="im">import</span> util</span>
<span id="cb20-982"><a href="#cb20-982" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> util.cos_sim(query_emb, embeds)[<span class="dv">0</span>]</span>
<span id="cb20-983"><a href="#cb20-983" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> <span class="bu">float</span>(scores.<span class="bu">max</span>())</span>
<span id="cb20-984"><a href="#cb20-984" aria-hidden="true" tabindex="-1"></a>        best_idx <span class="op">=</span> <span class="bu">int</span>(scores.argmax())</span>
<span id="cb20-985"><a href="#cb20-985" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> best_idx, best_score</span>
<span id="cb20-986"><a href="#cb20-986" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> sim_err:</span>
<span id="cb20-987"><a href="#cb20-987" aria-hidden="true" tabindex="-1"></a>        logger.error(<span class="ss">f"Error during cosine similarity calculation for key '</span><span class="sc">{</span>dict_key<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>sim_err<span class="sc">}</span><span class="ss">"</span>, exc_info<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-988"><a href="#cb20-988" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="fl">0.0</span></span>
<span id="cb20-989"><a href="#cb20-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-990"><a href="#cb20-990" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-991"><a href="#cb20-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-992"><a href="#cb20-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-993"><a href="#cb20-993" aria-hidden="true" tabindex="-1"></a>The metric used for this comparison is the Cosine Similarity, which measures the cosine of the angle (θ) between the two vectors and serves as a measure of their orientation and semantic alignment, independent of their magnitude. The formula for this calculation is:</span>
<span id="cb20-994"><a href="#cb20-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-995"><a href="#cb20-995" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-996"><a href="#cb20-996" aria-hidden="true" tabindex="-1"></a>\operatorname{sim}(\mathbf q,\mathbf c)</span>
<span id="cb20-997"><a href="#cb20-997" aria-hidden="true" tabindex="-1"></a>  = \cos\theta</span>
<span id="cb20-998"><a href="#cb20-998" aria-hidden="true" tabindex="-1"></a>  = \frac{\mathbf q <span class="sc">\!</span>\cdot<span class="sc">\!</span> \mathbf c}</span>
<span id="cb20-999"><a href="#cb20-999" aria-hidden="true" tabindex="-1"></a>         {\lVert \mathbf q \rVert \,\lVert \mathbf c \rVert}</span>
<span id="cb20-1000"><a href="#cb20-1000" aria-hidden="true" tabindex="-1"></a>  = \frac{\displaystyle\sum_{i=1}^{n} q_i\,c_i}</span>
<span id="cb20-1001"><a href="#cb20-1001" aria-hidden="true" tabindex="-1"></a>         {\sqrt{\displaystyle\sum_{i=1}^{n} q_i^{2}}\,</span>
<span id="cb20-1002"><a href="#cb20-1002" aria-hidden="true" tabindex="-1"></a>          \sqrt{\displaystyle\sum_{i=1}^{n} c_i^{2}}}</span>
<span id="cb20-1003"><a href="#cb20-1003" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-1004"><a href="#cb20-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1005"><a href="#cb20-1005" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb20-1006"><a href="#cb20-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1007"><a href="#cb20-1007" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$n$ is the embedding dimension (384 for **all-MiniLM-L6-v2**);</span>
<span id="cb20-1008"><a href="#cb20-1008" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\mathbf q \in \mathbb R^{n}$ is the 384-dimensional vector representing the new, unstructured query term;</span>
<span id="cb20-1009"><a href="#cb20-1009" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\mathbf c \in \mathbb R^{n}$ is a 384-dimensional candidate vector from the pre-computed knowledge base;</span>
<span id="cb20-1010"><a href="#cb20-1010" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$q_i$ and $c_i$ are the $i$-th components of the query and candidate vectors, respectively.</span>
<span id="cb20-1011"><a href="#cb20-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1012"><a href="#cb20-1012" aria-hidden="true" tabindex="-1"></a>The cosine similarity ranges from $-1$ (opposite) to $1$ (identical), where $1$ signifies that the vectors point in the exact same direction (a perfect semantic match), $0$ indicates they are orthogonal (semantically unrelated), and $-1$ indicates they are diametrically opposed.</span>
<span id="cb20-1013"><a href="#cb20-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1014"><a href="#cb20-1014" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Threshold-Based Mapping for Ontological Consistency</span></span>
<span id="cb20-1015"><a href="#cb20-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1016"><a href="#cb20-1016" aria-hidden="true" tabindex="-1"></a>After calculating the similarity score between the query vector and all candidate vectors, the system identifies the maximum score, representing the "closest" known term in the semantic space. The final step of the unification process is to transform this probabilistic similarity score into a deterministic mapping. This is achieved by comparing the highest score against a pre-defined confidence threshold (e.g., 0.55). If the score is greater than or equal to this threshold, the match is accepted as valid, and the system uses the pre-computed label_map to retrieve the canonical term associated with the best-matching vector. This canonical term is then used for ingestion into the knowledge graph. If the highest score falls below the threshold, the query term is considered a non-match, ensuring that ambiguous or out-of-domain terms are rejected, thereby safeguarding the ontological integrity and consistency of the final knowledge graph.</span>
<span id="cb20-1017"><a href="#cb20-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1018"><a href="#cb20-1018" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Knowledge Graph Integration</span></span>
<span id="cb20-1019"><a href="#cb20-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1020"><a href="#cb20-1020" aria-hidden="true" tabindex="-1"></a>The final stage of the ETL pipeline transforms the processed and enriched tabular data into a highly interconnected knowledge graph using a Neo4j graph database. The primary objective is to model the extracted metadata not as isolated rows, but as a network of distinct entities (such as articles, authors, diseases, and methods) and the explicit relationships that connect them. This graph structure enables complex, multi-faceted queries that would be inefficient or impossible with standard relational tables. The entire process, from data preparation to graph ingestion, is orchestrated by scripts/kg_pipeline_gui.py and is driven by a declarative schema defined in the config/kg_pipeline.json file.</span>
<span id="cb20-1021"><a href="#cb20-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1022"><a href="#cb20-1022" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Dynamic Graph Construction</span></span>
<span id="cb20-1023"><a href="#cb20-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1024"><a href="#cb20-1024" aria-hidden="true" tabindex="-1"></a>The construction of the knowledge graph is a dynamic, two-step process for each article processed by the pipeline:</span>
<span id="cb20-1025"><a href="#cb20-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1026"><a href="#cb20-1026" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Article Node Creation:** The process begins by creating or merging a central :Article node, using the article's Digital Object Identifier (DOI) as its unique key. All primary metadata, such as the title, citation count, Zotero key, and the results of the topic modeling stage, are set as properties on this main node.</span>
<span id="cb20-1027"><a href="#cb20-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1028"><a href="#cb20-1028" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Entity and Relationship Mapping** The create_or_update_kg function then iterates through the field_mappings defined in the configuration file. For each field in the dataset (e.g., ml_methods_used), the system dynamically creates MERGE statements in the Cypher query language. For instance, for an article that used "Random Forest", the system ensures a node (:MLMethod {name: 'Random Forest'}) exists and then creates a <span class="co">[</span><span class="ot">:USES_ML_METHOD</span><span class="co">]</span> relationship from the :Article node to it. This procedure is applied across all mapped fields, creating a rich network of interconnected entities such as :HeartDisease, :PollutantTerm, and :StudyType, as visualized in the graph schema (@fig-graph-query-example).</span>
<span id="cb20-1029"><a href="#cb20-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1030"><a href="#cb20-1030" aria-hidden="true" tabindex="-1"></a><span class="al">![Query example](figures/nodes_relationships.PNG)</span>{#fig-graph-query-example}</span>
<span id="cb20-1031"><a href="#cb20-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1032"><a href="#cb20-1032" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Querying for Methodological Insights</span></span>
<span id="cb20-1033"><a href="#cb20-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1034"><a href="#cb20-1034" aria-hidden="true" tabindex="-1"></a>This graph-based structure allows for powerful analytical queries to uncover trends in the literature. For example, to identify the most frequently used machine learning methods specifically within the subset of empirical studies examining ozone and heart disease, the following Cypher query is executed. This query traverses multiple relationship types to precisely filter and aggregate the data, a task well-suited to a graph database.</span>
<span id="cb20-1035"><a href="#cb20-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1036"><a href="#cb20-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1039"><a href="#cb20-1039" aria-hidden="true" tabindex="-1"></a><span class="in">```{cypher}</span></span>
<span id="cb20-1040"><a href="#cb20-1040" aria-hidden="true" tabindex="-1"></a><span class="in">MATCH (a:Article)</span></span>
<span id="cb20-1041"><a href="#cb20-1041" aria-hidden="true" tabindex="-1"></a><span class="in">// Collect study type names per article</span></span>
<span id="cb20-1042"><a href="#cb20-1042" aria-hidden="true" tabindex="-1"></a><span class="in">OPTIONAL MATCH (a)-[:STUDY_TYPE]-&gt;(st:StudyType)</span></span>
<span id="cb20-1043"><a href="#cb20-1043" aria-hidden="true" tabindex="-1"></a><span class="in">WITH a, COLLECT(DISTINCT toLower(st.name)) AS studyTypeNames</span></span>
<span id="cb20-1044"><a href="#cb20-1044" aria-hidden="true" tabindex="-1"></a><span class="in">// Filter for empirical studies only (exclude non-empirical types)</span></span>
<span id="cb20-1045"><a href="#cb20-1045" aria-hidden="true" tabindex="-1"></a><span class="in">WHERE SIZE(studyTypeNames) &gt; 0</span></span>
<span id="cb20-1046"><a href="#cb20-1046" aria-hidden="true" tabindex="-1"></a><span class="in">  AND NONE(stName IN studyTypeNames WHERE stName IN [</span></span>
<span id="cb20-1047"><a href="#cb20-1047" aria-hidden="true" tabindex="-1"></a><span class="in">    'review', 'systematic review', 'meta-analysis', 'expert opinion',</span></span>
<span id="cb20-1048"><a href="#cb20-1048" aria-hidden="true" tabindex="-1"></a><span class="in">    'scoping review', 'dissertation/thesis', 'short communication',</span></span>
<span id="cb20-1049"><a href="#cb20-1049" aria-hidden="true" tabindex="-1"></a><span class="in">    'methodological paper', 'theoretical study', 'report'</span></span>
<span id="cb20-1050"><a href="#cb20-1050" aria-hidden="true" tabindex="-1"></a><span class="in">  ])</span></span>
<span id="cb20-1051"><a href="#cb20-1051" aria-hidden="true" tabindex="-1"></a><span class="in">// Confirm articles explicitly have ozone, ML methods, and heart disease</span></span>
<span id="cb20-1052"><a href="#cb20-1052" aria-hidden="true" tabindex="-1"></a><span class="in">AND EXISTS((a)-[:USES_ML_METHOD]-&gt;(:MLMethod))</span></span>
<span id="cb20-1053"><a href="#cb20-1053" aria-hidden="true" tabindex="-1"></a><span class="in">AND EXISTS((a)-[:ASSOCIATED_WITH_HEART_DISEASE]-&gt;(:HeartDisease))</span></span>
<span id="cb20-1054"><a href="#cb20-1054" aria-hidden="true" tabindex="-1"></a><span class="in">AND EXISTS {</span></span>
<span id="cb20-1055"><a href="#cb20-1055" aria-hidden="true" tabindex="-1"></a><span class="in">  MATCH (a)-[:RELATED_TO_POLLUTANT]-&gt;(oz:PollutantTerm)</span></span>
<span id="cb20-1056"><a href="#cb20-1056" aria-hidden="true" tabindex="-1"></a><span class="in">  WHERE toLower(oz.name) CONTAINS 'ozone'</span></span>
<span id="cb20-1057"><a href="#cb20-1057" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb20-1058"><a href="#cb20-1058" aria-hidden="true" tabindex="-1"></a><span class="in">// Exclude comment/reply-type articles explicitly</span></span>
<span id="cb20-1059"><a href="#cb20-1059" aria-hidden="true" tabindex="-1"></a><span class="in">AND NOT toLower(a.title) CONTAINS 'comment'</span></span>
<span id="cb20-1060"><a href="#cb20-1060" aria-hidden="true" tabindex="-1"></a><span class="in">AND NOT toLower(a.title) CONTAINS 'reply'</span></span>
<span id="cb20-1061"><a href="#cb20-1061" aria-hidden="true" tabindex="-1"></a><span class="in">// Retrieve ML methods and count their occurrences</span></span>
<span id="cb20-1062"><a href="#cb20-1062" aria-hidden="true" tabindex="-1"></a><span class="in">MATCH (a)-[:USES_ML_METHOD]-&gt;(ml:MLMethod)</span></span>
<span id="cb20-1063"><a href="#cb20-1063" aria-hidden="true" tabindex="-1"></a><span class="in">RETURN</span></span>
<span id="cb20-1064"><a href="#cb20-1064" aria-hidden="true" tabindex="-1"></a><span class="in">    ml.name AS `ML Method`,</span></span>
<span id="cb20-1065"><a href="#cb20-1065" aria-hidden="true" tabindex="-1"></a><span class="in">    COUNT(DISTINCT a.doi) AS `Number of Articles`</span></span>
<span id="cb20-1066"><a href="#cb20-1066" aria-hidden="true" tabindex="-1"></a><span class="in">ORDER BY `Number of Articles` DESC</span></span>
<span id="cb20-1067"><a href="#cb20-1067" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1068"><a href="#cb20-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1069"><a href="#cb20-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1070"><a href="#cb20-1070" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1071"><a href="#cb20-1071" aria-hidden="true" tabindex="-1"></a><span class="in">╒════════════════════════════════════╤══════════════════╕</span></span>
<span id="cb20-1072"><a href="#cb20-1072" aria-hidden="true" tabindex="-1"></a><span class="in">│ML Method                           │Number of Articles│</span></span>
<span id="cb20-1073"><a href="#cb20-1073" aria-hidden="true" tabindex="-1"></a><span class="in">╞════════════════════════════════════╪══════════════════╡</span></span>
<span id="cb20-1074"><a href="#cb20-1074" aria-hidden="true" tabindex="-1"></a><span class="in">│"Random Forest"                     │20                │</span></span>
<span id="cb20-1075"><a href="#cb20-1075" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1076"><a href="#cb20-1076" aria-hidden="true" tabindex="-1"></a><span class="in">│"Gradient Boosting Machines"        │14                │</span></span>
<span id="cb20-1077"><a href="#cb20-1077" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1078"><a href="#cb20-1078" aria-hidden="true" tabindex="-1"></a><span class="in">│"Support Vector Machines"           │13                │</span></span>
<span id="cb20-1079"><a href="#cb20-1079" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1080"><a href="#cb20-1080" aria-hidden="true" tabindex="-1"></a><span class="in">│"Neural Networks"                   │12                │</span></span>
<span id="cb20-1081"><a href="#cb20-1081" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1082"><a href="#cb20-1082" aria-hidden="true" tabindex="-1"></a><span class="in">│"K-Means Clustering"                │9                 │</span></span>
<span id="cb20-1083"><a href="#cb20-1083" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1084"><a href="#cb20-1084" aria-hidden="true" tabindex="-1"></a><span class="in">│"Recurrent Neural Networks"         │7                 │</span></span>
<span id="cb20-1085"><a href="#cb20-1085" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1086"><a href="#cb20-1086" aria-hidden="true" tabindex="-1"></a><span class="in">│"Deep Neural Networks"              │7                 │</span></span>
<span id="cb20-1087"><a href="#cb20-1087" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1088"><a href="#cb20-1088" aria-hidden="true" tabindex="-1"></a><span class="in">│"Convolutional Neural Networks"     │5                 │</span></span>
<span id="cb20-1089"><a href="#cb20-1089" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1090"><a href="#cb20-1090" aria-hidden="true" tabindex="-1"></a><span class="in">│"Geographically Weighted Regression"│3                 │</span></span>
<span id="cb20-1091"><a href="#cb20-1091" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1092"><a href="#cb20-1092" aria-hidden="true" tabindex="-1"></a><span class="in">│"Long Short-Term Memory Networks"   │2                 │</span></span>
<span id="cb20-1093"><a href="#cb20-1093" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1094"><a href="#cb20-1094" aria-hidden="true" tabindex="-1"></a><span class="in">│"Hierarchical Clustering"           │2                 │</span></span>
<span id="cb20-1095"><a href="#cb20-1095" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1096"><a href="#cb20-1096" aria-hidden="true" tabindex="-1"></a><span class="in">│"Markov Chain Monte Carlo"          │2                 │</span></span>
<span id="cb20-1097"><a href="#cb20-1097" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1098"><a href="#cb20-1098" aria-hidden="true" tabindex="-1"></a><span class="in">│"Autoencoders"                      │2                 │</span></span>
<span id="cb20-1099"><a href="#cb20-1099" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1100"><a href="#cb20-1100" aria-hidden="true" tabindex="-1"></a><span class="in">│"Prophet"                           │2                 │</span></span>
<span id="cb20-1101"><a href="#cb20-1101" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1102"><a href="#cb20-1102" aria-hidden="true" tabindex="-1"></a><span class="in">│"Principal Component Analysis"      │1                 │</span></span>
<span id="cb20-1103"><a href="#cb20-1103" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1104"><a href="#cb20-1104" aria-hidden="true" tabindex="-1"></a><span class="in">│"Extreme Learning Machines"         │1                 │</span></span>
<span id="cb20-1105"><a href="#cb20-1105" aria-hidden="true" tabindex="-1"></a><span class="in">├────────────────────────────────────┼──────────────────┤</span></span>
<span id="cb20-1106"><a href="#cb20-1106" aria-hidden="true" tabindex="-1"></a><span class="in">│"Named Entity Recognition"          │1                 │</span></span>
<span id="cb20-1107"><a href="#cb20-1107" aria-hidden="true" tabindex="-1"></a><span class="in">└────────────────────────────────────┴──────────────────┘</span></span>
<span id="cb20-1108"><a href="#cb20-1108" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1109"><a href="#cb20-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1110"><a href="#cb20-1110" aria-hidden="true" tabindex="-1"></a>The execution of this query against the knowledge graph yields a clear distribution of methodologies employed across the 34 included studies. Tree-based ensemble methods are most prominent, with Random Forest being the predominant technique, utilized in 20 articles (58.8%). Gradient Boosting Machines and Support Vector Machines were also frequently applied, appearing in 14 and 13 studies, respectively. Neural network architectures showed considerable diversity, with the general category of Neural Networks reported in 12 studies, and more specific forms such as Recurrent and Deep Neural Networks each used in 7 studies. The data indicates a strong trend towards applying multiple ML techniques within a single study, with the 34 articles reporting 102 instances of ML method applications, averaging approximately three distinct methods per paper. This detailed, queryable insight into methodological trends is a direct result of structuring the extracted literature as a knowledge graph.</span>
<span id="cb20-1111"><a href="#cb20-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1112"><a href="#cb20-1112" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-1113"><a href="#cb20-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1114"><a href="#cb20-1114" aria-hidden="true" tabindex="-1"></a><span class="fu">## ETL Pipeline Summary and Output</span></span>
<span id="cb20-1115"><a href="#cb20-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1116"><a href="#cb20-1116" aria-hidden="true" tabindex="-1"></a>The sequential execution of these seven stages transforms a collection of raw, unstructured PDF documents into a highly structured, semantically unified, and queryable knowledge graph. The process is designed to be both robust and efficient, leveraging parallel processing for I/O-bound tasks like API calls and GPU-bound tasks like document parsing. Sophisticated, configurable heuristics are employed at multiple stages, from pre-emptive artifact filtering and layout post-processing to semantic unification to ensure the quality and consistency of the final data product.</span>
<span id="cb20-1117"><a href="#cb20-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1118"><a href="#cb20-1118" aria-hidden="true" tabindex="-1"></a>The ultimate output of this ETL pipeline is a Neo4j graph database. This database serves as the foundational "non-parametric memory" for the downstream Retrieval-Augmented Generation (RAG) system. Each :Article node in the graph is not simply a bibliographic entry, it is a rich object, connected through explicit relationships to its authors, its key concepts, its methodologies, its findings, and its thematic context within the broader literature. It is this interconnected structure, built upon layers of meticulous data processing and enhancement, that enables the RAG agent to perform nuanced, context-aware queries and synthesize high-fidelity, traceable information to address complex research questions.</span>
<span id="cb20-1119"><a href="#cb20-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1120"><a href="#cb20-1120" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-1121"><a href="#cb20-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1122"><a href="#cb20-1122" aria-hidden="true" tabindex="-1"></a><span class="fu"># HySemRAG-QA Framework: An Agentic Approach for Verifiable Generation</span></span>
<span id="cb20-1123"><a href="#cb20-1123" aria-hidden="true" tabindex="-1"></a>With the creation of a structured knowledge graph, the pipeline transitions from data ingestion to its primary purpose of enabling a user to ask complex questions and receive accurate, synthesized, and fully verifiable answers. This is accomplished through the Hybrid Semantic Retrieval-Augmented Generation with Quality Assurance (HySemRAG-QA) framework, a system designed to overcome the critical limitations of standard RAG architectures, namely noisy retrieval and a propensity for LLM hallucination. The system's architecture is founded on a multi-layered approach to ensure the trustworthiness of its final output, from initial query to final validation.</span>
<span id="cb20-1124"><a href="#cb20-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1125"><a href="#cb20-1125" aria-hidden="true" tabindex="-1"></a>The core of the framework is designed to create a "chain of custody" for every piece of information. This begins with a hybrid retrieval engine that goes beyond simple vector search. It combines results from three distinct sources; semantic search (Qdrant), keyword search, and structured graph traversals (Neo4j), and uses Reciprocal Rank Fusion (RRF) to produce a single, highly relevant context.</span>
<span id="cb20-1126"><a href="#cb20-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1127"><a href="#cb20-1127" aria-hidden="true" tabindex="-1"></a>This high-quality context is then passed to an agentic self-correction framework. This is not a single-pass generation process. It implements a multi-agent system where a primary "generator" LLM drafts a cited answer, and a secondary "QA agent" LLM audits that output for factual accuracy, logical consistency, and strict adherence to citation protocols. If the QA agent detects a flaw, it provides corrective feedback, and the generator is forced to revise its work in an iterative loop until a reasoned answer is produced.</span>
<span id="cb20-1128"><a href="#cb20-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1129"><a href="#cb20-1129" aria-hidden="true" tabindex="-1"></a>Finally, to ensure absolute verifiability, the system performs a post-hoc audit. Every citation in the final, agent-approved answer is checked against the ground-truth database to confirm its provenance. As a byproduct of this multi-stage validation process, the system generates a comprehensive interaction log, creating a large-scale, high-quality preference dataset of (prompt, rejected_answer, chosen_answer) triplets suitable for AI model training and fine tuning. The following sections will detail the technical implementation of each of these modules.</span>
<span id="cb20-1130"><a href="#cb20-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1131"><a href="#cb20-1131" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb20-1132"><a href="#cb20-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1133"><a href="#cb20-1133" aria-hidden="true" tabindex="-1"></a>This page will be updated as drafting is complete</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written by Alex Godinez | <a href="https://github.com/agodinezmm2007">GitHub</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>