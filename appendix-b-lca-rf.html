<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Appendix B — Preliminary LCA Cover crop Analysis – Portfolio</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./appendix-c-hardware.html" rel="next">
<link href="./appendix-a-etl-rag.html" rel="prev">
<link href="./pic/cover.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./appendix-a-etl-rag.html">Appendices</a></li><li class="breadcrumb-item"><a href="./appendix-b-lca-rf.html"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Preliminary LCA Cover crop Analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Portfolio</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-proposal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature Analytics and Methodological Gap Analysis in Geospatial Epidemiology</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-data-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Data Collection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-literature-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Literature Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-prelim-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Preliminary Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-aim3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Proposed Deep Learning Framework for UOG Site Monitoring (Aim 3)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-technical-report.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Technical Report</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./update.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Update</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-a-etl-rag.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">appendix-a-etl-rag.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-b-lca-rf.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Preliminary LCA Cover crop Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-c-hardware.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Appendix C: Computational Hardware</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-d-neo4j-queries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Knowledge Graph Queries and Supporting Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-e-db-queries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Database Queries</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#logging-configuration" id="toc-logging-configuration" class="nav-link active" data-scroll-target="#logging-configuration"><span class="header-section-number">B.1</span> Logging Configuration</a></li>
  <li><a href="#data-loading-and-exploration" id="toc-data-loading-and-exploration" class="nav-link" data-scroll-target="#data-loading-and-exploration"><span class="header-section-number">B.2</span> Data Loading and Exploration</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing"><span class="header-section-number">B.3</span> Data Preprocessing</a></li>
  <li><a href="#data-preprocessing-output-explanation" id="toc-data-preprocessing-output-explanation" class="nav-link" data-scroll-target="#data-preprocessing-output-explanation"><span class="header-section-number">B.4</span> Data Preprocessing – Output Explanation</a>
  <ul>
  <li><a href="#columns-after-one-hot-encoding" id="toc-columns-after-one-hot-encoding" class="nav-link" data-scroll-target="#columns-after-one-hot-encoding"><span class="header-section-number">B.4.1</span> Columns after One-Hot Encoding</a></li>
  <li><a href="#columns-after-dropping-excluded-columns" id="toc-columns-after-dropping-excluded-columns" class="nav-link" data-scroll-target="#columns-after-dropping-excluded-columns"><span class="header-section-number">B.4.2</span> Columns after Dropping Excluded Columns</a></li>
  <li><a href="#dropping-constant-columns" id="toc-dropping-constant-columns" class="nav-link" data-scroll-target="#dropping-constant-columns"><span class="header-section-number">B.4.3</span> Dropping Constant Columns</a></li>
  </ul></li>
  <li><a href="#traintest-split" id="toc-traintest-split" class="nav-link" data-scroll-target="#traintest-split"><span class="header-section-number">B.5</span> Train/Test Split</a></li>
  <li><a href="#traintest-split-output-explanation" id="toc-traintest-split-output-explanation" class="nav-link" data-scroll-target="#traintest-split-output-explanation"><span class="header-section-number">B.6</span> Train/Test Split – Output Explanation</a></li>
  <li><a href="#hyperparameter-tuning-with-randomizedsearchcv" id="toc-hyperparameter-tuning-with-randomizedsearchcv" class="nav-link" data-scroll-target="#hyperparameter-tuning-with-randomizedsearchcv"><span class="header-section-number">B.7</span> Hyperparameter Tuning with RandomizedSearchCV</a></li>
  <li><a href="#hyperparameter-tuning-with-randomizedsearchcv-output-explanation" id="toc-hyperparameter-tuning-with-randomizedsearchcv-output-explanation" class="nav-link" data-scroll-target="#hyperparameter-tuning-with-randomizedsearchcv-output-explanation"><span class="header-section-number">B.8</span> Hyperparameter Tuning with RandomizedSearchCV – Output Explanation</a></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation"><span class="header-section-number">B.9</span> Model Evaluation</a>
  <ul>
  <li><a href="#extra-validation-checks" id="toc-extra-validation-checks" class="nav-link" data-scroll-target="#extra-validation-checks"><span class="header-section-number">B.9.1</span> Extra Validation Checks</a></li>
  </ul></li>
  <li><a href="#model-evaluation-output-explanation" id="toc-model-evaluation-output-explanation" class="nav-link" data-scroll-target="#model-evaluation-output-explanation"><span class="header-section-number">B.10</span> Model Evaluation – Output Explanation</a>
  <ul>
  <li><a href="#performance-metrics" id="toc-performance-metrics" class="nav-link" data-scroll-target="#performance-metrics"><span class="header-section-number">B.10.1</span> Performance Metrics</a></li>
  <li><a href="#learning-curves" id="toc-learning-curves" class="nav-link" data-scroll-target="#learning-curves"><span class="header-section-number">B.10.2</span> Learning Curves</a></li>
  <li><a href="#permutation-feature-importance" id="toc-permutation-feature-importance" class="nav-link" data-scroll-target="#permutation-feature-importance"><span class="header-section-number">B.10.3</span> Permutation Feature Importance</a></li>
  <li><a href="#residual-analysis" id="toc-residual-analysis" class="nav-link" data-scroll-target="#residual-analysis"><span class="header-section-number">B.10.4</span> Residual Analysis</a></li>
  <li><a href="#baseline-comparison" id="toc-baseline-comparison" class="nav-link" data-scroll-target="#baseline-comparison"><span class="header-section-number">B.10.5</span> Baseline Comparison</a></li>
  <li><a href="#summary-of-model-evaluation" id="toc-summary-of-model-evaluation" class="nav-link" data-scroll-target="#summary-of-model-evaluation"><span class="header-section-number">B.10.6</span> <strong>Summary of Model Evaluation</strong></a></li>
  </ul></li>
  <li><a href="#feature-importance-and-partial-dependence" id="toc-feature-importance-and-partial-dependence" class="nav-link" data-scroll-target="#feature-importance-and-partial-dependence"><span class="header-section-number">B.11</span> Feature Importance and Partial Dependence</a>
  <ul>
  <li><a href="#built-in-feature-importance" id="toc-built-in-feature-importance" class="nav-link" data-scroll-target="#built-in-feature-importance"><span class="header-section-number">B.11.1</span> Built-in Feature Importance</a></li>
  <li><a href="#bar-chart-visualization" id="toc-bar-chart-visualization" class="nav-link" data-scroll-target="#bar-chart-visualization"><span class="header-section-number">B.11.2</span> Bar Chart Visualization</a></li>
  <li><a href="#partial-dependence-plots-pdp" id="toc-partial-dependence-plots-pdp" class="nav-link" data-scroll-target="#partial-dependence-plots-pdp"><span class="header-section-number">B.11.3</span> Partial Dependence Plots (PDP)</a></li>
  <li><a href="#interpretation-of-key-findings" id="toc-interpretation-of-key-findings" class="nav-link" data-scroll-target="#interpretation-of-key-findings"><span class="header-section-number">B.11.4</span> Interpretation of Key Findings</a></li>
  </ul></li>
  <li><a href="#shap-analysis-for-model-interpretability" id="toc-shap-analysis-for-model-interpretability" class="nav-link" data-scroll-target="#shap-analysis-for-model-interpretability"><span class="header-section-number">B.12</span> SHAP Analysis for Model Interpretability</a>
  <ul>
  <li><a href="#interpretation-of-key-findings-1" id="toc-interpretation-of-key-findings-1" class="nav-link" data-scroll-target="#interpretation-of-key-findings-1"><span class="header-section-number">B.12.1</span> Interpretation of Key Findings</a></li>
  <li><a href="#shap-summary-plot-understanding-feature-effects" id="toc-shap-summary-plot-understanding-feature-effects" class="nav-link" data-scroll-target="#shap-summary-plot-understanding-feature-effects"><span class="header-section-number">B.12.2</span> SHAP Summary Plot: Understanding Feature Effects</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-key-findings-2" id="toc-interpretation-of-key-findings-2" class="nav-link" data-scroll-target="#interpretation-of-key-findings-2"><span class="header-section-number">B.12.2.1</span> Interpretation of Key Findings</a></li>
  </ul></li>
  <li><a href="#shap-force-plot-explaining-a-single-prediction" id="toc-shap-force-plot-explaining-a-single-prediction" class="nav-link" data-scroll-target="#shap-force-plot-explaining-a-single-prediction"><span class="header-section-number">B.12.3</span> SHAP Force Plot: Explaining a Single Prediction</a></li>
  </ul></li>
  <li><a href="#conclusions-from-shap-analysis" id="toc-conclusions-from-shap-analysis" class="nav-link" data-scroll-target="#conclusions-from-shap-analysis"><span class="header-section-number">B.13</span> Conclusions from SHAP Analysis</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./appendix-a-etl-rag.html">Appendices</a></li><li class="breadcrumb-item"><a href="./appendix-b-lca-rf.html"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Preliminary LCA Cover crop Analysis</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-appendix-b-lca" class="quarto-section-identifier">Appendix B — Preliminary LCA Cover crop Analysis</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This appendix documents a preliminary machine learning pipeline developed to predict the total global warming potential (GWP_Total) in agricultural systems using Random Forests. It is presented here not as a definitive study, but as a transparent account of a foundational learning experience.</p>
<p>This initial work was instrumental in developing the more sophisticated data processing techniques and modeling intuition applied in the main body of the dissertation. It highlighted critical challenges in feature selection, the handling of highly skewed distributions, and revealed the limitations of a standard Random Forest model for this type of complex environmental data.</p>
<p>Therefore, while the pipeline performs a full suite of tasks; from preprocessing and hyperparameter tuning to evaluation and SHAP analysis, its primary value lies in demonstrating the iterative process of model development and the analytical growth that informed the subsequent, more advanced research detailed in this portfolio.</p>
<p>The notebook performs the following tasks:</p>
<ol type="1">
<li><strong>Data Loading and Exploration:</strong> Reads the dataset (a Feather file) and inspects its structure and summary statistics.</li>
<li><strong>Data Preprocessing:</strong> Cleans the data by one-hot encoding categorical variables, dropping irrelevant/constant columns, and handling missing values.</li>
<li><strong>Train/Test Split:</strong> Splits the dataset into training and testing sets to ensure that our model generalizes well.</li>
<li><strong>Hyperparameter Tuning:</strong> Uses <code>RandomizedSearchCV</code> to optimize the Random Forest parameters (including <code>min_samples_leaf</code> and <code>max_features</code>) using cross-validation.</li>
<li><strong>Model Evaluation:</strong> Evaluates the best model’s performance using R², RMSE, MAE, Out-of-Bag (OOB) score, learning curves, permutation feature importance, and residual analysis.</li>
<li><strong>Interpretability:</strong> Visualizes feature importance, partial dependence plots, and uses SHAP for both global and local model explanations.</li>
<li><strong>Baseline Comparison:</strong> Compares our Random Forest results with a baseline Linear Regression model.</li>
<li><strong>Logging:</strong> Logs progress and key results for reproducibility and transparency.</li>
</ol>
<p>This pipeline predicts the outcome and provides interpretable insights.</p>
<div id="import-packages" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> r2_score, mean_squared_error, mean_absolute_error</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> PartialDependenceDisplay, permutation_importance</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, RandomizedSearchCV, learning_curve</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold  <span class="co"># if needed for classification; here we use regression</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Print current working directory, version and file of sklearn</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current working directory:"</span>, os.getcwd())</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sklearn version:"</span>, sklearn.__version__)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sklearn location:"</span>, sklearn.<span class="va">__file__</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="logging-configuration" class="level2" data-number="B.1">
<h2 data-number="B.1" class="anchored" data-anchor-id="logging-configuration"><span class="header-section-number">B.1</span> Logging Configuration</h2>
<p>In this section, we set up logging to track the progress and important events throughout the pipeline. Logging ensures that all steps (data loading, preprocessing, model tuning, etc.) are recorded in a log file. This is useful for debugging, reproducibility, and for providing transparency.</p>
<div id="logging-and-configuration" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 0) LOGGING CONFIG</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up logging</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the absolute path for the G: drive (mounted as /mnt/g)</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>LOG_FILE <span class="op">=</span> <span class="st">"/data/GWP_randomforest_2.log"</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    filename<span class="op">=</span>LOG_FILE,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    level<span class="op">=</span>logging.INFO,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"</span><span class="sc">%(asctime)s</span><span class="st"> [</span><span class="sc">%(levelname)s</span><span class="st">] </span><span class="sc">%(message)s</span><span class="st">"</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    datefmt<span class="op">=</span><span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st"> %H:%M:%S"</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Starting the GWP Prediction Pipeline"</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="data-loading-and-exploration" class="level2" data-number="B.2">
<h2 data-number="B.2" class="anchored" data-anchor-id="data-loading-and-exploration"><span class="header-section-number">B.2</span> Data Loading and Exploration</h2>
<p>Here we read the dataset from a Feather file. Feather is a fast binary file format for data frames, making data loading efficient. We then: - <strong>Preview the Data:</strong> Using <code>df.head()</code> to view the first few rows. - <strong>Summarize the Data:</strong> Using <code>df.describe()</code> to get summary statistics (mean, standard deviation, etc.) for each feature.</p>
<p>This step helps us understand the structure, size, and distribution of the data before any processing.</p>
<div id="data-loading-exploration" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Data Loading and Exploration</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 1: Reading data and exploring"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in the data (adjust the file path as needed)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_feather(<span class="st">'data/LCA_DATA.feather'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Data Exploration: Print head and summary statistics</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Data Head:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, df.head())</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Data Summary:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, df.describe())</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Data Head:"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Data Summary:"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.describe())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="data-preprocessing" class="level2" data-number="B.3">
<h2 data-number="B.3" class="anchored" data-anchor-id="data-preprocessing"><span class="header-section-number">B.3</span> Data Preprocessing</h2>
<p>In this section, we prepare the data for modeling by:</p>
<ul>
<li><strong>One-Hot Encoding:</strong> Converting categorical variables (oil, Scenario, LULC, CoverCrop, Rotation) into dummy variables. This allows us to use them in machine learning models.</li>
<li><strong>Dropping Irrelevant Columns:</strong> Removing columns that are not useful for prediction (identifiers, target variables already separated).</li>
<li><strong>Handling Missing Values:</strong> Filling missing values with a simple strategy (here, 0) to avoid issues during training.</li>
<li><strong>Dropping Constant Columns:</strong> Removing columns that have only one unique value since they don’t contribute any information.</li>
</ul>
<p>These steps ensure the dataset is clean and structured appropriately for machine learning.</p>
<div id="data-preprocessing" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Data Preprocessing</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 2: Data preprocessing and one-hot encoding"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define target and categorical columns</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>target_column <span class="op">=</span> <span class="st">"GWP_Total"</span>  <span class="co"># variable to predict</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>categorical_cols <span class="op">=</span> [<span class="st">"Soil"</span>, <span class="st">"Scenario"</span>, <span class="st">"LULC"</span>, <span class="st">"CoverCrop"</span>, <span class="st">"Rotation"</span>]  <span class="co"># categorical features</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>excluded_cols <span class="op">=</span> [target_column, <span class="st">"seed_GWP_main"</span>]  <span class="co"># columns to exclude</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode categorical columns (drop_first=False to retain all info)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>df_encoded <span class="op">=</span> pd.get_dummies(df, columns<span class="op">=</span>categorical_cols, drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Columns after one-hot encoding:"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Columns after one-hot encoding:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, df_encoded.columns)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_encoded.columns)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate target y and drop excluded columns from predictors</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_encoded[target_column]</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>df_encoded <span class="op">=</span> df_encoded.drop(columns<span class="op">=</span>excluded_cols, errors<span class="op">=</span><span class="st">"ignore"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_encoded.drop(columns<span class="op">=</span>excluded_cols, errors<span class="op">=</span><span class="st">"ignore"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill missing values simply and drop constant columns</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>cols_constant <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> X.columns <span class="cf">if</span> X[col].nunique() <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Dropping constant columns: </span><span class="sc">%s</span><span class="st">"</span>, cols_constant)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.drop(columns<span class="op">=</span>cols_constant)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Columns after dropping excluded columns:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, X.columns)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Columns after dropping excluded cols:"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.columns)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dropping constant columns:"</span>, cols_constant)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="data-preprocessing-output-explanation" class="level2" data-number="B.4">
<h2 data-number="B.4" class="anchored" data-anchor-id="data-preprocessing-output-explanation"><span class="header-section-number">B.4</span> Data Preprocessing – Output Explanation</h2>
<p>After running the data preprocessing code, we observe the following outputs:</p>
<p>Columns after one-hot encoding: Index([‘BROIL_FERT’, ‘BROIL_MINN’, ‘BROIL_MINP’, ‘BROIL_ORGN’, ‘BROIL_ORGP’, ‘BROIL_NH3N’, ‘UAN30_FERT’, ‘UAN30_MINN’, ‘UAN30_NH3N’, ‘P_APPkg_ha’, ‘pest_app_LULC’, ‘pest_app_CoverCrop’, ‘sowing_count’, ‘GWP_Total’, ‘total_glyphosate_kg’, ‘seed_rate_main’, ‘seed_rate_cc’, ‘seed_GWP_main’, ‘Soil_LoSa’, ‘Soil_SaLo’, ‘Soil_SiLo’, ‘Scenario_BE’, ‘Scenario_BL’, ‘Scenario_RE’, ‘Scenario_RL’, ‘Scenario_WE’, ‘Scenario_WL’, ‘LULC_SOYb’, ‘LULC_WWHT’, ‘CoverCrop_RYE’, ‘CoverCrop_WWHT’, ‘CoverCrop_WbAR’, ‘Rotation_AGA2’, ‘Rotation_AGAB’, ‘Rotation_AGB1’, ‘Rotation_AGB2’, ‘Rotation_AGC1’, ‘Rotation_AGD1’, ‘Rotation_AGD2’], dtype=‘object’)</p>
<section id="columns-after-one-hot-encoding" class="level3" data-number="B.4.1">
<h3 data-number="B.4.1" class="anchored" data-anchor-id="columns-after-one-hot-encoding"><span class="header-section-number">B.4.1</span> Columns after One-Hot Encoding</h3>
<p>The printed output shows an index (list) of column names:</p>
<p>Index([‘BROIL_FERT’, ‘BROIL_MINN’, ‘BROIL_MINP’, ‘BROIL_ORGN’, ‘BROIL_ORGP’, ‘BROIL_NH3N’, ‘UAN30_FERT’, ‘UAN30_MINN’, ‘UAN30_NH3N’, ‘P_APPkg_ha’, ‘pest_app_LULC’, ‘pest_app_CoverCrop’, ‘sowing_count’, ‘total_glyphosate_kg’, ‘seed_rate_main’, ‘seed_rate_cc’, ‘Soil_LoSa’, ‘Soil_SaLo’, ‘Soil_SiLo’, ‘Scenario_BE’, ‘Scenario_BL’, ‘Scenario_RE’, ‘Scenario_RL’, ‘Scenario_WE’, ‘Scenario_WL’, ‘LULC_SOYb’, ‘LULC_WWHT’, ‘CoverCrop_RYE’, ‘CoverCrop_WWHT’, ‘CoverCrop_WbAR’, ‘Rotation_AGA2’, ‘Rotation_AGAB’, ‘Rotation_AGB1’, ‘Rotation_AGB2’, ‘Rotation_AGC1’, ‘Rotation_AGD1’, ‘Rotation_AGD2’], dtype=‘object’)</p>
<ul>
<li><strong>What This Means:</strong><br>
All the original columns are present, including the target variable <code>GWP_Total</code>. Additionally, the categorical variables (like <code>Soil</code>, <code>Scenario</code>, etc.) have been converted into dummy variables. For example, we see columns like <code>Soil_LoSa</code>, <code>Soil_SaLo</code>, etc., which represent the different levels of the original <code>Soil</code> category.</li>
</ul>
</section>
<section id="columns-after-dropping-excluded-columns" class="level3" data-number="B.4.2">
<h3 data-number="B.4.2" class="anchored" data-anchor-id="columns-after-dropping-excluded-columns"><span class="header-section-number">B.4.2</span> Columns after Dropping Excluded Columns</h3>
<p>After removing the columns that are not needed for prediction (the target variable and any explicitly excluded ones), the output is:</p>
<p>Index([‘BROIL_FERT’, ‘BROIL_MINN’, ‘BROIL_MINP’, ‘BROIL_ORGN’, ‘BROIL_ORGP’, ‘BROIL_NH3N’, ‘UAN30_FERT’, ‘UAN30_MINN’, ‘UAN30_NH3N’, ‘P_APPkg_ha’, ‘pest_app_LULC’, ‘pest_app_CoverCrop’, ‘sowing_count’, ‘total_glyphosate_kg’, ‘seed_rate_main’, ‘seed_rate_cc’, ‘Soil_LoSa’, ‘Soil_SaLo’, ‘Soil_SiLo’, ‘Scenario_BE’, ‘Scenario_BL’, ‘Scenario_RE’, ‘Scenario_RL’, ‘Scenario_WE’, ‘Scenario_WL’, ‘LULC_SOYb’, ‘LULC_WWHT’, ‘CoverCrop_RYE’, ‘CoverCrop_WWHT’, ‘CoverCrop_WbAR’, ‘Rotation_AGA2’, ‘Rotation_AGAB’, ‘Rotation_AGB1’, ‘Rotation_AGB2’, ‘Rotation_AGC1’, ‘Rotation_AGD1’, ‘Rotation_AGD2’], dtype=‘object’)</p>
<ul>
<li><strong>What This Means:</strong><br>
The target column <code>GWP_Total</code> and the excluded column <code>seed_GWP_main</code> have been removed from the predictor matrix <code>X</code>. Now, <code>X</code> contains only the features that will be used to predict <code>GWP_Total</code>.</li>
</ul>
</section>
<section id="dropping-constant-columns" class="level3" data-number="B.4.3">
<h3 data-number="B.4.3" class="anchored" data-anchor-id="dropping-constant-columns"><span class="header-section-number">B.4.3</span> Dropping Constant Columns</h3>
<p>The output: Dropping constant columns: []</p>
<ul>
<li><strong>What This Means:</strong><br>
This indicates that no columns in <code>X</code> had only a single unique value. In other words, every predictor varies to some extent, so no columns were dropped for being constant.</li>
</ul>
<hr>
<p>these outputs confirm that: - Categorical variables have been correctly transformed into dummy variables. - Irrelevant columns (target and specified exclusions) have been successfully removed. - The data is clean and ready for the next steps in the pipeline.</p>
</section>
</section>
<section id="traintest-split" class="level2" data-number="B.5">
<h2 data-number="B.5" class="anchored" data-anchor-id="traintest-split"><span class="header-section-number">B.5</span> Train/Test Split</h2>
<p>We split the dataset into: - <strong>Training Set (80%):</strong> Used to train the model. - <strong>Testing Set (20%):</strong> Held out for evaluating the model’s performance on unseen data.</p>
<p>This split is crucial to validate that our model generalizes well beyond the data it was trained on.</p>
<div id="train-test-split" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Train/Test Split</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 3: Splitting data into train/test"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data (80% train, 20% test)</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Train shape = </span><span class="sc">%s</span><span class="st">, Test shape = </span><span class="sc">%s</span><span class="st">"</span>, X_train.shape, X_test.shape)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Training set size: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, Test set size: </span><span class="sc">{</span>X_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="traintest-split-output-explanation" class="level2" data-number="B.6">
<h2 data-number="B.6" class="anchored" data-anchor-id="traintest-split-output-explanation"><span class="header-section-number">B.6</span> Train/Test Split – Output Explanation</h2>
<p>After splitting the dataset into training and testing sets, we obtain the following:</p>
<ul>
<li><strong>Training Set Size:</strong> (11883, 37)<br>
This means there are 11,883 samples (rows) and 37 features (columns) in the training set.<br>
</li>
<li><strong>Testing Set Size:</strong> (2971, 37)<br>
The test set contains 2,971 samples and the same 37 features.</li>
</ul>
<p><strong>What This Means:</strong></p>
<ul>
<li><p><strong>80/20 Split:</strong><br>
Approximately 80% of the data is used for training the model, and 20% is reserved for evaluating its performance on unseen data. This is crucial to ensure that our model generalizes well and is not overfitting.</p></li>
<li><p><strong>Consistent Feature Dimensions:</strong><br>
Both sets have 37 features, which confirms that the preprocessing steps (e.g., one-hot encoding, dropping irrelevant columns) have been applied consistently across the entire dataset.</p></li>
</ul>
<p>Overall, these numbers indicate a successful split, setting the stage for model training and evaluation.</p>
</section>
<section id="hyperparameter-tuning-with-randomizedsearchcv" class="level2" data-number="B.7">
<h2 data-number="B.7" class="anchored" data-anchor-id="hyperparameter-tuning-with-randomizedsearchcv"><span class="header-section-number">B.7</span> Hyperparameter Tuning with RandomizedSearchCV</h2>
<p>Here, we use <code>RandomizedSearchCV</code> to optimize the hyperparameters of the Random Forest model. Key points include:</p>
<ul>
<li><strong>Parameter Grid:</strong> We define ranges for parameters such as <code>n_estimators</code>, <code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code>, and <code>max_features</code>.</li>
<li><strong>Randomized Search:</strong> Rather than checking every possible combination (which can be computationally expensive), we sample a subset of parameter settings.</li>
<li><strong>Cross-Validation (CV=3):</strong> For each parameter combination, 3-fold cross-validation is used to estimate performance (using negative RMSE here).</li>
</ul>
<p>The best hyperparameters are selected based on cross-validation results, ensuring that the final model is tuned for optimal performance.</p>
<div id="hyperparameter-tuning" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Hyperparameter Tuning with RandomizedSearchCV</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 4: Hyperparameter Tuning with RandomizedSearchCV"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Expanded parameter grid includes min_samples_leaf and max_features</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>param_dist <span class="op">=</span> {</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>],</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>],</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'auto'</span>, <span class="st">'sqrt'</span>]</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate RandomForestRegressor with oob_score enabled for extra validation</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">42</span>, oob_score<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>search <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>rf,</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>param_dist,</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>, </span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>search.fit(X_train, y_train)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> search.best_estimator_</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best params:"</span>, search.best_params_)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Best params from RandomizedSearchCV: </span><span class="sc">%s</span><span class="st">"</span>, search.best_params_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="hyperparameter-tuning-with-randomizedsearchcv-output-explanation" class="level2" data-number="B.8">
<h2 data-number="B.8" class="anchored" data-anchor-id="hyperparameter-tuning-with-randomizedsearchcv-output-explanation"><span class="header-section-number">B.8</span> Hyperparameter Tuning with RandomizedSearchCV – Output Explanation</h2>
<p>In this step, we optimize the hyperparameters of our Random Forest model using <code>RandomizedSearchCV</code>. Here’s what the output tells us:</p>
<ul>
<li><p><strong>Fitting 3 folds for each of 10 candidates, totalling 30 fits:</strong><br>
This message indicates that the search evaluated 10 different hyperparameter configurations (candidates). For each configuration, 3-fold cross-validation was performed (i.e., the training set was split into 3 parts, with 2 used for training and 1 for validation), resulting in a total of 30 model fits.</p></li>
<li><p><strong>Best params:</strong><br>
The output displays the best set of hyperparameters found by the search. In this example, the best parameters are:</p>
<ul>
<li><code>n_estimators</code>: 200 (i.e., the model will use 200 trees)<br>
</li>
<li><code>min_samples_split</code>: 10 (i.e., a node must have at least 10 samples before it can be split)<br>
</li>
<li><code>min_samples_leaf</code>: 2 (i.e., each leaf must have at least 2 samples)<br>
</li>
<li><code>max_features</code>: ‘sqrt’ (i.e., the square root of the number of features will be considered when looking for the best split)<br>
</li>
<li><code>max_depth</code>: 20 (i.e., the maximum depth of each tree is set to 20)</li>
</ul></li>
</ul>
<p><strong>Interpretation:</strong><br>
- The RandomizedSearchCV process sampled 10 configurations from our parameter grid and evaluated each using 3-fold cross-validation.<br>
- The configuration with the lowest (negative) RMSE during cross-validation was chosen.<br>
- These best parameters will be used in the final model, ensuring that our Random Forest is well-tuned to our data, potentially leading to improved performance on unseen data.</p>
<p>This detailed tuning step helps in making our model more robust and generalizable.</p>
</section>
<section id="model-evaluation" class="level2" data-number="B.9">
<h2 data-number="B.9" class="anchored" data-anchor-id="model-evaluation"><span class="header-section-number">B.9</span> Model Evaluation</h2>
<p>This section evaluates the performance of our tuned Random Forest model using multiple metrics:</p>
<ul>
<li><strong>R² (Coefficient of Determination):</strong> Measures the proportion of variance explained by the model.</li>
<li><strong>RMSE (Root Mean Squared Error):</strong> Indicates the average magnitude of the prediction error.</li>
<li><strong>MAE (Mean Absolute Error):</strong> Provides the average absolute error, which is less sensitive to outliers than RMSE.</li>
<li><strong>Out-of-Bag (OOB) Score:</strong> An internal cross-validation metric available in Random Forests when <code>oob_score=True</code>.</li>
</ul>
<section id="extra-validation-checks" class="level3" data-number="B.9.1">
<h3 data-number="B.9.1" class="anchored" data-anchor-id="extra-validation-checks"><span class="header-section-number">B.9.1</span> Extra Validation Checks</h3>
<ul>
<li><strong>Learning Curves:</strong> We plot learning curves to see how training and validation errors change with increasing training set size. This helps diagnose overfitting or underfitting.</li>
<li><strong>Permutation Feature Importance:</strong> Measures how the model’s performance deteriorates when a feature’s values are randomly shuffled. This provides an alternative view of feature importance.</li>
<li><strong>Residual Analysis:</strong> A plot of residuals (actual minus predicted values) to check for any systematic patterns that the model might be missing.</li>
<li><strong>Baseline Comparison:</strong> We also fit a simple Linear Regression model to compare its performance with our Random Forest model.</li>
</ul>
<p>These checks provide a comprehensive view of the model’s performance and robustness.</p>
<div id="model-evaluation" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) Model Evaluation</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5: Evaluating the best model"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions on training and test sets</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> best_model.predict(X_train)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rmse(y_true, y_pred):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_squared_error(y_true, y_pred, squared<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute performance metrics</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>r2_train  <span class="op">=</span> r2_score(y_train, y_train_pred)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>rmse_train <span class="op">=</span> rmse(y_train, y_train_pred)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>mae_train  <span class="op">=</span> mean_absolute_error(y_train, y_train_pred)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>r2_test   <span class="op">=</span> r2_score(y_test, y_test_pred)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>rmse_test <span class="op">=</span> rmse(y_test, y_test_pred)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>mae_test  <span class="op">=</span> mean_absolute_error(y_test, y_test_pred)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Training set: R^2 = </span><span class="sc">%.3f</span><span class="st">, RMSE = </span><span class="sc">%.3f</span><span class="st">, MAE = </span><span class="sc">%.3f</span><span class="st">"</span>, r2_train, rmse_train, mae_train)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Test set:     R^2 = </span><span class="sc">%.3f</span><span class="st">, RMSE = </span><span class="sc">%.3f</span><span class="st">, MAE = </span><span class="sc">%.3f</span><span class="st">"</span>, r2_test, rmse_test, mae_test)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Model Performance:"</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: R^2 = </span><span class="sc">{</span>r2_train<span class="sc">:.3f}</span><span class="ss">, RMSE = </span><span class="sc">{</span>rmse_train<span class="sc">:.3f}</span><span class="ss">, MAE = </span><span class="sc">{</span>mae_train<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test set: R^2 = </span><span class="sc">{</span>r2_test<span class="sc">:.3f}</span><span class="ss">, RMSE = </span><span class="sc">{</span>rmse_test<span class="sc">:.3f}</span><span class="ss">, MAE = </span><span class="sc">{</span>mae_test<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB Error (if available)</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(best_model, <span class="st">"oob_score_"</span>):</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    oob_score <span class="op">=</span> best_model.oob_score_</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    logging.info(<span class="st">"OOB Score: </span><span class="sc">%.3f</span><span class="st">"</span>, oob_score)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"OOB Score: </span><span class="sc">{</span>oob_score<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Learning Curves</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5A: Generating learning curves"</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    best_model, X_train, y_train, cv<span class="op">=</span><span class="dv">3</span>, scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    train_sizes<span class="op">=</span>np.linspace(<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="dv">10</span>), n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>train_scores_mean <span class="op">=</span> <span class="op">-</span>np.mean(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>test_scores_mean <span class="op">=</span> <span class="op">-</span>np.mean(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, train_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"r"</span>, label<span class="op">=</span><span class="st">"Training RMSE"</span>)</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, test_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"g"</span>, label<span class="op">=</span><span class="st">"Validation RMSE"</span>)</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Learning Curves"</span>)</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Training examples"</span>)</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RMSE"</span>)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Learning curves generated"</span>)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Permutation Feature Importance</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5B: Calculating permutation feature importance"</span>)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>perm_importance <span class="op">=</span> permutation_importance(best_model, X_test, y_test, scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>, n_repeats<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>perm_importance_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: X_test.columns,</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance_mean'</span>: perm_importance.importances_mean,</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance_std'</span>: perm_importance.importances_std</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>}).sort_values(by<span class="op">=</span><span class="st">'importance_mean'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Permutation Feature Importance:"</span>)</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(perm_importance_df)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Permutation Feature Importance:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, perm_importance_df)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Residual Analysis</span></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5C: Plotting residuals"</span>)</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_test <span class="op">-</span> y_test_pred</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test_pred, residuals, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted Values"</span>)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Residuals (Actual - Predicted)"</span>)</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Residual Plot"</span>)</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Residual plot generated"</span>)</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Baseline Model Comparison</span></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5D: Comparing with baseline Linear Regression"</span>)</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>baseline_model <span class="op">=</span> LinearRegression()</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>baseline_model.fit(X_train, y_train)</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>y_test_pred_baseline <span class="op">=</span> baseline_model.predict(X_test)</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>r2_baseline <span class="op">=</span> r2_score(y_test, y_test_pred_baseline)</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>rmse_baseline <span class="op">=</span> rmse(y_test, y_test_pred_baseline)</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>mae_baseline <span class="op">=</span> mean_absolute_error(y_test, y_test_pred_baseline)</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Baseline Linear Regression Performance:"</span>)</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R^2 = </span><span class="sc">{</span>r2_baseline<span class="sc">:.3f}</span><span class="ss">, RMSE = </span><span class="sc">{</span>rmse_baseline<span class="sc">:.3f}</span><span class="ss">, MAE = </span><span class="sc">{</span>mae_baseline<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Baseline Linear Regression: R^2 = </span><span class="sc">%.3f</span><span class="st">, RMSE = </span><span class="sc">%.3f</span><span class="st">, MAE = </span><span class="sc">%.3f</span><span class="st">"</span>, r2_baseline, rmse_baseline, mae_baseline)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="model-evaluation-output-explanation" class="level2" data-number="B.10">
<h2 data-number="B.10" class="anchored" data-anchor-id="model-evaluation-output-explanation"><span class="header-section-number">B.10</span> Model Evaluation – Output Explanation</h2>
<p>After training our tuned Random Forest model, we evaluate its performance using multiple metrics and additional validation checks.</p>
<section id="performance-metrics" class="level3" data-number="B.10.1">
<h3 data-number="B.10.1" class="anchored" data-anchor-id="performance-metrics"><span class="header-section-number">B.10.1</span> Performance Metrics</h3>
<p><strong>Printed Output:</strong> Training set: R^2 = 0.702, RMSE = 3001.564, MAE = 2025.280 Test set: R^2 = 0.668, RMSE = 3155.660, MAE = 2147.089 OOB Score: 0.661</p>
<ul>
<li><strong>R² (Coefficient of Determination):</strong>
<ul>
<li><strong>Training R² = 0.702</strong><br>
</li>
<li><strong>Test R² = 0.668</strong><br>
The model explains roughly 70% of the variance in the training set and about 67% in the test set. This small gap indicates that the model generalizes reasonably well with some overfitting.</li>
</ul></li>
<li><strong>RMSE (Root Mean Squared Error):</strong>
<ul>
<li><strong>Training RMSE ≈ 3001</strong><br>
</li>
<li><strong>Test RMSE ≈ 3156</strong><br>
RMSE gives the average magnitude of the prediction error. A difference of about 155 between training and test suggests the model’s performance on unseen data is close to its performance on training data.</li>
</ul></li>
<li><strong>MAE (Mean Absolute Error):</strong>
<ul>
<li><strong>Training MAE ≈ 2025</strong><br>
</li>
<li><strong>Test MAE ≈ 2147</strong><br>
MAE is less sensitive to outliers than RMSE. These values confirm a consistent level of error on training vs.&nbsp;test sets.</li>
</ul></li>
<li><strong>Out-of-Bag (OOB) Score = 0.661</strong><br>
Random Forests can estimate performance internally (without a separate validation set) by using out-of-bag samples. An OOB score of 0.661 aligns closely with the test R² of 0.668, giving additional confidence in the model’s ability to generalize.</li>
</ul>
</section>
<section id="learning-curves" class="level3" data-number="B.10.2">
<h3 data-number="B.10.2" class="anchored" data-anchor-id="learning-curves"><span class="header-section-number">B.10.2</span> Learning Curves</h3>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="pic/learning_curves.png" class="img-fluid figure-img"></p>
<figcaption>Learning Curves</figcaption>
</figure>
</div>
<p>The <strong>Learning Curves</strong> plot shows how RMSE (vertical axis) changes for both the training set (red line) and validation set (green line) as the number of training examples increases (horizontal axis). Key observations:</p>
<ul>
<li><strong>Training RMSE</strong> remains relatively stable around ~3000, indicating the model fits the training data consistently.</li>
<li><strong>Validation RMSE</strong> decreases from about 3700 to 3200 as more training data is used, suggesting that adding more data helps reduce error.</li>
<li>The gap between training and validation curves narrows but does not fully converge, indicating the model could still benefit from more data or further hyperparameter tuning. However, the gap is not excessively large, suggesting a reasonable balance between bias and variance.</li>
</ul>
</section>
<section id="permutation-feature-importance" class="level3" data-number="B.10.3">
<h3 data-number="B.10.3" class="anchored" data-anchor-id="permutation-feature-importance"><span class="header-section-number">B.10.3</span> Permutation Feature Importance</h3>
<p><strong>What It Means:</strong><br>
Permutation Importance measures how much the model’s performance deteriorates when each feature’s values are randomly shuffled. Features with higher <code>importance_mean</code> cause a greater increase in RMSE when shuffled, indicating they are more crucial to the model. This provides an alternative perspective on which predictors matter most, complementing the built-in feature importance from the Random Forest itself.</p>
</section>
<section id="residual-analysis" class="level3" data-number="B.10.4">
<h3 data-number="B.10.4" class="anchored" data-anchor-id="residual-analysis"><span class="header-section-number">B.10.4</span> Residual Analysis</h3>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="pic/residual_plot.png" class="img-fluid figure-img"></p>
<figcaption>Residual Plot</figcaption>
</figure>
</div>
<p>The <strong>Residual Plot</strong> (actual minus predicted values on the y-axis vs.&nbsp;predicted values on the x-axis) helps identify patterns in the errors:</p>
<ul>
<li><p>The red dashed line at y=0 indicates perfect predictions.</p></li>
<li><p>Residuals appear mostly centered around 0, with some spread at higher predicted values.</p></li>
<li><p>No strong pattern or trend suggests the model is systematically over- or under-predicting at particular ranges.</p></li>
<li><p>The moderate spread indicates the model still has some variance in its predictions, but there is no major sign of severe bias.</p></li>
</ul>
</section>
<section id="baseline-comparison" class="level3" data-number="B.10.5">
<h3 data-number="B.10.5" class="anchored" data-anchor-id="baseline-comparison"><span class="header-section-number">B.10.5</span> Baseline Comparison</h3>
<p>Baseline Linear Regression Performance: R^2 = 0.576, RMSE = 3566.148, MAE = 2570.892</p>
<ul>
<li><strong>Linear Regression R² = 0.576</strong><br>
</li>
<li><strong>Linear Regression RMSE ≈ 3566</strong><br>
</li>
<li><strong>Linear Regression MAE ≈ 2571</strong></li>
</ul>
<p>Compared to our Random Forest (R² = 0.668), the baseline linear model performs worse across all metrics (lower R², higher RMSE, and higher MAE). This shows that the additional complexity of the Random Forest provides a noticeable performance boost and better captures the underlying relationships in the data.</p>
<hr>
</section>
<section id="summary-of-model-evaluation" class="level3" data-number="B.10.6">
<h3 data-number="B.10.6" class="anchored" data-anchor-id="summary-of-model-evaluation"><span class="header-section-number">B.10.6</span> <strong>Summary of Model Evaluation</strong></h3>
<ol type="1">
<li><strong>Metrics</strong> show the Random Forest performing well, with a small gap between training and test performance.</li>
<li><strong>Learning Curves</strong> suggest the model benefits from more data and is not drastically overfitting.</li>
<li><strong>Permutation Importance</strong> corroborates which features the model relies on most.</li>
<li><strong>Residual Plot</strong> reveals no major systematic bias in predictions, though there is some variance at higher values.</li>
<li><strong>Baseline Comparison</strong> confirms that the Random Forest outperforms a simpler Linear Regression model.</li>
</ol>
<p>Overall, these results indicate a reasonably robust model that generalizes better than a simple baseline and provides insights into which features matter most.</p>
</section>
</section>
<section id="feature-importance-and-partial-dependence" class="level2" data-number="B.11">
<h2 data-number="B.11" class="anchored" data-anchor-id="feature-importance-and-partial-dependence"><span class="header-section-number">B.11</span> Feature Importance and Partial Dependence</h2>
<p>Understanding which features drive the model predictions is key for interpretability. In this section, we:</p>
<ul>
<li><strong>Built-in Feature Importance:</strong> Leverage the Random Forest’s internal mechanism to rank features based on how much they reduce impurity.</li>
<li><strong>Bar Plot Visualization:</strong> Display the top 10 features to quickly identify the most influential predictors.</li>
<li><strong>Partial Dependence Plots (PDP):</strong> Visualize the effect of the top features on the predicted outcome while holding other features constant. This helps in understanding the relationship between predictors and the target variable.</li>
</ul>
<p>These tools help understand the model’s decision-making process.</p>
<div id="feature-importance-partial-dependence" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) Feature Importance and Partial Dependence</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">FEATURE IMPORTANCE:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Lists the top 10 features based on the model’s built-in importance scores.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Produces a bar chart so we can visually interpret which features are most influential.</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 6: Feature importance plots &amp; partial dependence"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Built-in feature importance from Random Forest</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> best_model.feature_importances_</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>feat_imp <span class="op">=</span> pd.Series(importances, index<span class="op">=</span>X_train.columns).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Top 10 Features:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, feat_imp.head(<span class="dv">20</span>))</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 10 Feature Importances:"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(feat_imp.head(<span class="dv">10</span>))</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Bar plot for feature importance</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span>feat_imp.head(<span class="dv">10</span>), y<span class="op">=</span>feat_imp.head(<span class="dv">10</span>).index, palette<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top 10 Feature Importances (Random Forest)"</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Importance"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Feature"</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial Dependence Plots for top features</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>top_features <span class="op">=</span> feat_imp.head(<span class="dv">3</span>).index.tolist()</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Generating partial dependence plots for top features: </span><span class="sc">%s</span><span class="st">"</span>, top_features)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>PartialDependenceDisplay.from_estimator(best_model, X_train, top_features, n_jobs<span class="op">=-</span><span class="dv">1</span>, ax<span class="op">=</span>ax)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">"Partial Dependence of Top Features"</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="built-in-feature-importance" class="level3" data-number="B.11.1">
<h3 data-number="B.11.1" class="anchored" data-anchor-id="built-in-feature-importance"><span class="header-section-number">B.11.1</span> Built-in Feature Importance</h3>
<p><strong>Printed Output (Top 10 Features):</strong></p>
<p>Soil_SaLo 0.077287 Soil_SiLo 0.076775 BROIL_MINN 0.065416 pest_app_LULC 0.056814 seed_rate_main 0.055316 P_APPkg_ha 0.053626 Rotation_AGC1 0.052144 BROIL_MINP 0.050464 BROIL_NH3N 0.046516 BROIL_ORGN 0.043727 dtype: float64</p>
<ul>
<li><strong>What It Means:</strong><br>
The Random Forest model computes an “importance” score for each feature, reflecting how much that feature reduces impurity (variance) in the trees. Higher values indicate more influential predictors.
<ul>
<li><strong>Soil_SaLo and Soil_SiLo</strong> appear to be the top two features, suggesting that soil types have a significant impact on <code>GWP_Total</code>.<br>
</li>
<li><strong>BROIL_MINN</strong> (a nitrogen-related feature) also ranks high, indicating fertilizer- or manure-related variables may be critical to the model.<br>
</li>
<li><strong>pest_app_LULC</strong>, <strong>seed_rate_main</strong>, and <strong>P_APPkg_ha</strong> follow, further suggesting that application rates and crop management choices drive <code>GWP_Total</code>.</li>
</ul></li>
</ul>
</section>
<section id="bar-chart-visualization" class="level3" data-number="B.11.2">
<h3 data-number="B.11.2" class="anchored" data-anchor-id="bar-chart-visualization"><span class="header-section-number">B.11.2</span> Bar Chart Visualization</h3>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="pic/feature_importance.png" class="img-fluid figure-img"></p>
<figcaption>Top 10 Feature Importances (Random Forest)</figcaption>
</figure>
</div>
<p>In the bar chart:</p>
<ul>
<li><p><strong>Horizontal Axis (Importance):</strong> Shows how crucial each feature is.</p></li>
<li><p><strong>Vertical Axis (Feature):</strong> Lists the top 10 features in descending order of importance.</p></li>
</ul>
<p>This chart helps both technical and non-technical audiences quickly see which variables most influence the model’s predictions.</p>
</section>
<section id="partial-dependence-plots-pdp" class="level3" data-number="B.11.3">
<h3 data-number="B.11.3" class="anchored" data-anchor-id="partial-dependence-plots-pdp"><span class="header-section-number">B.11.3</span> Partial Dependence Plots (PDP)</h3>
<p>Partial Dependence Plots help us understand how the most important features influence the model’s prediction for GWP_Total. Each plot shows the effect of a single feature while averaging out the effects of all other features.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="pic/partial_dependence.png" class="img-fluid figure-img"></p>
<figcaption>Partial Dependence of Top Features</figcaption>
</figure>
</div>
<ul>
<li><strong>What PDP Shows:</strong><br>
Each subplot corresponds to one of the top 3 features. On the x-axis is the feature’s possible values; on the y-axis is the <strong>partial dependence</strong>, which represents the model’s predicted outcome when varying that feature while holding others constant.</li>
</ul>
</section>
<section id="interpretation-of-key-findings" class="level3" data-number="B.11.4">
<h3 data-number="B.11.4" class="anchored" data-anchor-id="interpretation-of-key-findings"><span class="header-section-number">B.11.4</span> Interpretation of Key Findings</h3>
<ul>
<li><p><strong>Soil_SaLo (Sandy Loam Soil)</strong><br>
This plot shows a strong negative relationship. The model predicts a significantly lower <strong>GWP_Total</strong> when the soil type is Sandy Loam (value of <code>1</code>) compared to other types. This suggests that sandy loam soil is a key factor in mitigating GWP in this model.</p></li>
<li><p><strong>Soil_SiLo (Silty Loam Soil)</strong><br>
In contrast, the model associates Silty Loam soil (value of <code>1</code>) with an increase in the predicted <strong>GWP_Total</strong>.</p></li>
<li><p><strong>BROIL_MINN</strong><br>
This feature displays a clear non-linear relationship. The model predicts the lowest <strong>GWP_Total</strong> when <strong>BROIL_MINN</strong> is at a “sweet spot” of approximately <code>25</code>. Values lower or higher than this optimum level are associated with an increase in predicted GWP. This complex pattern highlights the model’s ability to capture relationships that a standard linear regression would miss.</p></li>
</ul>
</section>
</section>
<section id="shap-analysis-for-model-interpretability" class="level2" data-number="B.12">
<h2 data-number="B.12" class="anchored" data-anchor-id="shap-analysis-for-model-interpretability"><span class="header-section-number">B.12</span> SHAP Analysis for Model Interpretability</h2>
<p>To understand the drivers of the model’s predictions, SHAP (SHapley Additive exPlanations) analysis was performed. The following bar plot shows the global feature importance, ranking features by their average impact on the model’s output magnitude.</p>
<section id="interpretation-of-key-findings-1" class="level3" data-number="B.12.1">
<h3 data-number="B.12.1" class="anchored" data-anchor-id="interpretation-of-key-findings-1"><span class="header-section-number">B.12.1</span> Interpretation of Key Findings</h3>
<ul>
<li><strong>Primary Drivers (Soil Type):</strong> Consistent with other importance measures, soil types are the most influential features. Soil_SaLo (Sandy Loam) has the largest average impact on the model’s predictions, followed closely by Soil_LoSa (Loamy Sand).</li>
<li><strong>Secondary Drivers (Management &amp; Inputs):</strong> After soil type, key management and input variables show significant importance. These include BROIL_MINN (a nitrogen-related feature), seed_rate_main, and pest_app_LULC (pesticide application).</li>
<li><strong>Tertiary Drivers:</strong> Other nitrogen and phosphorus-related inputs (BROIL_MINP, P_APPkg_ha) and specific crop rotations (Rotation_AGC1) also contribute meaningfully to the predictions, but with less overall impact than the primary and secondary drivers.</li>
</ul>
<p>This ranking confirms that the model’s predictions for GWP_Total are most heavily influenced by the inherent soil characteristics of a site, followed by key agricultural inputs and management decisions.</p>
<div id="shap-analysis" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 7) SHAP Analysis for Interpretability</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">SHAP ANALYSIS</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Provides global interpretability through SHAP summary plots.</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Gives a local explanation (force plot) for one individual test case, </span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    which shows how each feature contributes to a single prediction.</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 7: SHAP analysis for global &amp; local interpretability"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># SHAP: Global and local interpretability</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.TreeExplainer(best_model)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer.shap_values(X_train)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Global SHAP plots (bar and summary)</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values, X_train, plot_type<span class="op">=</span><span class="st">"bar"</span>, show<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"SHAP Feature Importance (Bar Plot)"</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values, X_train, show<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"SHAP Summary Plot"</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Local SHAP explanation for the first test instance</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Generating SHAP force plot for the first test instance"</span>)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Generating SHAP force plot for the first test instance..."</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>instance <span class="op">=</span> X_test.iloc[<span class="dv">0</span>]</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>instance_shap <span class="op">=</span> explainer.shap_values(instance)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>shap.force_plot(explainer.expected_value, instance_shap, instance, matplotlib<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="pic/shap_bar_plot.png" class="img-fluid figure-img"></p>
<figcaption>SHAP Feature Importance (Bar Plot)</figcaption>
</figure>
</div>
<hr>
</section>
<section id="shap-summary-plot-understanding-feature-effects" class="level3" data-number="B.12.2">
<h3 data-number="B.12.2" class="anchored" data-anchor-id="shap-summary-plot-understanding-feature-effects"><span class="header-section-number">B.12.2</span> SHAP Summary Plot: Understanding Feature Effects</h3>
<p>The SHAP summary plot provides a powerful global view of the model’s behavior. It reveals not only the importance of each feature but also the direction and consistency of its effect on the <strong>GWP_Total</strong> prediction.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="pic/shap_summary_plot.png" class="img-fluid figure-img"></p>
<figcaption>SHAP Summary Plot</figcaption>
</figure>
</div>
<section id="interpretation-of-key-findings-2" class="level4" data-number="B.12.2.1">
<h4 data-number="B.12.2.1" class="anchored" data-anchor-id="interpretation-of-key-findings-2"><span class="header-section-number">B.12.2.1</span> Interpretation of Key Findings</h4>
<p>This plot reveals several clear patterns learned by the model:</p>
<p><strong>The Dominant Effect of Soil Type:</strong></p>
<ul>
<li><strong>Soil_SaLo (Sandy Loam) &amp; Soil_LoSa (Loamy Sand):</strong> These are the two most important features. The plot shows a very clean separation: when these soils are present (red dots, feature value = 1), they consistently have a large negative SHAP value, meaning they strongly push the model to predict a lower GWP. Their absence (blue dots, feature value = 0) results in a positive SHAP value, increasing the predicted GWP.<br>
</li>
<li><strong>Soil_SiLo (Silty Loam):</strong> This feature shows the opposite effect. Its presence (red dots) consistently pushes the prediction higher (positive SHAP values).</li>
</ul>
<p><strong>The Non-Linear Impact of Nitrogen (BROIL_MINN):</strong></p>
<ul>
<li>This feature shows a complex, non-linear pattern. Both low values (blue dots) and high values (red dots) push the GWP prediction higher. The optimal values that push the prediction lower (negative SHAP values) are in the middle of the range (represented by the purple-ish dots). This confirms the “V” shape we saw in the Partial Dependence Plot, indicating an optimal “sweet spot” for this input.</li>
</ul>
<p><strong>Impact of Management Practices:</strong></p>
<ul>
<li><strong>seed_rate_main &amp; pest_app_LULC:</strong> For both of these features, there is a clear trend. High values (red dots) are on the right, pushing the prediction higher, while low values (blue dots) are on the left, pushing it lower. This indicates the model has learned a positive correlation: higher seeding and pesticide rates are associated with higher predicted GWP.</li>
</ul>
<p>This single visualization confirms the model’s logic. It’s not just a black box; it has learned scientifically plausible relationships, such as the strong mitigating effect of certain soil types and the non-linear impact of nitrogen inputs.</p>
<hr>
</section>
</section>
<section id="shap-force-plot-explaining-a-single-prediction" class="level3" data-number="B.12.3">
<h3 data-number="B.12.3" class="anchored" data-anchor-id="shap-force-plot-explaining-a-single-prediction"><span class="header-section-number">B.12.3</span> SHAP Force Plot: Explaining a Single Prediction</h3>
<p>This force plot provides a detailed look at how the model arrived at its prediction for one specific data point in your dataset. It can be described as a “tug-of-war” between features that push the prediction higher and those that pull it lower.</p>
<p>Lets walk through the story of this specific prediction, from right to left:</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="pic/shap_force_plot.png" class="img-fluid figure-img"></p>
<figcaption>SHAP Force Plot</figcaption>
</figure>
</div>
<p><strong>The Starting Point (base value):</strong> The model starts with the base value, which is the average prediction for GWP_Total across all data points in your training set. While the exact number isn’t represented, we can see it’s a positive value on the x-axis. This is the model’s initial “guess” before considering any of this specific data points features.</p>
<p><strong>The “Pushing” Forces (Red):</strong> Next, the features in red push the prediction to the right (higher GWP). For this specific data point:</p>
<ul>
<li><p><strong>Soil_SaLo = 0.0:</strong> The absence of the highly mitigating Sandy Loam soil is the single biggest factor pushing the GWP prediction up.</p></li>
<li><p><strong>Rotation_AGC1 = 1.0:</strong> The presence of this specific crop rotation also contributes to a higher predicted GWP.</p></li>
<li><p><strong>P_APPkg_ha = 43.96:</strong> The value of this phosphorus input adds another push higher.</p></li>
</ul>
<p><strong>The “Pulling” Forces (Blue):</strong> These are the features that drag the prediction to the left (lower GWP). For this data point, their combined effect is much stronger:</p>
<ul>
<li><p><strong>BROIL_MINN = 24.8:</strong> This nitrogen value is extremely close to the optimal spot of 25 identified earlier, so it has a very strong effect, pulling the GWP prediction down.</p></li>
<li><p><strong>seed_rate_main = 168.1, pest_app_LULC = 1.08, and BROIL_MINP = 9.92:</strong> The specific values for seed rate, pesticide application, and another manure input all contribute to lowering the predicted GWP for this instance.</p></li>
</ul>
<p><strong>The Final Prediction (f(x)):</strong> The final prediction of -1846.89 is the result of this tug-of-war. The combined downward pull of the blue features was so significant that it completely overwhelmed both the high starting base value and the upward push of the red features.</p>
<p>In summary, this plot tells a clear story: This specific field scenario was predicted to have a very low (even negative) GWP, despite lacking the beneficial Sandy Loam soil, primarily because its nitrogen input (BROIL_MINN) was at the perfect level to minimize emissions, with other management practices also contributing favorably.</p>
<hr>
</section>
</section>
<section id="conclusions-from-shap-analysis" class="level2" data-number="B.13">
<h2 data-number="B.13" class="anchored" data-anchor-id="conclusions-from-shap-analysis"><span class="header-section-number">B.13</span> Conclusions from SHAP Analysis</h2>
<p>Collectively, the SHAP analyses provide a multi-layered understanding of the Random Forest model’s behavior, moving from high-level feature importance to the specifics of individual predictions.</p>
<p>The analysis confirms that soil type (Soil_SaLo, Soil_LoSa) is the single most dominant factor driving GWP_Total predictions, with specific types having a strong mitigating effect. Beyond this, the plots reveal the complex, non-linear impact of key inputs. A prime example is the nitrogen-related feature BROIL_MINN, where the model identified an optimal “sweet spot” for minimizing GWP—an insight unavailable from simpler linear models.</p>
<p>Furthermore, the force plot provides a clear case study of how these global trends interact at a local level, demonstrating a specific instance where optimal management inputs could overcome less-than-ideal soil conditions to produce a favorable outcome.</p>
<p>This deep interpretability, which clarifies both what the model prioritizes and why it makes a specific prediction, is crucial. It validates the model as more than just a predictor, but as a trustworthy tool for generating actionable scientific hypotheses about the drivers of GWP in agricultural systems.</p>
<hr>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./appendix-a-etl-rag.html" class="pagination-link" aria-label="appendix-a-etl-rag.html">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">appendix-a-etl-rag.html</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./appendix-c-hardware.html" class="pagination-link" aria-label="Appendix C: Computational Hardware">
        <span class="nav-page-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Appendix C: Computational Hardware</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Preliminary LCA Cover crop Analysis {#sec-appendix-b-lca}</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>This appendix documents a preliminary machine learning pipeline developed to predict the total global warming potential (GWP_Total) in agricultural systems using Random Forests. It is presented here not as a definitive study, but as a transparent account of a foundational learning experience.</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>This initial work was instrumental in developing the more sophisticated data processing techniques and modeling intuition applied in the main body of the dissertation. It highlighted critical challenges in feature selection, the handling of highly skewed distributions, and revealed the limitations of a standard Random Forest model for this type of complex environmental data.</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>Therefore, while the pipeline performs a full suite of tasks; from preprocessing and hyperparameter tuning to evaluation and SHAP analysis, its primary value lies in demonstrating the iterative process of model development and the analytical growth that informed the subsequent, more advanced research detailed in this portfolio.</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>The notebook performs the following tasks:</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Data Loading and Exploration:** Reads the dataset (a Feather file) and inspects its structure and summary statistics.</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Data Preprocessing:** Cleans the data by one-hot encoding categorical variables, dropping irrelevant/constant columns, and handling missing values.</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Train/Test Split:** Splits the dataset into training and testing sets to ensure that our model generalizes well.</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Hyperparameter Tuning:** Uses <span class="in">`RandomizedSearchCV`</span> to optimize the Random Forest parameters (including <span class="in">`min_samples_leaf`</span> and <span class="in">`max_features`</span>) using cross-validation.</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Model Evaluation:** Evaluates the best model's performance using R², RMSE, MAE, Out-of-Bag (OOB) score, learning curves, permutation feature importance, and residual analysis.</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Interpretability:** Visualizes feature importance, partial dependence plots, and uses SHAP for both global and local model explanations.</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Baseline Comparison:** Compares our Random Forest results with a baseline Linear Regression model.</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**Logging:** Logs progress and key results for reproducibility and transparency.</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>This pipeline predicts the outcome and provides interpretable insights.</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: import-packages</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> r2_score, mean_squared_error, mean_absolute_error</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> PartialDependenceDisplay, permutation_importance</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, RandomizedSearchCV, learning_curve</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold  <span class="co"># if needed for classification; here we use regression</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Print current working directory, version and file of sklearn</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Current working directory:"</span>, os.getcwd())</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sklearn version:"</span>, sklearn.__version__)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sklearn location:"</span>, sklearn.<span class="va">__file__</span>)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## Logging Configuration</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>In this section, we set up logging to track the progress and important events throughout the pipeline. Logging ensures that all steps (data loading, preprocessing, model tuning, etc.) are recorded in a log file. This is useful for debugging, reproducibility, and for providing transparency.</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: logging-and-configuration</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a><span class="co"># 0) LOGGING CONFIG</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up logging</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the absolute path for the G: drive (mounted as /mnt/g)</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>LOG_FILE <span class="op">=</span> <span class="st">"/data/GWP_randomforest_2.log"</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    filename<span class="op">=</span>LOG_FILE,</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>    level<span class="op">=</span>logging.INFO,</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"</span><span class="sc">%(asctime)s</span><span class="st"> [</span><span class="sc">%(levelname)s</span><span class="st">] </span><span class="sc">%(message)s</span><span class="st">"</span>,</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    datefmt<span class="op">=</span><span class="st">"%Y-%m-</span><span class="sc">%d</span><span class="st"> %H:%M:%S"</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Starting the GWP Prediction Pipeline"</span>)</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Loading and Exploration</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>Here we read the dataset from a Feather file. Feather is a fast binary file format for data frames, making data loading efficient. We then:</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Preview the Data:** Using <span class="in">`df.head()`</span> to view the first few rows.</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Summarize the Data:** Using <span class="in">`df.describe()`</span> to get summary statistics (mean, standard deviation, etc.) for each feature.</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>This step helps us understand the structure, size, and distribution of the data before any processing.</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: data-loading-exploration</span></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Data Loading and Exploration</span></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 1: Reading data and exploring"</span>)</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in the data (adjust the file path as needed)</span></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_feather(<span class="st">'data/LCA_DATA.feather'</span>)</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a><span class="co"># Data Exploration: Print head and summary statistics</span></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Data Head:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, df.head())</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Data Summary:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, df.describe())</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Data Head:"</span>)</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Data Summary:"</span>)</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.describe())</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Preprocessing</span></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>In this section, we prepare the data for modeling by:</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**One-Hot Encoding:** Converting categorical variables (oil, Scenario, LULC, CoverCrop, Rotation) into dummy variables. This allows us to use them in machine learning models.</span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dropping Irrelevant Columns:** Removing columns that are not useful for prediction (identifiers, target variables already separated).</span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Handling Missing Values:** Filling missing values with a simple strategy (here, 0) to avoid issues during training.</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dropping Constant Columns:** Removing columns that have only one unique value since they don’t contribute any information.</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>These steps ensure the dataset is clean and structured appropriately for machine learning.</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: data-preprocessing</span></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Data Preprocessing</span></span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 2: Data preprocessing and one-hot encoding"</span>)</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a><span class="co"># Define target and categorical columns</span></span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>target_column <span class="op">=</span> <span class="st">"GWP_Total"</span>  <span class="co"># variable to predict</span></span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a>categorical_cols <span class="op">=</span> [<span class="st">"Soil"</span>, <span class="st">"Scenario"</span>, <span class="st">"LULC"</span>, <span class="st">"CoverCrop"</span>, <span class="st">"Rotation"</span>]  <span class="co"># categorical features</span></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>excluded_cols <span class="op">=</span> [target_column, <span class="st">"seed_GWP_main"</span>]  <span class="co"># columns to exclude</span></span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode categorical columns (drop_first=False to retain all info)</span></span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>df_encoded <span class="op">=</span> pd.get_dummies(df, columns<span class="op">=</span>categorical_cols, drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Columns after one-hot encoding:"</span>)</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Columns after one-hot encoding:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, df_encoded.columns)</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_encoded.columns)</span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate target y and drop excluded columns from predictors</span></span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_encoded[target_column]</span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>df_encoded <span class="op">=</span> df_encoded.drop(columns<span class="op">=</span>excluded_cols, errors<span class="op">=</span><span class="st">"ignore"</span>)</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_encoded.drop(columns<span class="op">=</span>excluded_cols, errors<span class="op">=</span><span class="st">"ignore"</span>)</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill missing values simply and drop constant columns</span></span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)</span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>cols_constant <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> X.columns <span class="cf">if</span> X[col].nunique() <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Dropping constant columns: </span><span class="sc">%s</span><span class="st">"</span>, cols_constant)</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.drop(columns<span class="op">=</span>cols_constant)</span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Columns after dropping excluded columns:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, X.columns)</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Columns after dropping excluded cols:"</span>)</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.columns)</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dropping constant columns:"</span>, cols_constant)</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Preprocessing – Output Explanation</span></span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>After running the data preprocessing code, we observe the following outputs:</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>Columns after one-hot encoding:</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a>Index(['BROIL_FERT', 'BROIL_MINN', 'BROIL_MINP', 'BROIL_ORGN', 'BROIL_ORGP',</span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a>       'BROIL_NH3N', 'UAN30_FERT', 'UAN30_MINN', 'UAN30_NH3N', 'P_APPkg_ha',</span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a>       'pest_app_LULC', 'pest_app_CoverCrop', 'sowing_count', 'GWP_Total',</span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a>       'total_glyphosate_kg', 'seed_rate_main', 'seed_rate_cc',</span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a>       'seed_GWP_main', 'Soil_LoSa', 'Soil_SaLo', 'Soil_SiLo', 'Scenario_BE',</span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a>       'Scenario_BL', 'Scenario_RE', 'Scenario_RL', 'Scenario_WE',</span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>       'Scenario_WL', 'LULC_SOYb', 'LULC_WWHT', 'CoverCrop_RYE',</span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>       'CoverCrop_WWHT', 'CoverCrop_WbAR', 'Rotation_AGA2', 'Rotation_AGAB',</span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a>       'Rotation_AGB1', 'Rotation_AGB2', 'Rotation_AGC1', 'Rotation_AGD1',</span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a>       'Rotation_AGD2'],</span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a>      dtype='object')</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a><span class="fu">### Columns after One-Hot Encoding</span></span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a>The printed output shows an index (list) of column names:</span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a>Index(['BROIL_FERT', 'BROIL_MINN', 'BROIL_MINP', 'BROIL_ORGN', 'BROIL_ORGP',</span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a>       'BROIL_NH3N', 'UAN30_FERT', 'UAN30_MINN', 'UAN30_NH3N', 'P_APPkg_ha',</span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a>       'pest_app_LULC', 'pest_app_CoverCrop', 'sowing_count',</span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a>       'total_glyphosate_kg', 'seed_rate_main', 'seed_rate_cc', 'Soil_LoSa',</span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a>       'Soil_SaLo', 'Soil_SiLo', 'Scenario_BE', 'Scenario_BL', 'Scenario_RE',</span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a>       'Scenario_RL', 'Scenario_WE', 'Scenario_WL', 'LULC_SOYb', 'LULC_WWHT',</span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a>       'CoverCrop_RYE', 'CoverCrop_WWHT', 'CoverCrop_WbAR', 'Rotation_AGA2',</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a>       'Rotation_AGAB', 'Rotation_AGB1', 'Rotation_AGB2', 'Rotation_AGC1',</span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a>       'Rotation_AGD1', 'Rotation_AGD2'],</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a>      dtype='object')</span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**What This Means:**  </span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a>  All the original columns are present, including the target variable <span class="in">`GWP_Total`</span>. Additionally, the categorical variables (like <span class="in">`Soil`</span>, <span class="in">`Scenario`</span>, etc.) have been converted into dummy variables. For example, we see columns like <span class="in">`Soil_LoSa`</span>, <span class="in">`Soil_SaLo`</span>, etc., which represent the different levels of the original <span class="in">`Soil`</span> category.</span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a><span class="fu">### Columns after Dropping Excluded Columns</span></span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a>After removing the columns that are not needed for prediction (the target variable and any explicitly excluded ones), the output is:</span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>Index(<span class="co">[</span><span class="ot">'BROIL_FERT', 'BROIL_MINN', 'BROIL_MINP', 'BROIL_ORGN', 'BROIL_ORGP', 'BROIL_NH3N', 'UAN30_FERT', 'UAN30_MINN', 'UAN30_NH3N', 'P_APPkg_ha', 'pest_app_LULC', 'pest_app_CoverCrop', 'sowing_count', 'total_glyphosate_kg', 'seed_rate_main', 'seed_rate_cc', 'Soil_LoSa', 'Soil_SaLo', 'Soil_SiLo', 'Scenario_BE', 'Scenario_BL', 'Scenario_RE', 'Scenario_RL', 'Scenario_WE', 'Scenario_WL', 'LULC_SOYb', 'LULC_WWHT', 'CoverCrop_RYE', 'CoverCrop_WWHT', 'CoverCrop_WbAR', 'Rotation_AGA2', 'Rotation_AGAB', 'Rotation_AGB1', 'Rotation_AGB2', 'Rotation_AGC1', 'Rotation_AGD1', 'Rotation_AGD2'</span><span class="co">]</span>, dtype='object')</span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**What This Means:**  </span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>  The target column <span class="in">`GWP_Total`</span> and the excluded column <span class="in">`seed_GWP_main`</span> have been removed from the predictor matrix <span class="in">`X`</span>. Now, <span class="in">`X`</span> contains only the features that will be used to predict <span class="in">`GWP_Total`</span>.</span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dropping Constant Columns</span></span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>The output:</span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a>Dropping constant columns: []</span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**What This Means:**  </span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a>  This indicates that no columns in <span class="in">`X`</span> had only a single unique value. In other words, every predictor varies to some extent, so no columns were dropped for being constant.</span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a>these outputs confirm that:</span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Categorical variables have been correctly transformed into dummy variables.</span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Irrelevant columns (target and specified exclusions) have been successfully removed.</span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The data is clean and ready for the next steps in the pipeline.</span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a><span class="fu">## Train/Test Split</span></span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a>We split the dataset into:</span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training Set (80%):** Used to train the model.</span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Testing Set (20%):** Held out for evaluating the model’s performance on unseen data.</span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a>This split is crucial to validate that our model generalizes well beyond the data it was trained on.</span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: train-test-split</span></span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Train/Test Split</span></span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 3: Splitting data into train/test"</span>)</span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data (80% train, 20% test)</span></span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Train shape = </span><span class="sc">%s</span><span class="st">, Test shape = </span><span class="sc">%s</span><span class="st">"</span>, X_train.shape, X_test.shape)</span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Training set size: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, Test set size: </span><span class="sc">{</span>X_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a><span class="fu">## Train/Test Split – Output Explanation</span></span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a>After splitting the dataset into training and testing sets, we obtain the following:</span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training Set Size:** (11883, 37)  </span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a>  This means there are 11,883 samples (rows) and 37 features (columns) in the training set.  </span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Testing Set Size:** (2971, 37)  </span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a>  The test set contains 2,971 samples and the same 37 features.</span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a>**What This Means:**</span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**80/20 Split:**  </span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a>  Approximately 80% of the data is used for training the model, and 20% is reserved for evaluating its performance on unseen data. This is crucial to ensure that our model generalizes well and is not overfitting.</span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Consistent Feature Dimensions:**  </span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a>  Both sets have 37 features, which confirms that the preprocessing steps (e.g., one-hot encoding, dropping irrelevant columns) have been applied consistently across the entire dataset.</span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a>Overall, these numbers indicate a successful split, setting the stage for model training and evaluation.</span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hyperparameter Tuning with RandomizedSearchCV</span></span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a>Here, we use <span class="in">`RandomizedSearchCV`</span> to optimize the hyperparameters of the Random Forest model. Key points include:</span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parameter Grid:** We define ranges for parameters such as <span class="in">`n_estimators`</span>, <span class="in">`max_depth`</span>, <span class="in">`min_samples_split`</span>, <span class="in">`min_samples_leaf`</span>, and <span class="in">`max_features`</span>.</span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Randomized Search:** Rather than checking every possible combination (which can be computationally expensive), we sample a subset of parameter settings.</span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cross-Validation (CV=3):** For each parameter combination, 3-fold cross-validation is used to estimate performance (using negative RMSE here).</span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a>The best hyperparameters are selected based on cross-validation results, ensuring that the final model is tuned for optimal performance.</span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: hyperparameter-tuning</span></span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-291"><a href="#cb10-291" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-292"><a href="#cb10-292" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Hyperparameter Tuning with RandomizedSearchCV</span></span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 4: Hyperparameter Tuning with RandomizedSearchCV"</span>)</span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> RandomizedSearchCV</span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a><span class="co"># Expanded parameter grid includes min_samples_leaf and max_features</span></span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a>param_dist <span class="op">=</span> {</span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],</span>
<span id="cb10-301"><a href="#cb10-301" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>],</span>
<span id="cb10-302"><a href="#cb10-302" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>],</span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'auto'</span>, <span class="st">'sqrt'</span>]</span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate RandomForestRegressor with oob_score enabled for extra validation</span></span>
<span id="cb10-308"><a href="#cb10-308" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">42</span>, oob_score<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-309"><a href="#cb10-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a>search <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb10-311"><a href="#cb10-311" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>rf,</span>
<span id="cb10-312"><a href="#cb10-312" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>param_dist,</span>
<span id="cb10-313"><a href="#cb10-313" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb10-314"><a href="#cb10-314" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb10-315"><a href="#cb10-315" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>, </span>
<span id="cb10-316"><a href="#cb10-316" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-317"><a href="#cb10-317" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb10-318"><a href="#cb10-318" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb10-319"><a href="#cb10-319" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-320"><a href="#cb10-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-321"><a href="#cb10-321" aria-hidden="true" tabindex="-1"></a>search.fit(X_train, y_train)</span>
<span id="cb10-322"><a href="#cb10-322" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> search.best_estimator_</span>
<span id="cb10-323"><a href="#cb10-323" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best params:"</span>, search.best_params_)</span>
<span id="cb10-324"><a href="#cb10-324" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Best params from RandomizedSearchCV: </span><span class="sc">%s</span><span class="st">"</span>, search.best_params_)</span>
<span id="cb10-325"><a href="#cb10-325" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-326"><a href="#cb10-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-327"><a href="#cb10-327" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hyperparameter Tuning with RandomizedSearchCV – Output Explanation</span></span>
<span id="cb10-328"><a href="#cb10-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-329"><a href="#cb10-329" aria-hidden="true" tabindex="-1"></a>In this step, we optimize the hyperparameters of our Random Forest model using <span class="in">`RandomizedSearchCV`</span>. Here’s what the output tells us:</span>
<span id="cb10-330"><a href="#cb10-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-331"><a href="#cb10-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fitting 3 folds for each of 10 candidates, totalling 30 fits:**  </span>
<span id="cb10-332"><a href="#cb10-332" aria-hidden="true" tabindex="-1"></a>  This message indicates that the search evaluated 10 different hyperparameter configurations (candidates). For each configuration, 3-fold cross-validation was performed (i.e., the training set was split into 3 parts, with 2 used for training and 1 for validation), resulting in a total of 30 model fits.</span>
<span id="cb10-333"><a href="#cb10-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-334"><a href="#cb10-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Best params:**  </span>
<span id="cb10-335"><a href="#cb10-335" aria-hidden="true" tabindex="-1"></a>  The output displays the best set of hyperparameters found by the search. In this example, the best parameters are:  </span>
<span id="cb10-336"><a href="#cb10-336" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="in">`n_estimators`</span>: 200 (i.e., the model will use 200 trees)  </span>
<span id="cb10-337"><a href="#cb10-337" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="in">`min_samples_split`</span>: 10 (i.e., a node must have at least 10 samples before it can be split)  </span>
<span id="cb10-338"><a href="#cb10-338" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="in">`min_samples_leaf`</span>: 2 (i.e., each leaf must have at least 2 samples)  </span>
<span id="cb10-339"><a href="#cb10-339" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="in">`max_features`</span>: 'sqrt' (i.e., the square root of the number of features will be considered when looking for the best split)  </span>
<span id="cb10-340"><a href="#cb10-340" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="in">`max_depth`</span>: 20 (i.e., the maximum depth of each tree is set to 20)</span>
<span id="cb10-341"><a href="#cb10-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-342"><a href="#cb10-342" aria-hidden="true" tabindex="-1"></a>**Interpretation:**  </span>
<span id="cb10-343"><a href="#cb10-343" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The RandomizedSearchCV process sampled 10 configurations from our parameter grid and evaluated each using 3-fold cross-validation.  </span>
<span id="cb10-344"><a href="#cb10-344" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The configuration with the lowest (negative) RMSE during cross-validation was chosen.  </span>
<span id="cb10-345"><a href="#cb10-345" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These best parameters will be used in the final model, ensuring that our Random Forest is well-tuned to our data, potentially leading to improved performance on unseen data.</span>
<span id="cb10-346"><a href="#cb10-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-347"><a href="#cb10-347" aria-hidden="true" tabindex="-1"></a>This detailed tuning step helps in making our model more robust and generalizable.</span>
<span id="cb10-348"><a href="#cb10-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-349"><a href="#cb10-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-350"><a href="#cb10-350" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Evaluation</span></span>
<span id="cb10-351"><a href="#cb10-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-352"><a href="#cb10-352" aria-hidden="true" tabindex="-1"></a>This section evaluates the performance of our tuned Random Forest model using multiple metrics:</span>
<span id="cb10-353"><a href="#cb10-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-354"><a href="#cb10-354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**R² (Coefficient of Determination):** Measures the proportion of variance explained by the model.</span>
<span id="cb10-355"><a href="#cb10-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RMSE (Root Mean Squared Error):** Indicates the average magnitude of the prediction error.</span>
<span id="cb10-356"><a href="#cb10-356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MAE (Mean Absolute Error):** Provides the average absolute error, which is less sensitive to outliers than RMSE.</span>
<span id="cb10-357"><a href="#cb10-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Out-of-Bag (OOB) Score:** An internal cross-validation metric available in Random Forests when <span class="in">`oob_score=True`</span>.</span>
<span id="cb10-358"><a href="#cb10-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-359"><a href="#cb10-359" aria-hidden="true" tabindex="-1"></a><span class="fu">### Extra Validation Checks</span></span>
<span id="cb10-360"><a href="#cb10-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-361"><a href="#cb10-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Learning Curves:** We plot learning curves to see how training and validation errors change with increasing training set size. This helps diagnose overfitting or underfitting.</span>
<span id="cb10-362"><a href="#cb10-362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Permutation Feature Importance:** Measures how the model’s performance deteriorates when a feature’s values are randomly shuffled. This provides an alternative view of feature importance.</span>
<span id="cb10-363"><a href="#cb10-363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residual Analysis:** A plot of residuals (actual minus predicted values) to check for any systematic patterns that the model might be missing.</span>
<span id="cb10-364"><a href="#cb10-364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Baseline Comparison:** We also fit a simple Linear Regression model to compare its performance with our Random Forest model.</span>
<span id="cb10-365"><a href="#cb10-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-366"><a href="#cb10-366" aria-hidden="true" tabindex="-1"></a>These checks provide a comprehensive view of the model’s performance and robustness.</span>
<span id="cb10-367"><a href="#cb10-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-368"><a href="#cb10-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-371"><a href="#cb10-371" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-372"><a href="#cb10-372" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model-evaluation</span></span>
<span id="cb10-373"><a href="#cb10-373" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-374"><a href="#cb10-374" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-375"><a href="#cb10-375" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) Model Evaluation</span></span>
<span id="cb10-376"><a href="#cb10-376" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-377"><a href="#cb10-377" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5: Evaluating the best model"</span>)</span>
<span id="cb10-378"><a href="#cb10-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-379"><a href="#cb10-379" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions on training and test sets</span></span>
<span id="cb10-380"><a href="#cb10-380" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> best_model.predict(X_train)</span>
<span id="cb10-381"><a href="#cb10-381" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb10-382"><a href="#cb10-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-383"><a href="#cb10-383" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rmse(y_true, y_pred):</span>
<span id="cb10-384"><a href="#cb10-384" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_squared_error(y_true, y_pred, squared<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-385"><a href="#cb10-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-386"><a href="#cb10-386" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute performance metrics</span></span>
<span id="cb10-387"><a href="#cb10-387" aria-hidden="true" tabindex="-1"></a>r2_train  <span class="op">=</span> r2_score(y_train, y_train_pred)</span>
<span id="cb10-388"><a href="#cb10-388" aria-hidden="true" tabindex="-1"></a>rmse_train <span class="op">=</span> rmse(y_train, y_train_pred)</span>
<span id="cb10-389"><a href="#cb10-389" aria-hidden="true" tabindex="-1"></a>mae_train  <span class="op">=</span> mean_absolute_error(y_train, y_train_pred)</span>
<span id="cb10-390"><a href="#cb10-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-391"><a href="#cb10-391" aria-hidden="true" tabindex="-1"></a>r2_test   <span class="op">=</span> r2_score(y_test, y_test_pred)</span>
<span id="cb10-392"><a href="#cb10-392" aria-hidden="true" tabindex="-1"></a>rmse_test <span class="op">=</span> rmse(y_test, y_test_pred)</span>
<span id="cb10-393"><a href="#cb10-393" aria-hidden="true" tabindex="-1"></a>mae_test  <span class="op">=</span> mean_absolute_error(y_test, y_test_pred)</span>
<span id="cb10-394"><a href="#cb10-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-395"><a href="#cb10-395" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Training set: R^2 = </span><span class="sc">%.3f</span><span class="st">, RMSE = </span><span class="sc">%.3f</span><span class="st">, MAE = </span><span class="sc">%.3f</span><span class="st">"</span>, r2_train, rmse_train, mae_train)</span>
<span id="cb10-396"><a href="#cb10-396" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Test set:     R^2 = </span><span class="sc">%.3f</span><span class="st">, RMSE = </span><span class="sc">%.3f</span><span class="st">, MAE = </span><span class="sc">%.3f</span><span class="st">"</span>, r2_test, rmse_test, mae_test)</span>
<span id="cb10-397"><a href="#cb10-397" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Model Performance:"</span>)</span>
<span id="cb10-398"><a href="#cb10-398" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: R^2 = </span><span class="sc">{</span>r2_train<span class="sc">:.3f}</span><span class="ss">, RMSE = </span><span class="sc">{</span>rmse_train<span class="sc">:.3f}</span><span class="ss">, MAE = </span><span class="sc">{</span>mae_train<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-399"><a href="#cb10-399" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test set: R^2 = </span><span class="sc">{</span>r2_test<span class="sc">:.3f}</span><span class="ss">, RMSE = </span><span class="sc">{</span>rmse_test<span class="sc">:.3f}</span><span class="ss">, MAE = </span><span class="sc">{</span>mae_test<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-400"><a href="#cb10-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-401"><a href="#cb10-401" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB Error (if available)</span></span>
<span id="cb10-402"><a href="#cb10-402" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(best_model, <span class="st">"oob_score_"</span>):</span>
<span id="cb10-403"><a href="#cb10-403" aria-hidden="true" tabindex="-1"></a>    oob_score <span class="op">=</span> best_model.oob_score_</span>
<span id="cb10-404"><a href="#cb10-404" aria-hidden="true" tabindex="-1"></a>    logging.info(<span class="st">"OOB Score: </span><span class="sc">%.3f</span><span class="st">"</span>, oob_score)</span>
<span id="cb10-405"><a href="#cb10-405" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"OOB Score: </span><span class="sc">{</span>oob_score<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-406"><a href="#cb10-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-407"><a href="#cb10-407" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-408"><a href="#cb10-408" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Learning Curves</span></span>
<span id="cb10-409"><a href="#cb10-409" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-410"><a href="#cb10-410" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5A: Generating learning curves"</span>)</span>
<span id="cb10-411"><a href="#cb10-411" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb10-412"><a href="#cb10-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-413"><a href="#cb10-413" aria-hidden="true" tabindex="-1"></a>train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(</span>
<span id="cb10-414"><a href="#cb10-414" aria-hidden="true" tabindex="-1"></a>    best_model, X_train, y_train, cv<span class="op">=</span><span class="dv">3</span>, scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb10-415"><a href="#cb10-415" aria-hidden="true" tabindex="-1"></a>    train_sizes<span class="op">=</span>np.linspace(<span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="dv">10</span>), n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-416"><a href="#cb10-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-417"><a href="#cb10-417" aria-hidden="true" tabindex="-1"></a>train_scores_mean <span class="op">=</span> <span class="op">-</span>np.mean(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-418"><a href="#cb10-418" aria-hidden="true" tabindex="-1"></a>test_scores_mean <span class="op">=</span> <span class="op">-</span>np.mean(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-419"><a href="#cb10-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-420"><a href="#cb10-420" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb10-421"><a href="#cb10-421" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, train_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"r"</span>, label<span class="op">=</span><span class="st">"Training RMSE"</span>)</span>
<span id="cb10-422"><a href="#cb10-422" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, test_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"g"</span>, label<span class="op">=</span><span class="st">"Validation RMSE"</span>)</span>
<span id="cb10-423"><a href="#cb10-423" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Learning Curves"</span>)</span>
<span id="cb10-424"><a href="#cb10-424" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Training examples"</span>)</span>
<span id="cb10-425"><a href="#cb10-425" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"RMSE"</span>)</span>
<span id="cb10-426"><a href="#cb10-426" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"best"</span>)</span>
<span id="cb10-427"><a href="#cb10-427" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb10-428"><a href="#cb10-428" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-429"><a href="#cb10-429" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-430"><a href="#cb10-430" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Learning curves generated"</span>)</span>
<span id="cb10-431"><a href="#cb10-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-432"><a href="#cb10-432" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-433"><a href="#cb10-433" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Permutation Feature Importance</span></span>
<span id="cb10-434"><a href="#cb10-434" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-435"><a href="#cb10-435" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5B: Calculating permutation feature importance"</span>)</span>
<span id="cb10-436"><a href="#cb10-436" aria-hidden="true" tabindex="-1"></a>perm_importance <span class="op">=</span> permutation_importance(best_model, X_test, y_test, scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>, n_repeats<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-437"><a href="#cb10-437" aria-hidden="true" tabindex="-1"></a>perm_importance_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb10-438"><a href="#cb10-438" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: X_test.columns,</span>
<span id="cb10-439"><a href="#cb10-439" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance_mean'</span>: perm_importance.importances_mean,</span>
<span id="cb10-440"><a href="#cb10-440" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance_std'</span>: perm_importance.importances_std</span>
<span id="cb10-441"><a href="#cb10-441" aria-hidden="true" tabindex="-1"></a>}).sort_values(by<span class="op">=</span><span class="st">'importance_mean'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-442"><a href="#cb10-442" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Permutation Feature Importance:"</span>)</span>
<span id="cb10-443"><a href="#cb10-443" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(perm_importance_df)</span>
<span id="cb10-444"><a href="#cb10-444" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Permutation Feature Importance:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, perm_importance_df)</span>
<span id="cb10-445"><a href="#cb10-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-446"><a href="#cb10-446" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-447"><a href="#cb10-447" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Residual Analysis</span></span>
<span id="cb10-448"><a href="#cb10-448" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-449"><a href="#cb10-449" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5C: Plotting residuals"</span>)</span>
<span id="cb10-450"><a href="#cb10-450" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_test <span class="op">-</span> y_test_pred</span>
<span id="cb10-451"><a href="#cb10-451" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb10-452"><a href="#cb10-452" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test_pred, residuals, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb10-453"><a href="#cb10-453" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb10-454"><a href="#cb10-454" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted Values"</span>)</span>
<span id="cb10-455"><a href="#cb10-455" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Residuals (Actual - Predicted)"</span>)</span>
<span id="cb10-456"><a href="#cb10-456" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Residual Plot"</span>)</span>
<span id="cb10-457"><a href="#cb10-457" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb10-458"><a href="#cb10-458" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-459"><a href="#cb10-459" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-460"><a href="#cb10-460" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Residual plot generated"</span>)</span>
<span id="cb10-461"><a href="#cb10-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-462"><a href="#cb10-462" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-463"><a href="#cb10-463" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra Validation: Baseline Model Comparison</span></span>
<span id="cb10-464"><a href="#cb10-464" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-465"><a href="#cb10-465" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 5D: Comparing with baseline Linear Regression"</span>)</span>
<span id="cb10-466"><a href="#cb10-466" aria-hidden="true" tabindex="-1"></a>baseline_model <span class="op">=</span> LinearRegression()</span>
<span id="cb10-467"><a href="#cb10-467" aria-hidden="true" tabindex="-1"></a>baseline_model.fit(X_train, y_train)</span>
<span id="cb10-468"><a href="#cb10-468" aria-hidden="true" tabindex="-1"></a>y_test_pred_baseline <span class="op">=</span> baseline_model.predict(X_test)</span>
<span id="cb10-469"><a href="#cb10-469" aria-hidden="true" tabindex="-1"></a>r2_baseline <span class="op">=</span> r2_score(y_test, y_test_pred_baseline)</span>
<span id="cb10-470"><a href="#cb10-470" aria-hidden="true" tabindex="-1"></a>rmse_baseline <span class="op">=</span> rmse(y_test, y_test_pred_baseline)</span>
<span id="cb10-471"><a href="#cb10-471" aria-hidden="true" tabindex="-1"></a>mae_baseline <span class="op">=</span> mean_absolute_error(y_test, y_test_pred_baseline)</span>
<span id="cb10-472"><a href="#cb10-472" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Baseline Linear Regression Performance:"</span>)</span>
<span id="cb10-473"><a href="#cb10-473" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R^2 = </span><span class="sc">{</span>r2_baseline<span class="sc">:.3f}</span><span class="ss">, RMSE = </span><span class="sc">{</span>rmse_baseline<span class="sc">:.3f}</span><span class="ss">, MAE = </span><span class="sc">{</span>mae_baseline<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-474"><a href="#cb10-474" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Baseline Linear Regression: R^2 = </span><span class="sc">%.3f</span><span class="st">, RMSE = </span><span class="sc">%.3f</span><span class="st">, MAE = </span><span class="sc">%.3f</span><span class="st">"</span>, r2_baseline, rmse_baseline, mae_baseline)</span>
<span id="cb10-475"><a href="#cb10-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-476"><a href="#cb10-476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-477"><a href="#cb10-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-478"><a href="#cb10-478" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Evaluation – Output Explanation</span></span>
<span id="cb10-479"><a href="#cb10-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-480"><a href="#cb10-480" aria-hidden="true" tabindex="-1"></a>After training our tuned Random Forest model, we evaluate its performance using multiple metrics and additional validation checks.</span>
<span id="cb10-481"><a href="#cb10-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-482"><a href="#cb10-482" aria-hidden="true" tabindex="-1"></a><span class="fu">### Performance Metrics</span></span>
<span id="cb10-483"><a href="#cb10-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-484"><a href="#cb10-484" aria-hidden="true" tabindex="-1"></a>**Printed Output:**</span>
<span id="cb10-485"><a href="#cb10-485" aria-hidden="true" tabindex="-1"></a>Training set: R^2 = 0.702, RMSE = 3001.564, MAE = 2025.280 Test set: R^2 = 0.668, RMSE = 3155.660, MAE = 2147.089 OOB Score: 0.661</span>
<span id="cb10-486"><a href="#cb10-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-487"><a href="#cb10-487" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**R² (Coefficient of Determination):**  </span>
<span id="cb10-488"><a href="#cb10-488" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Training R² = 0.702**  </span>
<span id="cb10-489"><a href="#cb10-489" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Test R² = 0.668**  </span>
<span id="cb10-490"><a href="#cb10-490" aria-hidden="true" tabindex="-1"></a>  The model explains roughly 70% of the variance in the training set and about 67% in the test set. This small gap indicates that the model generalizes reasonably well with some overfitting.</span>
<span id="cb10-491"><a href="#cb10-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-492"><a href="#cb10-492" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RMSE (Root Mean Squared Error):**  </span>
<span id="cb10-493"><a href="#cb10-493" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Training RMSE ≈ 3001**  </span>
<span id="cb10-494"><a href="#cb10-494" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Test RMSE ≈ 3156**  </span>
<span id="cb10-495"><a href="#cb10-495" aria-hidden="true" tabindex="-1"></a>  RMSE gives the average magnitude of the prediction error. A difference of about 155 between training and test suggests the model’s performance on unseen data is close to its performance on training data.</span>
<span id="cb10-496"><a href="#cb10-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-497"><a href="#cb10-497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MAE (Mean Absolute Error):**  </span>
<span id="cb10-498"><a href="#cb10-498" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Training MAE ≈ 2025**  </span>
<span id="cb10-499"><a href="#cb10-499" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Test MAE ≈ 2147**  </span>
<span id="cb10-500"><a href="#cb10-500" aria-hidden="true" tabindex="-1"></a>  MAE is less sensitive to outliers than RMSE. These values confirm a consistent level of error on training vs. test sets.</span>
<span id="cb10-501"><a href="#cb10-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-502"><a href="#cb10-502" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Out-of-Bag (OOB) Score = 0.661**  </span>
<span id="cb10-503"><a href="#cb10-503" aria-hidden="true" tabindex="-1"></a>  Random Forests can estimate performance internally (without a separate validation set) by using out-of-bag samples. An OOB score of 0.661 aligns closely with the test R² of 0.668, giving additional confidence in the model’s ability to generalize.</span>
<span id="cb10-504"><a href="#cb10-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-505"><a href="#cb10-505" aria-hidden="true" tabindex="-1"></a><span class="fu">### Learning Curves</span></span>
<span id="cb10-506"><a href="#cb10-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-507"><a href="#cb10-507" aria-hidden="true" tabindex="-1"></a><span class="al">![Learning Curves](pic/learning_curves.png)</span></span>
<span id="cb10-508"><a href="#cb10-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-509"><a href="#cb10-509" aria-hidden="true" tabindex="-1"></a>The **Learning Curves** plot shows how RMSE (vertical axis) changes for both the training set (red line) and validation set (green line) as the number of training examples increases (horizontal axis). Key observations:</span>
<span id="cb10-510"><a href="#cb10-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-511"><a href="#cb10-511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training RMSE** remains relatively stable around ~3000, indicating the model fits the training data consistently.</span>
<span id="cb10-512"><a href="#cb10-512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Validation RMSE** decreases from about 3700 to 3200 as more training data is used, suggesting that adding more data helps reduce error.</span>
<span id="cb10-513"><a href="#cb10-513" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The gap between training and validation curves narrows but does not fully converge, indicating the model could still benefit from more data or further hyperparameter tuning. However, the gap is not excessively large, suggesting a reasonable balance between bias and variance.</span>
<span id="cb10-514"><a href="#cb10-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-515"><a href="#cb10-515" aria-hidden="true" tabindex="-1"></a><span class="fu">### Permutation Feature Importance</span></span>
<span id="cb10-516"><a href="#cb10-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-517"><a href="#cb10-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-518"><a href="#cb10-518" aria-hidden="true" tabindex="-1"></a>**What It Means:**  </span>
<span id="cb10-519"><a href="#cb10-519" aria-hidden="true" tabindex="-1"></a>Permutation Importance measures how much the model’s performance deteriorates when each feature’s values are randomly shuffled. Features with higher <span class="in">`importance_mean`</span> cause a greater increase in RMSE when shuffled, indicating they are more crucial to the model. This provides an alternative perspective on which predictors matter most, complementing the built-in feature importance from the Random Forest itself.</span>
<span id="cb10-520"><a href="#cb10-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-521"><a href="#cb10-521" aria-hidden="true" tabindex="-1"></a><span class="fu">### Residual Analysis</span></span>
<span id="cb10-522"><a href="#cb10-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-523"><a href="#cb10-523" aria-hidden="true" tabindex="-1"></a><span class="al">![Residual Plot](pic/residual_plot.png)</span></span>
<span id="cb10-524"><a href="#cb10-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-525"><a href="#cb10-525" aria-hidden="true" tabindex="-1"></a>The **Residual Plot** (actual minus predicted values on the y-axis vs. predicted values on the x-axis) helps identify patterns in the errors:</span>
<span id="cb10-526"><a href="#cb10-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-527"><a href="#cb10-527" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The red dashed line at y=0 indicates perfect predictions.</span>
<span id="cb10-528"><a href="#cb10-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-529"><a href="#cb10-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Residuals appear mostly centered around 0, with some spread at higher predicted values. </span>
<span id="cb10-530"><a href="#cb10-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-531"><a href="#cb10-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No strong pattern or trend suggests the model is systematically over- or under-predicting at particular ranges. </span>
<span id="cb10-532"><a href="#cb10-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-533"><a href="#cb10-533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The moderate spread indicates the model still has some variance in its predictions, but there is no major sign of severe bias.</span>
<span id="cb10-534"><a href="#cb10-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-535"><a href="#cb10-535" aria-hidden="true" tabindex="-1"></a><span class="fu">### Baseline Comparison</span></span>
<span id="cb10-536"><a href="#cb10-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-537"><a href="#cb10-537" aria-hidden="true" tabindex="-1"></a>Baseline Linear Regression Performance: R^2 = 0.576, RMSE = 3566.148, MAE = 2570.892</span>
<span id="cb10-538"><a href="#cb10-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-539"><a href="#cb10-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-540"><a href="#cb10-540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Linear Regression R² = 0.576**  </span>
<span id="cb10-541"><a href="#cb10-541" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Linear Regression RMSE ≈ 3566**  </span>
<span id="cb10-542"><a href="#cb10-542" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Linear Regression MAE ≈ 2571**</span>
<span id="cb10-543"><a href="#cb10-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-544"><a href="#cb10-544" aria-hidden="true" tabindex="-1"></a>Compared to our Random Forest (R² = 0.668), the baseline linear model performs worse across all metrics (lower R², higher RMSE, and higher MAE). This shows that the additional complexity of the Random Forest provides a noticeable performance boost and better captures the underlying relationships in the data.</span>
<span id="cb10-545"><a href="#cb10-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-546"><a href="#cb10-546" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-547"><a href="#cb10-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-548"><a href="#cb10-548" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Summary of Model Evaluation**</span></span>
<span id="cb10-549"><a href="#cb10-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-550"><a href="#cb10-550" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Metrics** show the Random Forest performing well, with a small gap between training and test performance.</span>
<span id="cb10-551"><a href="#cb10-551" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Learning Curves** suggest the model benefits from more data and is not drastically overfitting.</span>
<span id="cb10-552"><a href="#cb10-552" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Permutation Importance** corroborates which features the model relies on most.</span>
<span id="cb10-553"><a href="#cb10-553" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Residual Plot** reveals no major systematic bias in predictions, though there is some variance at higher values.</span>
<span id="cb10-554"><a href="#cb10-554" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Baseline Comparison** confirms that the Random Forest outperforms a simpler Linear Regression model.</span>
<span id="cb10-555"><a href="#cb10-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-556"><a href="#cb10-556" aria-hidden="true" tabindex="-1"></a>Overall, these results indicate a reasonably robust model that generalizes better than a simple baseline and provides insights into which features matter most. </span>
<span id="cb10-557"><a href="#cb10-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-558"><a href="#cb10-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-559"><a href="#cb10-559" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Importance and Partial Dependence</span></span>
<span id="cb10-560"><a href="#cb10-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-561"><a href="#cb10-561" aria-hidden="true" tabindex="-1"></a>Understanding which features drive the model predictions is key for interpretability. In this section, we:</span>
<span id="cb10-562"><a href="#cb10-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-563"><a href="#cb10-563" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Built-in Feature Importance:** Leverage the Random Forest's internal mechanism to rank features based on how much they reduce impurity.</span>
<span id="cb10-564"><a href="#cb10-564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bar Plot Visualization:** Display the top 10 features to quickly identify the most influential predictors.</span>
<span id="cb10-565"><a href="#cb10-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Partial Dependence Plots (PDP):** Visualize the effect of the top features on the predicted outcome while holding other features constant. This helps in understanding the relationship between predictors and the target variable.</span>
<span id="cb10-566"><a href="#cb10-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-567"><a href="#cb10-567" aria-hidden="true" tabindex="-1"></a>These tools help understand the model’s decision-making process.</span>
<span id="cb10-568"><a href="#cb10-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-569"><a href="#cb10-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-572"><a href="#cb10-572" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-573"><a href="#cb10-573" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: feature-importance-partial-dependence</span></span>
<span id="cb10-574"><a href="#cb10-574" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-575"><a href="#cb10-575" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-576"><a href="#cb10-576" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) Feature Importance and Partial Dependence</span></span>
<span id="cb10-577"><a href="#cb10-577" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-578"><a href="#cb10-578" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-579"><a href="#cb10-579" aria-hidden="true" tabindex="-1"></a><span class="co">FEATURE IMPORTANCE:</span></span>
<span id="cb10-580"><a href="#cb10-580" aria-hidden="true" tabindex="-1"></a><span class="co">    Lists the top 10 features based on the model’s built-in importance scores.</span></span>
<span id="cb10-581"><a href="#cb10-581" aria-hidden="true" tabindex="-1"></a><span class="co">    Produces a bar chart so we can visually interpret which features are most influential.</span></span>
<span id="cb10-582"><a href="#cb10-582" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-583"><a href="#cb10-583" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 6: Feature importance plots &amp; partial dependence"</span>)</span>
<span id="cb10-584"><a href="#cb10-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-585"><a href="#cb10-585" aria-hidden="true" tabindex="-1"></a><span class="co"># Built-in feature importance from Random Forest</span></span>
<span id="cb10-586"><a href="#cb10-586" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> best_model.feature_importances_</span>
<span id="cb10-587"><a href="#cb10-587" aria-hidden="true" tabindex="-1"></a>feat_imp <span class="op">=</span> pd.Series(importances, index<span class="op">=</span>X_train.columns).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-588"><a href="#cb10-588" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Top 10 Features:</span><span class="ch">\n</span><span class="sc">%s</span><span class="st">"</span>, feat_imp.head(<span class="dv">20</span>))</span>
<span id="cb10-589"><a href="#cb10-589" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 10 Feature Importances:"</span>)</span>
<span id="cb10-590"><a href="#cb10-590" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(feat_imp.head(<span class="dv">10</span>))</span>
<span id="cb10-591"><a href="#cb10-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-592"><a href="#cb10-592" aria-hidden="true" tabindex="-1"></a><span class="co"># Bar plot for feature importance</span></span>
<span id="cb10-593"><a href="#cb10-593" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-594"><a href="#cb10-594" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span>feat_imp.head(<span class="dv">10</span>), y<span class="op">=</span>feat_imp.head(<span class="dv">10</span>).index, palette<span class="op">=</span><span class="st">"viridis"</span>)</span>
<span id="cb10-595"><a href="#cb10-595" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top 10 Feature Importances (Random Forest)"</span>)</span>
<span id="cb10-596"><a href="#cb10-596" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Importance"</span>)</span>
<span id="cb10-597"><a href="#cb10-597" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Feature"</span>)</span>
<span id="cb10-598"><a href="#cb10-598" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-599"><a href="#cb10-599" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-600"><a href="#cb10-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-601"><a href="#cb10-601" aria-hidden="true" tabindex="-1"></a><span class="co"># Partial Dependence Plots for top features</span></span>
<span id="cb10-602"><a href="#cb10-602" aria-hidden="true" tabindex="-1"></a>top_features <span class="op">=</span> feat_imp.head(<span class="dv">3</span>).index.tolist()</span>
<span id="cb10-603"><a href="#cb10-603" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Generating partial dependence plots for top features: </span><span class="sc">%s</span><span class="st">"</span>, top_features)</span>
<span id="cb10-604"><a href="#cb10-604" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb10-605"><a href="#cb10-605" aria-hidden="true" tabindex="-1"></a>PartialDependenceDisplay.from_estimator(best_model, X_train, top_features, n_jobs<span class="op">=-</span><span class="dv">1</span>, ax<span class="op">=</span>ax)</span>
<span id="cb10-606"><a href="#cb10-606" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">"Partial Dependence of Top Features"</span>)</span>
<span id="cb10-607"><a href="#cb10-607" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-608"><a href="#cb10-608" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-609"><a href="#cb10-609" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-610"><a href="#cb10-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-611"><a href="#cb10-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-612"><a href="#cb10-612" aria-hidden="true" tabindex="-1"></a><span class="fu">### Built-in Feature Importance</span></span>
<span id="cb10-613"><a href="#cb10-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-614"><a href="#cb10-614" aria-hidden="true" tabindex="-1"></a>**Printed Output (Top 10 Features):**</span>
<span id="cb10-615"><a href="#cb10-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-616"><a href="#cb10-616" aria-hidden="true" tabindex="-1"></a>Soil_SaLo 0.077287 </span>
<span id="cb10-617"><a href="#cb10-617" aria-hidden="true" tabindex="-1"></a>Soil_SiLo 0.076775 </span>
<span id="cb10-618"><a href="#cb10-618" aria-hidden="true" tabindex="-1"></a>BROIL_MINN 0.065416 </span>
<span id="cb10-619"><a href="#cb10-619" aria-hidden="true" tabindex="-1"></a>pest_app_LULC 0.056814 </span>
<span id="cb10-620"><a href="#cb10-620" aria-hidden="true" tabindex="-1"></a>seed_rate_main 0.055316 </span>
<span id="cb10-621"><a href="#cb10-621" aria-hidden="true" tabindex="-1"></a>P_APPkg_ha 0.053626 </span>
<span id="cb10-622"><a href="#cb10-622" aria-hidden="true" tabindex="-1"></a>Rotation_AGC1 0.052144 </span>
<span id="cb10-623"><a href="#cb10-623" aria-hidden="true" tabindex="-1"></a>BROIL_MINP 0.050464 </span>
<span id="cb10-624"><a href="#cb10-624" aria-hidden="true" tabindex="-1"></a>BROIL_NH3N 0.046516 </span>
<span id="cb10-625"><a href="#cb10-625" aria-hidden="true" tabindex="-1"></a>BROIL_ORGN 0.043727 </span>
<span id="cb10-626"><a href="#cb10-626" aria-hidden="true" tabindex="-1"></a>dtype: float64</span>
<span id="cb10-627"><a href="#cb10-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-628"><a href="#cb10-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-629"><a href="#cb10-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**What It Means:**  </span>
<span id="cb10-630"><a href="#cb10-630" aria-hidden="true" tabindex="-1"></a>  The Random Forest model computes an “importance” score for each feature, reflecting how much that feature reduces impurity (variance) in the trees. Higher values indicate more influential predictors.  </span>
<span id="cb10-631"><a href="#cb10-631" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Soil_SaLo and Soil_SiLo** appear to be the top two features, suggesting that soil types have a significant impact on <span class="in">`GWP_Total`</span>.  </span>
<span id="cb10-632"><a href="#cb10-632" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**BROIL_MINN** (a nitrogen-related feature) also ranks high, indicating fertilizer- or manure-related variables may be critical to the model.  </span>
<span id="cb10-633"><a href="#cb10-633" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**pest_app_LULC**, **seed_rate_main**, and **P_APPkg_ha** follow, further suggesting that application rates and crop management choices drive <span class="in">`GWP_Total`</span>.</span>
<span id="cb10-634"><a href="#cb10-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-635"><a href="#cb10-635" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bar Chart Visualization</span></span>
<span id="cb10-636"><a href="#cb10-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-637"><a href="#cb10-637" aria-hidden="true" tabindex="-1"></a><span class="al">![Top 10 Feature Importances (Random Forest)](pic/feature_importance.png)</span></span>
<span id="cb10-638"><a href="#cb10-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-639"><a href="#cb10-639" aria-hidden="true" tabindex="-1"></a>In the bar chart:</span>
<span id="cb10-640"><a href="#cb10-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-641"><a href="#cb10-641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Horizontal Axis (Importance):** Shows how crucial each feature is. </span>
<span id="cb10-642"><a href="#cb10-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-643"><a href="#cb10-643" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Vertical Axis (Feature):** Lists the top 10 features in descending order of importance.  </span>
<span id="cb10-644"><a href="#cb10-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-645"><a href="#cb10-645" aria-hidden="true" tabindex="-1"></a>This chart helps both technical and non-technical audiences quickly see which variables most influence the model’s predictions.</span>
<span id="cb10-646"><a href="#cb10-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-647"><a href="#cb10-647" aria-hidden="true" tabindex="-1"></a><span class="fu">### Partial Dependence Plots (PDP)</span></span>
<span id="cb10-648"><a href="#cb10-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-649"><a href="#cb10-649" aria-hidden="true" tabindex="-1"></a>Partial Dependence Plots help us understand how the most important features influence the model's prediction for GWP_Total. Each plot shows the effect of a single feature while averaging out the effects of all other features.</span>
<span id="cb10-650"><a href="#cb10-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-651"><a href="#cb10-651" aria-hidden="true" tabindex="-1"></a><span class="al">![Partial Dependence of Top Features](pic/partial_dependence.png)</span></span>
<span id="cb10-652"><a href="#cb10-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-653"><a href="#cb10-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**What PDP Shows:**  </span>
<span id="cb10-654"><a href="#cb10-654" aria-hidden="true" tabindex="-1"></a>  Each subplot corresponds to one of the top 3 features. On the x-axis is the feature’s possible values; on the y-axis is the **partial dependence**, which represents the model’s predicted outcome when varying that feature while holding others constant.</span>
<span id="cb10-655"><a href="#cb10-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-656"><a href="#cb10-656" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpretation of Key Findings</span></span>
<span id="cb10-657"><a href="#cb10-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-658"><a href="#cb10-658" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Soil_SaLo (Sandy Loam Soil)**  </span>
<span id="cb10-659"><a href="#cb10-659" aria-hidden="true" tabindex="-1"></a>  This plot shows a strong negative relationship. The model predicts a significantly lower **GWP_Total** when the soil type is Sandy Loam (value of <span class="in">`1`</span>) compared to other types. This suggests that sandy loam soil is a key factor in mitigating GWP in this model.</span>
<span id="cb10-660"><a href="#cb10-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-661"><a href="#cb10-661" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Soil_SiLo (Silty Loam Soil)**  </span>
<span id="cb10-662"><a href="#cb10-662" aria-hidden="true" tabindex="-1"></a>  In contrast, the model associates Silty Loam soil (value of <span class="in">`1`</span>) with an increase in the predicted **GWP_Total**.</span>
<span id="cb10-663"><a href="#cb10-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-664"><a href="#cb10-664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BROIL_MINN**  </span>
<span id="cb10-665"><a href="#cb10-665" aria-hidden="true" tabindex="-1"></a>  This feature displays a clear non-linear relationship. The model predicts the lowest **GWP_Total** when **BROIL_MINN** is at a “sweet spot” of approximately <span class="in">`25`</span>. Values lower or higher than this optimum level are associated with an increase in predicted GWP. This complex pattern highlights the model’s ability to capture relationships that a standard linear regression would miss.</span>
<span id="cb10-666"><a href="#cb10-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-667"><a href="#cb10-667" aria-hidden="true" tabindex="-1"></a><span class="fu">## SHAP Analysis for Model Interpretability</span></span>
<span id="cb10-668"><a href="#cb10-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-669"><a href="#cb10-669" aria-hidden="true" tabindex="-1"></a>To understand the drivers of the model’s predictions, SHAP (SHapley Additive exPlanations) analysis was performed. The following bar plot shows the global feature importance, ranking features by their average impact on the model’s output magnitude.</span>
<span id="cb10-670"><a href="#cb10-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-671"><a href="#cb10-671" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpretation of Key Findings</span></span>
<span id="cb10-672"><a href="#cb10-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-673"><a href="#cb10-673" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Primary Drivers (Soil Type):** Consistent with other importance measures, soil types are the most influential features. Soil_SaLo (Sandy Loam) has the largest average impact on the model’s predictions, followed closely by Soil_LoSa (Loamy Sand).</span>
<span id="cb10-674"><a href="#cb10-674" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Secondary Drivers (Management &amp; Inputs):** After soil type, key management and input variables show significant importance. These include BROIL_MINN (a nitrogen-related feature), seed_rate_main, and pest_app_LULC (pesticide application).</span>
<span id="cb10-675"><a href="#cb10-675" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tertiary Drivers:** Other nitrogen and phosphorus-related inputs (BROIL_MINP, P_APPkg_ha) and specific crop rotations (Rotation_AGC1) also contribute meaningfully to the predictions, but with less overall impact than the primary and secondary drivers.</span>
<span id="cb10-676"><a href="#cb10-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-677"><a href="#cb10-677" aria-hidden="true" tabindex="-1"></a>This ranking confirms that the model’s predictions for GWP_Total are most heavily influenced by the inherent soil characteristics of a site, followed by key agricultural inputs and management decisions.</span>
<span id="cb10-678"><a href="#cb10-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-681"><a href="#cb10-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-682"><a href="#cb10-682" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: shap-analysis</span></span>
<span id="cb10-683"><a href="#cb10-683" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb10-684"><a href="#cb10-684" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-685"><a href="#cb10-685" aria-hidden="true" tabindex="-1"></a><span class="co"># 7) SHAP Analysis for Interpretability</span></span>
<span id="cb10-686"><a href="#cb10-686" aria-hidden="true" tabindex="-1"></a><span class="co"># ================================</span></span>
<span id="cb10-687"><a href="#cb10-687" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-688"><a href="#cb10-688" aria-hidden="true" tabindex="-1"></a><span class="co">SHAP ANALYSIS</span></span>
<span id="cb10-689"><a href="#cb10-689" aria-hidden="true" tabindex="-1"></a><span class="co">    Provides global interpretability through SHAP summary plots.</span></span>
<span id="cb10-690"><a href="#cb10-690" aria-hidden="true" tabindex="-1"></a><span class="co">    Gives a local explanation (force plot) for one individual test case, </span></span>
<span id="cb10-691"><a href="#cb10-691" aria-hidden="true" tabindex="-1"></a><span class="co">    which shows how each feature contributes to a single prediction.</span></span>
<span id="cb10-692"><a href="#cb10-692" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-693"><a href="#cb10-693" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"STEP 7: SHAP analysis for global &amp; local interpretability"</span>)</span>
<span id="cb10-694"><a href="#cb10-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-695"><a href="#cb10-695" aria-hidden="true" tabindex="-1"></a><span class="co"># SHAP: Global and local interpretability</span></span>
<span id="cb10-696"><a href="#cb10-696" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.TreeExplainer(best_model)</span>
<span id="cb10-697"><a href="#cb10-697" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer.shap_values(X_train)</span>
<span id="cb10-698"><a href="#cb10-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-699"><a href="#cb10-699" aria-hidden="true" tabindex="-1"></a><span class="co"># Global SHAP plots (bar and summary)</span></span>
<span id="cb10-700"><a href="#cb10-700" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values, X_train, plot_type<span class="op">=</span><span class="st">"bar"</span>, show<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-701"><a href="#cb10-701" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"SHAP Feature Importance (Bar Plot)"</span>)</span>
<span id="cb10-702"><a href="#cb10-702" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-703"><a href="#cb10-703" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-704"><a href="#cb10-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-705"><a href="#cb10-705" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values, X_train, show<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-706"><a href="#cb10-706" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"SHAP Summary Plot"</span>)</span>
<span id="cb10-707"><a href="#cb10-707" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-708"><a href="#cb10-708" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-709"><a href="#cb10-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-710"><a href="#cb10-710" aria-hidden="true" tabindex="-1"></a><span class="co"># Local SHAP explanation for the first test instance</span></span>
<span id="cb10-711"><a href="#cb10-711" aria-hidden="true" tabindex="-1"></a>logging.info(<span class="st">"Generating SHAP force plot for the first test instance"</span>)</span>
<span id="cb10-712"><a href="#cb10-712" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Generating SHAP force plot for the first test instance..."</span>)</span>
<span id="cb10-713"><a href="#cb10-713" aria-hidden="true" tabindex="-1"></a>instance <span class="op">=</span> X_test.iloc[<span class="dv">0</span>]</span>
<span id="cb10-714"><a href="#cb10-714" aria-hidden="true" tabindex="-1"></a>instance_shap <span class="op">=</span> explainer.shap_values(instance)</span>
<span id="cb10-715"><a href="#cb10-715" aria-hidden="true" tabindex="-1"></a>shap.force_plot(explainer.expected_value, instance_shap, instance, matplotlib<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-716"><a href="#cb10-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-717"><a href="#cb10-717" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-718"><a href="#cb10-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-719"><a href="#cb10-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-720"><a href="#cb10-720" aria-hidden="true" tabindex="-1"></a><span class="al">![SHAP Feature Importance (Bar Plot)](pic/shap_bar_plot.png)</span></span>
<span id="cb10-721"><a href="#cb10-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-722"><a href="#cb10-722" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-723"><a href="#cb10-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-724"><a href="#cb10-724" aria-hidden="true" tabindex="-1"></a><span class="fu">### SHAP Summary Plot: Understanding Feature Effects</span></span>
<span id="cb10-725"><a href="#cb10-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-726"><a href="#cb10-726" aria-hidden="true" tabindex="-1"></a>The SHAP summary plot provides a powerful global view of the model's behavior. It reveals not only the importance of each feature but also the direction and consistency of its effect on the **GWP_Total** prediction.</span>
<span id="cb10-727"><a href="#cb10-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-728"><a href="#cb10-728" aria-hidden="true" tabindex="-1"></a><span class="al">![SHAP Summary Plot](pic/shap_summary_plot.png)</span></span>
<span id="cb10-729"><a href="#cb10-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-730"><a href="#cb10-730" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpretation of Key Findings</span></span>
<span id="cb10-731"><a href="#cb10-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-732"><a href="#cb10-732" aria-hidden="true" tabindex="-1"></a>This plot reveals several clear patterns learned by the model:</span>
<span id="cb10-733"><a href="#cb10-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-734"><a href="#cb10-734" aria-hidden="true" tabindex="-1"></a>**The Dominant Effect of Soil Type:**</span>
<span id="cb10-735"><a href="#cb10-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-736"><a href="#cb10-736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Soil_SaLo (Sandy Loam) &amp; Soil_LoSa (Loamy Sand):** These are the two most important features. The plot shows a very clean separation: when these soils are present (red dots, feature value = 1), they consistently have a large negative SHAP value, meaning they strongly push the model to predict a lower GWP. Their absence (blue dots, feature value = 0) results in a positive SHAP value, increasing the predicted GWP.  </span>
<span id="cb10-737"><a href="#cb10-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Soil_SiLo (Silty Loam):** This feature shows the opposite effect. Its presence (red dots) consistently pushes the prediction higher (positive SHAP values).</span>
<span id="cb10-738"><a href="#cb10-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-739"><a href="#cb10-739" aria-hidden="true" tabindex="-1"></a>**The Non-Linear Impact of Nitrogen (BROIL_MINN):**</span>
<span id="cb10-740"><a href="#cb10-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-741"><a href="#cb10-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This feature shows a complex, non-linear pattern. Both low values (blue dots) and high values (red dots) push the GWP prediction higher. The optimal values that push the prediction lower (negative SHAP values) are in the middle of the range (represented by the purple-ish dots). This confirms the "V" shape we saw in the Partial Dependence Plot, indicating an optimal "sweet spot" for this input.</span>
<span id="cb10-742"><a href="#cb10-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-743"><a href="#cb10-743" aria-hidden="true" tabindex="-1"></a>**Impact of Management Practices:**</span>
<span id="cb10-744"><a href="#cb10-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-745"><a href="#cb10-745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**seed_rate_main &amp; pest_app_LULC:** For both of these features, there is a clear trend. High values (red dots) are on the right, pushing the prediction higher, while low values (blue dots) are on the left, pushing it lower. This indicates the model has learned a positive correlation: higher seeding and pesticide rates are associated with higher predicted GWP.</span>
<span id="cb10-746"><a href="#cb10-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-747"><a href="#cb10-747" aria-hidden="true" tabindex="-1"></a>This single visualization confirms the model's logic. It's not just a black box; it has learned scientifically plausible relationships, such as the strong mitigating effect of certain soil types and the non-linear impact of nitrogen inputs.</span>
<span id="cb10-748"><a href="#cb10-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-749"><a href="#cb10-749" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-750"><a href="#cb10-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-751"><a href="#cb10-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### SHAP Force Plot: Explaining a Single Prediction</span></span>
<span id="cb10-752"><a href="#cb10-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-753"><a href="#cb10-753" aria-hidden="true" tabindex="-1"></a>This force plot provides a detailed look at how the model arrived at its prediction for one specific data point in your dataset. It can be described as a "tug-of-war" between features that push the prediction higher and those that pull it lower.</span>
<span id="cb10-754"><a href="#cb10-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-755"><a href="#cb10-755" aria-hidden="true" tabindex="-1"></a>Lets walk through the story of this specific prediction, from right to left:</span>
<span id="cb10-756"><a href="#cb10-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-757"><a href="#cb10-757" aria-hidden="true" tabindex="-1"></a><span class="al">![SHAP Force Plot](pic/shap_force_plot.png)</span></span>
<span id="cb10-758"><a href="#cb10-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-759"><a href="#cb10-759" aria-hidden="true" tabindex="-1"></a>**The Starting Point (base value):** The model starts with the base value, which is the average prediction for GWP_Total across all data points in your training set. While the exact number isn't represented, we can see it's a positive value on the x-axis. This is the model's initial "guess" before considering any of this specific data points features.</span>
<span id="cb10-760"><a href="#cb10-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-761"><a href="#cb10-761" aria-hidden="true" tabindex="-1"></a>**The "Pushing" Forces (Red):** Next, the features in red push the prediction to the right (higher GWP). For this specific data point:  </span>
<span id="cb10-762"><a href="#cb10-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-763"><a href="#cb10-763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Soil_SaLo = 0.0:** The absence of the highly mitigating Sandy Loam soil is the single biggest factor pushing the GWP prediction up. </span>
<span id="cb10-764"><a href="#cb10-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-765"><a href="#cb10-765" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Rotation_AGC1 = 1.0:** The presence of this specific crop rotation also contributes to a higher predicted GWP.  </span>
<span id="cb10-766"><a href="#cb10-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-767"><a href="#cb10-767" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**P_APPkg_ha = 43.96:** The value of this phosphorus input adds another push higher.  </span>
<span id="cb10-768"><a href="#cb10-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-769"><a href="#cb10-769" aria-hidden="true" tabindex="-1"></a>**The "Pulling" Forces (Blue):** These are the features that drag the prediction to the left (lower GWP). For this data point, their combined effect is much stronger:  </span>
<span id="cb10-770"><a href="#cb10-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-771"><a href="#cb10-771" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BROIL_MINN = 24.8:** This nitrogen value is extremely close to the optimal spot of 25 identified earlier, so it has a very strong effect, pulling the GWP prediction down.  </span>
<span id="cb10-772"><a href="#cb10-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-773"><a href="#cb10-773" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**seed_rate_main = 168.1, pest_app_LULC = 1.08, and BROIL_MINP = 9.92:** The specific values for seed rate, pesticide application, and another manure input all contribute to lowering the predicted GWP for this instance.</span>
<span id="cb10-774"><a href="#cb10-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-775"><a href="#cb10-775" aria-hidden="true" tabindex="-1"></a>**The Final Prediction (f(x)):** The final prediction of -1846.89 is the result of this tug-of-war. The combined downward pull of the blue features was so significant that it completely overwhelmed both the high starting base value and the upward push of the red features.</span>
<span id="cb10-776"><a href="#cb10-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-777"><a href="#cb10-777" aria-hidden="true" tabindex="-1"></a>In summary, this plot tells a clear story: This specific field scenario was predicted to have a very low (even negative) GWP, despite lacking the beneficial Sandy Loam soil, primarily because its nitrogen input (BROIL_MINN) was at the perfect level to minimize emissions, with other management practices also contributing favorably.</span>
<span id="cb10-778"><a href="#cb10-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-779"><a href="#cb10-779" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-780"><a href="#cb10-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-781"><a href="#cb10-781" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusions from SHAP Analysis</span></span>
<span id="cb10-782"><a href="#cb10-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-783"><a href="#cb10-783" aria-hidden="true" tabindex="-1"></a>Collectively, the SHAP analyses provide a multi-layered understanding of the Random Forest model's behavior, moving from high-level feature importance to the specifics of individual predictions.</span>
<span id="cb10-784"><a href="#cb10-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-785"><a href="#cb10-785" aria-hidden="true" tabindex="-1"></a>The analysis confirms that soil type (Soil_SaLo, Soil_LoSa) is the single most dominant factor driving GWP_Total predictions, with specific types having a strong mitigating effect. Beyond this, the plots reveal the complex, non-linear impact of key inputs. A prime example is the nitrogen-related feature BROIL_MINN, where the model identified an optimal "sweet spot" for minimizing GWP—an insight unavailable from simpler linear models.</span>
<span id="cb10-786"><a href="#cb10-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-787"><a href="#cb10-787" aria-hidden="true" tabindex="-1"></a>Furthermore, the force plot provides a clear case study of how these global trends interact at a local level, demonstrating a specific instance where optimal management inputs could overcome less-than-ideal soil conditions to produce a favorable outcome.</span>
<span id="cb10-788"><a href="#cb10-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-789"><a href="#cb10-789" aria-hidden="true" tabindex="-1"></a>This deep interpretability, which clarifies both what the model prioritizes and why it makes a specific prediction, is crucial. It validates the model as more than just a predictor, but as a trustworthy tool for generating actionable scientific hypotheses about the drivers of GWP in agricultural systems.</span>
<span id="cb10-790"><a href="#cb10-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-791"><a href="#cb10-791" aria-hidden="true" tabindex="-1"></a>---</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written by Alex Godinez | <a href="https://github.com/agodinezmm2007">GitHub</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>